CHAPTER 1INTRODUCTION 1.1INTRODUCTION The Transmission Control Protocol (TCP) is one of the pillars of the global Internet.
One of TCP’s critical functions is congestion control, and the objective of this book is to provide an overview of this topic, with special emphasis on analytical modeling of congestion control protocols.
TCP’s congestion control algorithm has been described as the largest human-made feedback- controlled system in the world.
It enables hundreds of millions of devices, ranging from huge servers to the smallest PDA, to coexist together and make efficient use of existing bandwidth resources.
It does so over link speeds that vary from a few kilobits per second to tens of gigabits per second.
How it is able to accomplish this task in a fully distributed manner forms the subject matter of this book.
Starting from its origins as a simple static-window /C0controlled algorithm, TCP congestion control went through several important enhancements in the years since 1986, starting with Van Jacobsen’s invention of the Slow Start and Congestion Avoidance algorithms, described in his sem- inal paper
In addition to significant developments in the area of new congestion control algorithms and enhancements to existing ones, we have seen a big increase in our theoretical under- standing of congestion control.
Several new important application areas, such as video streaming and data center networks (DCNs), have appeared in recent years, which have kept the pace of innovation in this subject area humming.
The objective of this book is twofold:
Part 1 provides an accessible overview of the main theo- retical developments in the area of modeling of congestion control systems.
Not only have these models helped our understanding of existing algorithms, but they have also played an essential role in the discovery of newer algorithms.
They have also helped us explore the limits of stability of congestion control algorithms.
Part 2 discusses the application of congestion control to some impor- tant areas that have arisen in the past 10 to 15 years, namely Wireless Access Networks, DCNs, high-speed links, and Packet Video Transport.
Each of these applications comes with its own unique characteristics that has resulted in new requirements from the congestion control point of view and has led to a proliferation of new algorithms in recent years.
Several of these are now as widely deployed as the legacy TCP congestion control algorithm.
The strong theoretical underpin- ning that has been established for this subject has helped us greatly in understanding the behavior of these new algorithms.
The rest of this chapter is organized as follows:  provides an introduction to the topic of congestion control in data networks and introduces the idea of additive increase/multiplicative 1Internet Congestion Control.
provides a description of the TCP Reno algorithm, and  discusses Active Queue Management (AQM).
TCP Vegas is described in  , and provides an overview of the rest of the chapters in this book.
1.2BASICS OF CONGESTION CONTROL
The problem of congestion control arises whenever multiple distributed agents try to make use of shared resources (  ).
It arises widely in different scenarios, including traffic control on highways or air traffic control.
This book focuses on the specific case of packet data networks, where the high-level objective of congestion control is to provide good utilization of network resources and at the same time provide acceptable performance for the users of the network.
The theory and practice of congestion control for data networks have evolved in the past 3 decades to the system that we use today.
The early Internet had very rudimentary congestion control, which led to a series of network collapses in 1986.
As a result, a congestion control algo- rithm called TCP Slow Start (also sometimes called TCP Tahoe) was put into place in 1987 and 1988, which, in its current incarnation as TCP Reno, remains one of the dominant algorithms used in the Internet today.
Since then, the Internet has evolved and now features transmissions over wire- less media, link speeds that are several orders of magnitudes faster, widespread transmission of video streams, and the recent rise of massive data centers.
The congestion control research and development community has successfully met the challenge of modifying the original congestion control algorithm and has come up with newer algorithms to address these new types of networks.
Traffic SourceCongestion Control Traffic SourceCongestion ControlShared ResourceFeedback  The congestion control problem.2  INTRODUCTION 1.2.1 THE CONGESTION CONTROL PROBLEM DEFINITION
To define the congestion control problem more formally, consider the graph in  in which the throughput of a data source has been plotted on the y-axis and the network traffic load from the source has been plotted on the x-axis.
We observe the following: 
If the load is small, the throughput keeps up with the increasing load.
After the load reaches the network capacity, at the “knee” of the curve, the throughput stops increasing.
Beyond the knee, as the load increases, queues start to build up in the network, thus increasing the end-to-end latency without adding to the throughput.
The network is then said to be in a state of congestion.
If the traffic load increases even further and gets to the “cliff” part of the curve, then the throughput experiences a rather drastic reduction.
This is because queues within the network have increased to the point that packets are being dropped.
If the sources choose to retransmit to recover from losses, then this adds further to the total load, resulting in a positive feedback loop that sends the network into congestion collapse.
To avoid congestion collapse, each data source should try to maintain the load it is sending into the network in the neighborhood of the knee.
An algorithm that accomplishes this objective is known as a congestion avoidance algorithm; a congestion control algorithm tries to prevent the sys- tem from going over the cliff.
This task is easier said than done and runs into the following chal- lenge: The location of the knee is not known and in fact changes with total network load and traffic patterns.
Hence, the problem of congestion control is inherently a distributed optimization problem in which each source has to constantly adapt its traffic load as a function of feedback information it is receiving from the network as it tries to stay near the knee of the curve.
The original TCP Tahoe algorithm and its descendants fall into the congestion control category because in the absence of support from the network nodes, the only way they can detect congestion is by filling up the network buffers and then detecting the subsequent packet drops as a sign of con- gestion.
Many routers today implement a more sophisticated form of buffer management called Random Early Detection (RED) that provides early warnings of a congestive situation.
TCP in Throughput   Traffic LoadKnee Cliff  Congestion Collapse   Throughput versus load.3 1.2BASICS OF CONGESTION CONTROL combination with RED then qualifies as a congestion avoidance algorithm.
Algorithms such as TCP Vegas operate the system around the knee without requiring special support in the network nodes.
Some of the questions that need to be answered in the design of a congestion control algorithm include the following (all of the new terms used below are defined in this chapter): 
Should the congestion control be implemented on an end-to-end or on a hop-by-hop basis?
 Should the traffic control be exercised using a windows-based mechanism or a rate-based mechanism?
 How does the network signal to the end points that it is congested (or conversely, not congested)?
 What is the best Traffic Rate Increment rule in the absence of congestion?
 What is the best Traffic Rate Decrement rule in the presence of congestion?
 How can the algorithm ensure fairness among all connections sharing a link, and what measure of fairness should be used?
 Should the congestion control algorithm prioritize high link utilization of low end-to-end latency?
There is a function closely related to congestion control, called flow control, and often the two terms are used interchangeably.
For the purposes of this book, flow control is defined as a function that is used at the destination node to prevent its receive buffers from overflowing.
It is implemen- ted by providing feedback to the sender, usually in the form of window-size constraints.
Later in this book, we will discuss the case of video streaming in which the objective is to prevent the receive buffer from underflowing.
We classify the algorithm used to accomplish this objective also in the flow control category.
1.2.2 OPTIMIZATION CRITERIA FOR CONGESTION CONTROL
This section and the next are based on some pioneering work by Chiu and Jain
[2]in the conges- tion control area.
They defined optimization criteria that the congestion control algorithms should satisfy, and then they showed that there is a class of controls called AIMD that satisfies many of these criteria.
We start by enunciating the Chiu-Jain optimization criteria in this section.
Consider a network with N data sources and define r i(t) to be the data rate (i.e., load) of the ith data source.
Also assume that the system is operating near the knee of the curve from  so that the network rate allocation to the ithsource after congestion control is also given by r i(t).
We assume that the system operation is synchronized and happens in discrete time so that the rate at time (t11) is a function of the rate at time t, and the network feedback received at time t.
We will also assume that the network congestion feedback is received at the sources instantaneously without any delay.
Define the following optimization criteria for selecting the best control:  Efficiency: Every source has a bottleneck node, which is the node where it gets the least throughput among all the nodes along its path.
Let A ibe the set of other sources that also pass through this bottleneck node; then the efficiency criterion states thatP jAAiriðnÞshould be as close as possible to C, where C is the capacity of the bottleneck node.
 Fairness: The efficiency criterion does not put any constraints on how the data rates are distributed among the various sources.
A fair allocation is when all the sources that share a4  INTRODUCTION bottleneck node get an equal allocation of the bottleneck link capacity.
The Jain fairness  defined below quantifies this notion of fairness.
The first aspect is the speed with which the rate r i(t) gets to its equilibrium rate at the knee after starting from zero (called responsiveness).
The second aspect is the magnitude of its oscillations around the equilibrium value while in steady state (called smoothness).
1.2.3 OPTIMALITY OF ADDITIVE INCREASE MULTIPLICATIVE DECREASE CONGESTION CONTROL
In this section, we restrict ourselves to a two-source network that shares a bottleneck link with capacity C. As shown in  , any rate allocation (r 1,r2) can be represented by a point in a two-dimensional space.
The objective of the optimal congestion control algorithm is to bring the system close to the optimal point (C/2, C/2) irrespective of the starting position.
We assume that the system evolves in discrete time, such that at the nthstep the following linear controls are used: Rate Increase Rule riðn11Þ5aI1bIriðnÞi51;2 if the bottleneck is not congested (2) Rate Decrease Rule riðn11Þ5aD1bDriðnÞi51;2 if the bottleneck is congested (3)
Note the following special cases: a I5aD50, b I.1 and 0 ,bD,1 corresponds to multiplicative increase/multiplicative decrease (MIMD) controls.
b I5bD51 corresponds to additive increase/additive decrease (AIAD) controls.
a I50, b D51 corresponds to multiplicative increase/additive decrease (MIAD) controls.
With reference to  , note the following:  All allocations for which r 11r25C are efficient allocations, which correspond to points on the “efficiency line” in  .
All points below the efficiency line represent underloaded systems, and the correct control decision in this region is to increase the rates.
Conversely, all points above the efficiency line represent overloaded systems, and the correct control decision in this region is to decrease the rates.
 All allocations for which r 15r2are fair allocations, which corresponds to points on the “fairness line” in  .
Hence all points on the straight line joining (r 1,r2) to the origin have the same fairness and such a line is called an “equi-fairness” line.
The efficiency line and the fairness line intersect at the point (C/2, C/2), which is the optimal operating point.
The additive increase policy of increasing both users’ allocations by a Icorresponds to moving up along a 45-degree line.
The multiplicative decrease policy of increasing both users’ allocations by bIcorresponds to moving along the line that connects the origin to the current operating point.
shows the trajectory of the two-user system starting from point a R 0using an AIMD policy.
Because R 0is below the efficiency line, the policy causes an additive increase along a 45- degree line that moves the system to point R 1(assuming a big enough increment value), which is above the efficiency line.
This leads to a multiplicative decrease in the next step by moving to point R2toward the origin on the line that joins R 1to
the origin (assuming a big enough decrement value).
Note that R 2has a higher fairness  than R 0.
This increase /C0decrease cycle repeats over and over until the system converges to the optimal state (C/2, C/2).
Using similar reasoning, it can be easily seen that with both the MIMD or the AIAD policies, the system never converges toward the optimal point but keeps oscillating back and forth across the6  INTRODUCTION efficiency line.
Hence, these systems achieve efficiency but not fairness.
The MIAD system, on the other hand, is not stable and causes one of the rates to blow up over time.
Indeed, most of the algorithms for congestion control that are described in this book adhere to the AIMD policy.
This analysis framework can also be extended to nonlinear increase /C0decrease algorithms of the type riðn11Þ5riðnÞ1αðriðnÞÞk(6)
Nonlinear algorithms have proven to be especially useful in the congestion control of high- speed links over large propagation delays, and several examples of this type of algorithm are pre- sented in  of this book.
Two assumptions were made in this analysis that greatly simplified it: (1) the network feedback is instantaneous, and (2) the sources are synchronized with each other and operate in discrete time.
More realistic models of the network that are introduced in later chapters relax these assumptions; as a result, the analysis requires more sophisticated tools.
The simple equations 2 and 3 for the evo- lution of the transmit rates have to be replaced by nonlinear delay-differential equations, and the proof of system stability and convergence requires sophisticated techniques from control theory such as the Nyquist criterion.
1.2.4 WINDOW-BASED CONGESTION CONTROL Window-based congestion control schemes have historically preceded rate-based schemes and, thanks to the popularity of TCP, constitute the most popular type of congestion control scheme in use today.
As shown in  , a window whose size can be set either in bytes or in multiples of the packet size drives the basic operation.
Assuming the latter and a window size of N packets, the system operates as follows:  Assume that the packets are numbered sequentially from 1 and up.
When a packet gets to the destination, an ACK is generated and sent back to the source.
The ACK carries the sequence number (SN) of packet that it is acknowledging.
The source maintains two counters, Ack’d and Sent.
Whenever a packet is transmitted, the Sent counter is incremented by 1, and when an ACK arrives from the destination, the Ack’d counter is incremented by 1.
Because the window size is N packets, the difference un_ackd 5 (Sent /C0Ack’d) is not allowed to exceed N. When unAck’d 5N, then the source stops transmitting and waits for more ACKs to arrive.
 ACKs can be of two types: An ACK can either acknowledge individual packets, which is known as Selective Repeat ARQ, or it can carry a single SN that acknowledges multiple packets with lower SNs.
The latter strategy is called Go Back N. This simple windowing scheme with the Go Back N Acking strategy is basically the way in which TCP congestion control operated before 1988.7 1.2BASICS OF CONGESTION CONTROL  illustrates the mechanism by which a simple static window scheme is able to react to con- gestion in the network.
This served as the primary means of congestion control in the Internet before the introduction of the TCP Slow Start algorithm.
Data ACKs  Self-clocking property.8  INTRODUCTION in its queue.
As a result, the source sending rate gets autoregulated to the rate at which the bottleneck node can serve the packets from that source.
This is also known as the “self-clocking” property and is one of the benefits of the window-based congestion control over the rate-based schemes.
The window-based scheme also functions as an effective flow control algorithm because if the destination is running out of buffers, then it can withhold sending ACKs and thereby reduce the rate at which packets are being transmitted in the forward direction.
Readers will notice that there are a number of potential problems with the static window con- gestion control algorithm.
In particular: 1.When the algorithm is started, there is no mechanism to regulate the rate at which the source sends packets into the network.
Indeed, the source may send a whole windowfull of packets back to back, thus overwhelming nodes along the route and leading to lost packets.
When the ACKs start to flow back, the sending rate settles down to the self-clocked rate.
2.Given a window size of W and a round trip time of T, the average throughput R is given by the formula R 5W/T in steady state (this is explained in ).
If the network is congested at a node, then the only way to clear it is by having the sources traversing that node reduce their sending rate.
However, there is no way this can be done without reducing the corresponding window sizes.
If we do introduce a mechanism to reduce window sizes in response to congestion, then we also need to introduce a complementary mechanism to increase window sizes after the congestion goes away.
3.If the source were to vary its window size in response to congestion, then there needs to be a mechanism by which the network can signal the presence of congestion back to the source.
Partially as a result of a lack of mechanism to address the first two issues, the Internet suffered a so-called “congestion collapse” in 1986, during which all traffic ground to a halt.
As a result, a number of proposals made by Van Jacobsen and Karels [1]were adopted that addressed these shortcomings.
These are described in detail in  .
1.2.5 RATE-BASED CONGESTION CONTROL Instead of using a window to regulate the rate at which the source is sending packets into the net- work, the congestion control algorithm may alternatively release packets using a packet rate control mechanism.
This can be implemented by using a timer to time the interpacket intervals.
However, because ABR was not widely implemented in even ATM networks, there is less practical experience with rate-based schemes than with dynamic window schemes.
More recently, rate-based control has been used for the TCP Friendly Rate Control (TFRC) algorithm, which is used for streaming applications.
Video streaming was thought to be an area in which rate-based control fits in more naturally than window-based control.
However, even for video, the industry has settled on a scheme that uses TCP for congestion control for reasons that are explained in .
Rate control has been used in recent years in non-TCP contexts, and a couple of examples of this are provided in this book: The IEEE802.1Qau algorithm, also known as Quantum Congestion Notification (QCN;9 1.2BASICS OF CONGESTION CONTROL see ), uses rate-based control, and so does the Google Congestion Control (GCC; see ) algorithm that is used for real-time interbrowser communications.
Compared with window-based schemes, rate-based schemes have the following disadvantages: 
Window-based schemes have a built-in choke-off mechanism in the presence of extreme congestion when the ACKs stop coming back.
However, in rate-based schemes, there is no such cut-off, and the source can potentially keep pumping data into the network.
 Rate-based schemes require a more complex implementation at both the end nodes and in the network nodes.
For example, at the end node, the system requires a fine-grained timer mechanism to control the rates.
1.2.6 END-TO-END VERSUS HOP-BY-HOP CONGESTION CONTROL
In the discussion thus far, we have implicitly assumed that the congestion control mechanism exists on an end-to-end basis (i.e., other than providing congestion feedback, the network nodes do not take any other action to relieve the congestion).
However, there exists an alternate design called hop-by-hop congestion control, which operates on a node-by-node basis.
Such a scheme was actu- ally implemented as part of the IBM Advanced Peer to Peer Network (APPN) architecture
[4]and was one of the two competing proposals for the ATM ABR congestion control algorithm
[3]. Hop-by-hop congestion control typically uses a window-based congestion control scheme at each hop and operates as follows: If the congestion occurs at the nthnode along the route and its buffers fill up, then that node stops sending ACKs to the (n-1)rstnode.
Because the (n-1)rstnode can no longer transmit, its buffers will start to fill up and consequently will stop sending ACKs to the (n-2)ndnode.
This process continues until the backpressure signal reaches the source node.
Some of the benefits of hop-by-hop congestion control are:  Hop-by-hop schemes can do congestion control without letting the congestion deteriorate to the extent that packets are being dropped.
Individual connections can be controlled using hop-by-hop schemes because the window scheme described earlier operated on a connection-by-connection basis.
Hence, only the connections that are causing congestion need be controlled, as opposed to all the connections originating at the source node or traversing a congested link.
Note that these benefits require that the hop-by-hop window be controlled on a per-connection basis.
This can lead to greater complexity of the routing nodes within the network, which is the reason why hop-by-hop schemes have not found much favor in the Internet community, whose design philosophy has been to push all complexity to the end nodes and keep the network nodes as simple as possible.
The big benefit of hop-by-hop schemes is that they react to congestion much faster than end-to-end schemes, which typically take a round trip of latency or longer.
The rise of DCNs, which are very latency sensitive, has made this feature more attractive, and as seen in , some recent data center /C0oriented congestion control protocols feature hop-by-hop congestion con- trol.
Also, hop-by-hop schemes are attractive for systems that cannot tolerate any packet loss.
An example of this is the Fiber Channel protocol used in Storage Area Networks (SAN), which is a user of hop-by-hop congestion control.10  INTRODUCTION 1.2.7 IMPLICIT VERSUS EXPLICIT INDICATION OF NETWORK CONGESTION
The source can either implicitly infer the existence of network congestion, or the network can send it explicit signals that it is congested.
Both of these mechanisms are used in the Internet.
Regular TCP is an example of a protocol that implicitly infers network congestion by using the information con- tained in the ACKs coming back from the destination, and TCP Explicit Congestion Notification (ECN) is an example of a protocol in which the network nodes explicitly signal congestion. 1.3DESCRIPTION OF TCP RENO
Using the taxonomy from  , TCP congestion control can be classified as a window-based algorithm that operates on an end-to-end basis and uses implicit feedback from the network to detect congestion.
The initial design was a simple fixed window scheme that is described in Sections 1.3.1 and 1.3.2 .
This was found to be inadequate as the Internet grew, and a number of enhancements were made in the late 1980s and early 1990s (described in Sections 1.3.3 and 1.3.4 ), which culminated in a protocol called TCP Reno, which still remains one of the most widely imple- mented congestion control algorithms today.
1.3.1 BASIC TCP WINDOWING OPERATIONS TCP uses byte-based sequence numbering at the transport layer.
The sender maintains two counters: 
Transmit Sequence Number (TSN) is the SN of the next byte to be transmitted.
When a data packet is created, the SN field in the TCP header is filled with the current value of the TSN, and the sender then increments the TSN counter by the number of bytes in the data portion of the packet.
The number of bytes in a packet is equal to Maximum Segment Size (MSS) and can be smaller if the application runs out of bytes to send.
Ack’d Sequence Number (ASN) is the SN of the next byte that the receiver is expecting.
The ASN is copied from the ACK field of the last packet that is received.
To maintain the window operation, the sender enforces the following rule: ðSN2ASN Þ#W (7) where W is the transmit window size in bytes and SN is the sequence number of the next packet that sender wishes to transmit.
If the inequality is not satisfied, then the transmit window is full, and no more packets can be transmitted.
The receiver maintains a single counter: 
Next Sequence Number (NSN): This is the SN of the next byte that the receiver is expecting.
If the SN field of the next packet matches the NSN value, then the NSN is incremented by the number of bytes in the packet.
If the SN field in the received packet does not match the NSN value, then the NSN is left unchanged.
This event typically happens if a packet is lost in transmission and one of the following packets makes it to the receiver.11 1.3DESCRIPTION OF TCP RENO
The receiver inserts the NSN value into the ACK field of the next packet that is being sent back.
Note that ACKs are cumulative, so that if one of them is lost, then the next ACK makes up for it.
Also, an ACK is always generated even if SN of a received packet does not match the NSN.
In this case, the ACK field will be set to the old NSN again.
This is a so-called Duplicate ACK, and as discussed in  , the presence of Duplicate ACKs can be used by the transmitter as an indicator of lost packets, and indirectly, network congestion.
ACKs are usually piggybacked on the data flowing back in the opposite direction, but if there is no data flowing back, then the receiver generates a special ACK packet with an empty data field.
To reduce the number of ACK packets flowing back, most receiver implementations start a 200-msec timer on receipt of a packet, and if another packet arrives within that period, then a sin- gle ACK is generated.
This rule, which is called Delayed ACKs, cuts down the number of ACKs flowing back by half.
The receiver also implements a receive window (RW), whose value is initially set to the number of bytes in the receive buffer.
If the receiver has one or more packets in the receive buffer whose total size is B bytes, then it reduces the RW size to RW 5(RW2B).
All ACKs generated at the receiver carry the current RW value, and on receipt of the ACK, the sender sets the transmit win- dow to W 5min(W, RW).
This mechanism, which was referred to as flow control in  , allows the receiver with insufficient available buffer capacity to throttle the sender.
Note that even though TCP maintains byte counters to keep track of the window sizes, the con- gestion control algorithms to be described next are often specified in terms of packet sizes.
The packet that is used in these calculations is of size equal to the TCP packet size.
1.3.2 WINDOW SIZE DECREMENTS, PART 1: TIMER OPERATION AND TIMEOUTS The original TCP congestion control algorithm used a retransmit timer as the sole mechanism to detect packet losses and as an indicator of congestion.
This timer counts in ticks of 500-msec incre- ments, and the sender keeps track of the number of ticks between when a packet is transmitted and when its gets ACKed.
If the value of the timer exceeds the value of the retransmission timeout (RTO), then the sender assumes that the packet has been lost and retransmits it.
The sender also takes two other actions on retransmitting a packet: 
The sender doubles the value of RTO and keeps doubling if there are subsequent timeouts for the same packet until it is delivered to the destination.
Choosing an appropriate value for RTO is critical because if the value is too low, then it can cause unnecessary timeouts, and if the value is too high, then the sender will take too long to react to network congestion.
The current algorithm for RTO estimation was suggested by Jacobsen and Karels
[1]and works as follows:
Define the following: M: Latest measurement of the round trip latency value (RTT).
The sender only measures the RTT for one packet at a time.
When a timeout and retransmission occur on the packet being used for measurement, then the RTT estimators are not updated for that packet.
The smoothed RTT estimator12  INTRODUCTION D: The smoothed mean deviation of the RTT g, h: Smoothing constants used in the estimate of A and D, respectively.
The calculations are as follows: Err5M2A A’A1gErr D’D1hðjErrj2DÞ RTO5A14D(8)
Before Jacobsen and Karels’
[1]work, the RTO value was set to a multiple of A alone, which underestimated the timeout value, thus leading to unnecessary retransmissions.
1.3.3 WINDOW SIZE INCREMENTS: SLOW START AND CONGESTION AVOIDANCE
1.3.3.1 Slow Start The windowing algorithm described in  assumed static windows.
Among other pro- blems, this led to the scenario whereby on session start-up, a whole window full of packets would be injected at back to back into the network at full speed.
This led to packet drops as buffers became overwhelmed.
To correct this problem, Jacobsen and Karels
[1]proposed the Slow Start algorithm, which operates as follows: 
At the start of a new session, set the TCP transmit window size to W 51 packet (i.e., equal to MSS bytes).
The sender sends a packet and waits for its ACK.
Every time an ACK is received, increase the window size by one packet (i.e., W ’W11).
Even though the algorithm is called Slow Start, it actually increases the window size quite rapidly: W 51 when the first packet is sent.
When the ACK for that packet comes back, W is doubled to 2, and two packets are sent out.
When the ACK for the second packet arrives, then W is set to 3, and the ACK for the third packet causes W to increase to 4.
Hence, during the Slow Start phase, the correct reception of each windowfull of packets causes the W to double (i.e., the increase is exponential)
The rapid increase is intentional because it enables the sender to quickly increase its bandwidth to the value of the bottleneck capacity along its path, assuming that the maximum window size is large enough to achieve the bottleneck capacity.
If the maximum window size is not large enough to enable to connection to get to the bottleneck capacity, then the window stops incrementing when the maximum window size is reached.
Also note that during the Slow Start phase, the receipt of every ACK causes the sender to inject two packets into the net- work, the first packet because of the ACK itself and second packet because the window has increased by 1. 1.3.3.2 Congestion Avoidance During normal operation, the Slow Start algorithm described in  is too aggressive with its exponential window increase, and something more gradual is needed.
The congestion13 1.3DESCRIPTION OF TCP RENO avoidance algorithm, which was also part of Van Jacobsen and Karels’
[1]1988 paper, leads to a linear increase in window size and hence is more suitable.
It operates as follows:
1.Define a new sender counter called ssthresh.
At the start of the TCP session, ssthresh is initialized to large value, usually 64 KBytes.
2.Use the Slow Start algorithm at session start, which will lead to an exponential window increase, until the bottleneck node gets to the point where a packet gets dropped (assuming that there are enough packets being transmitted for the system to get to this point).
Let’s assume that this happens at W 5Wf.
3.Assuming that the packet loss is detected because of a timer expiry, then the sender resets the window to W 51 packet (i.e., MSS bytes) and sets ssthresh 5Wf/2 bytes.
4.After the session successfully retransmits the lost packet, the Slow Start algorithm is used to increment the window size until W $ssthresh bytes.
Beyond this value, the window is increased using the following rule on the receipt of every ACK packet:
W’W11=W in units of packets (9) 5.In step 3, if the packet gets dropped because of Duplicate ACKs (these are explained in  ) rather than a timer expiry, then a slightly diffe rent rule is used.
After the lost packet is recovered, the sender directly transitions to the congestion avoidanc e phase (because W 5ssthresh) and uses  to increment its windows.
Note that as a result of the window increment rule (  ), the receipt of ACKs for an entire window full of packets leads to an increase in W by just one packet.
Typically, it takes a round trip or more to transmit a window of packets and receive th eir ACKs; hence, under the congestion avoidance phase, the window increases by at most one packet in every round trip (i.e., a linear increase) (see  )
This allows the sender to probe whether the bot tleneck node is getting close to its buffer capacity one packet at a time, and indeed if the extra packet causes the buffer to overflow, then at most one packet is lost, which is easier to recover from.
Slow StartCongestion Avoidance Fast Recovery ssthresh1 ssthresh2 ssthresh3 Triple Duplicate  ACKTimeout TimeWindow Size  Phases of TCP Congestion Control.14  INTRODUCTION 1.3.4 WINDOW SIZE DECREMENTS, PART 2: FAST RETRANSMIT AND FAST RECOVERY  discusses the rules that TCP Reno uses to increment its window in the absence of net- work congestion.
This section discusses the rules that apply to decrementing the window size in the presence of congestion.
TCP uses the design philosophy that dropped packets are the best indicator of network congestion, which falls within the implicit congestion indicator category from  .
When TCP Slow Start was first designed, this was the only feedback that could be expected from the simpler routers that existed at that time.
In retrospect, this decision came with a few issues: The only way that the algorithm can detect congestion is by driving the system into a congestive state (i.e., near the cliff of the curve in  ).
Also, if the packet losses are because of a reason other than congestion (e.g., link errors), then this may lead to unnecessary rate decreases and hence performance loss.
This problem is particularly acute for error-prone wireless links, as explained in .
As alluded to in  , TCP uses two different mechanisms to detect packet loss.
The first technique uses the retransmit timer RTO and is discussed in  .
The second tech- nique is known as Fast Retransmit (  ) and tries to make the system more responsive to packet losses.
It works as follows: 
If the SN field in a packet at the receiver does not match the NSN counter, then the receiver immediately sends an ACK packet, with the ACK field set to NSN.
The receiver continues to do this on subsequent packets until SN matches NSN (i.e., the missing packet is received).
As a result of this, the sender begins to receive more than one ACK with the same ACK number; these are known as duplicate ACKs.
The sender does not immediately jump to the conclusion that the duplicate ACKs are caused by a missing packet because in some cases, they may be caused by packets getting reordered before reception.
However, if the number of duplicate ACKs reaches three, then the sender concludes that the packet has indeed been dropped, and it goes ahead and retransmits the packet (whose SN 5duplicate ACK value) without waiting for the retransmission timer to expire.
This action is known as Fast Retransmit.
 Just before the retransmission, the sender sets the ssthresh value to half the current window size W, and after the retransmission, it sets W ’ssthresh 13.MSS.
Each time another duplicate ACK arrives, the sender increases W by the packet size, and if the new value of W allows, it goes ahead and transmits a new packet.
This action is known as Fast Recovery.
The justification for this is the following: When the receiver finally receives the missing packet, the next ACK acknowledges all the other packets that it has received in the window, which on the average can be half the transmit window size.
When the sender receives this ACK, it is allowed to transmit up to half a window of back-to-back packets (because W is now at half the previous value), which can potentially overwhelm the buffers in the bottleneck node.
However, by doing FAST Recovery, the sender keeps sending additional packets while it waits for the ACK for the missing packet to arrive, and this prevents the sudden onslaught of new packets into the network.
When the ACK for the missing packet arrives, W is set back to the ssthresh value in the previous step.
This behavior can be seen in  ; note that the window size shows a spike of 3 MSS on error detection and falls back to W/2 after the missing packet is received.
The Fast Retransmit mechanism is able to efficiently recover from packet losses as long as no more than one packet is lost in the window.
If more than one packet is lost, then usually the retransmit timer for the second or later expires, which triggers the more drastic step of resetting W back to one packet.15 1.3DESCRIPTION OF TCP RENO 1.3.5 TCP NEW RENO Not long after the introduction of TCP Reno, it was discovered that it is possible to improve its performance significantly by maki
ng the following change to the Fast Recovery phase of the algorithm:
If a Partial ACK is received during the Fast Recove ry period that acknowledges some but not all of the packets that were outstanding at the start of the Fas t Recovery period, then this is treated as an indica- tion that the packet immediately following the acknowledged packet has also been lost and should be retransmitted.
Thus, if multiple packets are lost within a single window, then New Reno can recoverwithout a retransmission timeout by retransmitting one lost packet per round trip until all the lost pack- ets from the window have been retransmitted.
New R eno remains in Fast Rec overy until all data out- standing at the start of the Fast Recovery phase has been acknowledged.
In this book, we will assume that legacy TCP implies Reno, with the understanding that the sys- tem has also implemented the New Reno changes.
1.3.6 TCP RENO AND AIMD Even though it may not be very clear from the description of TCP Reno in the last few sub- sections, this algorithm does indeed fall into the class of AIMD congestion control algorithms.
Thiscan be most clearly seen by writing down the equations for the time evolution of the window size.16 171819202122ACK 16 ACK 16 ACK 16ACK16ACK 16ACK 16 173 Duplicate ACKsX  Illustration of fast retransmit via duplicate ACKs.16  INTRODUCTION In the Slow Start phase, as per the description in  , the window size after n round trip times is given by WðnTÞ52nW0 (10) where W 0is the initial window size.
This equation clearly captures the exponential increase in window size during this phase.
Illustration of Fast Recovery.17 1.3DESCRIPTION OF TCP RENO The AIMD nature of TCP Reno during the congestion avoidance/fast retransmit phases is clearly evident from equations 11 and 12 .
1.4NETWORK FEEDBACK TECHNIQUES Network feedback constitutes the second half of a congestion control algorithm design and has as much influence in determining the properties of the resulting algorithm as the window or rate con- trol performed at the source.
It is implemented using algorithms at a switch or router, and tradition- ally the design bias has been to make this part of the system as simple as possible to keep switch costs low by avoiding complex designs.
This bias still exists today, and even though switch logic is several orders of magnitudes faster, link speeds have increased at the same rate; hence, the cost constraint has not gone away.
1.4.1 PASSIVE QUEUE MANAGEMENT
The simplest congestion feedback that a switch can send back to the source is by simply dropping a packet when it runs out of buffers, which is referred to as tail-drop feedback.
This was how most routers and switches operated originally, and even today it is used quite commonly.
Packet loss triggers congestion avoidance at the TCP source, which takes action by reducing its window size.
Even though tail-drop feedback has the virtue of simplicity, it is not an ideal scheme for the following reasons: 
Tail-drop /C0based congestion signaling can lead to excess latencies across the network if the bottleneck node happens to have a large buffer size.
Note that the buffer need not be full for the session to attain full link capacity; hence, the excess packets queued at the node add to the latency without improving the throughput (this claim is proven in ).
If multiple TCP sessions are sharing the buffer at the bottleneck node, then the algorithm can cause synchronized packet drops across sessions.
This causes all the affected sessions to reduce their throughput, which leads to a periodic pattern, resulting in underutilization of the link capacity.
 Even for a single session, tail-drop feedback tends to drop multiple packets at a time, which can trigger the TCP RTO /C0based retransmission mechanism, with its attendant drop in window size to one packet.
An attempt to improve on this led to the invention of RED-based queue management, which is described in the next section. 1.4.2
ACTIVE QUEUE MANAGEMENT (AQM) To correct the problems with tail-drop buffer management, Floyd and Jacobsen
[5]introduced the concept of AQM, in which the routing nodes use a more sophisticated algorithm called RED to manage their queues.
The main objectives of RED are the following: 
Provide congestion avoidance by controlling the average queue size at a network node.
 Avoid global synchronization and a bias against bursty traffic sources.18  INTRODUCTION 
Maintain the network in a region of low delay and high throughput by keeping the average queue size low while fluctuations in the instantaneous queue size are allowed to accommodate bursty traffic and transient congestion.
As explained later, RED provides congestion feedback to the source when the average queue size exceeds some threshold, either by randomly dropping a packet or by randomly marking the ECN bit in a packet.
Furthermore, the packet dropping probability is a function of the average queue size, as shown in  .
The RED algorithm operates as follows:
For each packet arrival, 1.Calculate the average queue size avg as avg’ð12wqÞavg1wqq, where q is the current queue size.
The smoothing constant w qdetermines the time constant of this low-pass filter.
This average is computed every time a packet arrives at the node.
2.Define constants min thand max th, such that min this the minimum threshold for the average queue size and max this the maximum threshold for the average queue size (see  ).
If min th#avg#max th, then on packet arrival, calculate the probability p band p aas pb’max pðavg2min thÞ ðmax th2min thÞ pa’pb ð12count UpbÞ(13) where max pis the maximum value for p bandcount is the number of packets received since the last dropped packet.
Drop the arriving packet with probability p a. Note that p ais computed such that after each drop, the algorithm uniformly chooses one of the next 1/p b packets to drop next.
minthmaxthmaxp1.0 Average queue sizepb  Packet discard probability in Random Early Detection (RED).19 1.4NETWORK FEEDBACK TECHNIQUES This randomized selection of packets to be dropped is designed to avoid the problem of synchronized losses that is seen in tail-drop queues.
3.Ifavg$max th, then drop the arriving packet.
RED has been heavily analyzed, with the object ive of estimating the optimal queue thresh- olds and other parameters.
Some of this work is covered in , where tools from optimal control theory are brought to bear on the problem.
In general, researchers have discovered that RED is not an ideal AQM scheme because there are issues with its responsiveness and ability to suppress queue size oscillations as the link capacity or the end-to-end latency increases.
Choosing a set of RED parameters that work well across a wide range of network conditions is also a nontrivial problem, which has led to proposals that adapt the parameters as network con- ditions change.
However, it is widely implemented in routers today, and furthermore, all the analysis work done on RED has led to the discovery of other more effective AQM schemes.
Floyd [6]also went on to design a follow on enhancement to RED called ECN.
In ECN systems, the queue does not discard the packet but instead marks the ECN bits in TCP packet header.
The receiver then receives the marked packets and in turn marks the ECN bit in the return- ing ACK packet.
When the sender receives ACKs with marked packets, then it reacts to them in the same way as if it had detected a lost packet using the duplicate ACK mechanism and proceeds to reduce its window size.
This avoids unnecessary packet drops and thus unnecessary delays for packets.
It also increases the responsiveness of the algorithm because the sender does not have to wait for three duplicate ACKs before it can react to congestion.
1.5DELAY-BASED CONGESTION CONTROL: TCP VEGAS Brakmo and Peterson
[7]proposed a fundamentally different solution to the congestion control problem compared with TCP Reno.
Their algorithm, which they called TCP Vegas, uses end-to-end packet delay as a measure of congestion rather than packet drops.
TCP Vegas estimates the level of congestion in the network by calculating the difference in the expected and actual data rates, which it then uses to adjust the TCP window size.
Assuming a win- dow size of W and a minimum round trip latency of T seconds, the source computes an expected throughput R Eonce per round trip delay, by RE5W T(14)20  INTRODUCTION
The source also estimates the current throughput R by using the actual round trip time T s according to R5W Ts(15)
The source then computes the quantity Diffgiven by Diff5TðRE2RÞ (16)
Note that Diff can also be written as Diff5W TsðTs2TÞ5RðTs2TÞ (17)
By Little’s law [8], the expression on the RHS equals the number of packets belonging to the connection that are queued in the network and hence serves as a measure of congestion.
Define two thresholds αandβsuch that α,β.
The window increase decrease rules are given by: 
When α#Diff#β, then leave W unchanged.  When Diff.β, decrease W by 1 for the next RTT.
This condition implies that congestion is beginning to build up the network; hence, the sending rate should be reduced.
When Diff,α, increase W by 1 for the next RTT.
This condition implies the actual throughput is less than the expected throughput; hence, there is some danger that the connection may not use the full network bandwidth.
TCP Vegas tries to maintain the “right” amount of queued data in the network.
Too much queued data will cause congestion, and too little queued data will prevent the connection from rapidly taking advantage of transient increases in available network bandwidth.
Note that this mechanism also eliminates the oscillatory behavior of TCP Reno while reducing the end-to-end latency and jitter because each connection tends to keep only a few packets in the network.
A few issues with TCP Vegas were pointed out by Mo and colleagues
[9], among them the fact that when TCP Vegas and Reno compete for bandwidth on the same link, then Reno ends with a greater share because it is more aggressive in grabbing buffers, but Vegas is conservative and tries to occupy as little space as it can.
Also, if the connection’s route changes (e.g., because of mobil- ity), then this causes problems with Vegas’ round trip latency estimates and leads to a reduction in throughput.
Because of these reasons, especially the first one, Vegas is not widely deployed despite its desirable properties.
1.6OUTLINE OF THE REST OF THE BOOK The contents of this book are broadly split in to two parts:
Part 1 is devoted to the analysis of models of congestion control systems.
has a detailed discussion of TCP models, starting from simple models using fluid approximations to more sophisticated models using stochastic theory.
The well-known “square root” formula for the throughput of a TCP connection is derived and is further generalized21 1.6OUTLINE OF THE REST OF THE BOOK to AIMD congestion control schemes.
These formulae have proven to be very useful over the years and have formed an integral part of the discovery of new congestion control protocols, as shown inPart 2.
For example, several high-speed protocols discussed in  use techniques from to analyze their algorithms.
also analyzes systems with multiple parallelTCP connections and derives expressions for the throughput ratio as a function of the round trip latencies.
has more advanced material in which optimization theory and optimal control theory are applied to a differential equation /C0based fluid flow model of the packet network.
This results in the decomposition of the global optimization problem into independent optimizations at eachsource node and the explicit derivation of the optimal source rate control rules as a function of anetwork wide utility function.
In the case of TCP Reno, the source rate control rules are known,but then the theory can be used to derive the network utility function that Reno optimizes.
In addi-tion to the mathematical elegance of these results, the results of this theory have been used recently to obtain optimal congestion control algorithms using machine learning techniques (see discussion of Project Remy in ).
The system stability analysis techniques using the Nyquist criterionintroduced in  have become an essential tool in the study of congestion control algorithmsand are used several times in Part 2 of this book.
By using a linear approximation to the delay-differential equation describing the system dynamics, this technique enables us to derive explicitconditions on system parameters to achieve a stable system.
Part 2 of the book describes congestion control protocols used in five different application area, namely broadband wireless access networks, high-speed networks with large latencies, videotransmission networks, DCNs, and Ethernet networks.
discusses congestion control in broadband wireless networks.
This work was moti- vated by finding solutions to the performance problems that TCP Reno has with wireless links.
Because the algorithm cannot differentiate between congestion losses and losses caused by linkerrors, decreasing the window size whenever a packet is lost has a very detrimental effect onperformance.
The chapter describes TCP Westwood, which solved this problem by keeping an esti-mate of the data rate of the connection at the bottleneck node.
Thus, when Duplicate ACKs are received, the sender reduces its window size to the transmit rate at the bottleneck node (rather than blindly cutting the window size by half, as in Reno).
This works well for wireless links because ifthe packet was lost because of wireless errors, then the bottleneck rate my still be high, and this isreflected in the new window size.
The chapter also describes techniques, such as Split-ConnectionTCP and Loss Discrimination Algorithms, that are widely used in wireless networks today.
Thecombination of TCP at the end-to-end transport layer and link layer retransmissions (ARQ) or for-ward error correction (FEC) coding on the wireless link is also analyzed using the results from.
The large variation in link capacity that is observed in cellular networks has led to a problem called bufferbloat.
describes techniques using AQM at the nodes as well as end-to-end algorithms to solve this problem.
explores congestion control in high-speed networks with long latencies.
One of the consequences of the application of control theory to TCP congestion control, described in, was the realization that TCP Reno was inherently unstable as the delay-bandwidth prod-uct of the network became large or even for very large bandwidths.
As a result of this, a number ofnew congestion control designs were suggested with the objective of solving this problem such as22  INTRODUCTION High Speed TCP (HSTCP), TCP BIC, TCP CUBIC, and Compound TCP (CTCP), which are described in the chapter.
Currently, TCP CUBIC serves as the default congestion control algorithmfor Linux servers and as a result is as widely deployed as TCP Reno.
CTCP is used as the defaultoption for Windows servers and is also very widely deployed.
The chapter also describes the XCPand RCP algorithms that have been very influential in the design of high-speed congestion control protocols.
Finally, the chapter makes connections between the algorithms in this chapter and the stability theory from  and gives some general guidelines to be used in the design of high-speed congestion control algorithms.
covers congestion control for video streaming applications.
With the explosion in video streaming traffic in recent years, there arose a need to protect the network from congestionfrom these types of sources and at the same time ensure good video performance at the client end.
The industry has settled on the use of TCP for transmitting video even though at first cut, it seemsto be an unlikely match for video’s real-time needs.
This problem was solved by a combination of large receive playout buffers, which can smoothen out the fluctuations caused by TCP rate control, and an ingenious algorithm called HTTP Adaptive Streaming (HAS).
HAS runs at the client endand controls the rate at which “chunks” of video data are sent from the server, such that the sendingrate closely matches the rate at which the video is being consumed by the decoder.
describes the work in this area, including several ingenious HAS algorithms for controlling thevideo transmission rate.
discusses congestion control in DCNs.
This is the most current, and still rapidly evolving, area because of the enormous importance of DCNs in running the data centers that under- lie the modern Internet economy.
Because DCNs can form a relatively autonomous region, there is also the possibility of doing a significant departure from the norm of congestion control algorithmsif the resulting performance is worth it.
This has resulted in innovations in congestion control suchas the application of ideas from Earliest Deadline First (EDF) scheduling and even in-network con-gestion control techniques.
All of these are driven by the need to keep the end-to-end latencybetween two servers in a DCN to values that are in the tens of milliseconds or smaller to satisfythe real-time needs of applications such as web searching and social networking.
covers the topic of congestion control in Ethernet networks.
Traditionally, Ethernet, which operates at Layer 2, has left the task of congestion control to the TCP layer.
However recent developments such as the spread of Ethernet use in applications such as SANs has led the network-ing community to revisit this design because SANs have a very strict requirement that no packetsbe dropped.
As a result, the IEEE 802.1 Standards group has recently proposed a congestion controlalgorithm called IEEE802.1Qau or QCN for use in Ethernet networks.
This algorithm uses severaladvances in congestion control techniques described in the previous chapters, such as the use ofrate averaging at the sender, as well as AQM feedback, which takes the occupancy as well as therate of change of the buffer size into account.
discusses three different topics that are at the frontiers of research into congestion control: (1) We describe a project from MIT called Remy, that applies techniques from MachineLearning to congestion control.
It discovers the optimal congestion control rules for a specific (butpartially observed) state of the network, by doing an extensive simulation based optimization ofnetwork wide utility functions that were introduced in .
This is done offline, as part of thelearning phase, and then the resulting algorithm is applied to the operating network.
Remy has beenshown to out-perform human designed algorithms by a wide margin in preliminary tests.23 1.6OUTLINE OF THE REST OF THE BOOK (2) Software Defined Networks or SDNs have been one of the most exciting developments in net- working in recent years.
Most of their applications have been in the area of algorithms and rules that are used to control the route that a packet takes through the network.
However we describe a couple of instances in which ideas from SDNs can also be used to improve network congestion control.
In the first case SDNs are used to choose the most appropriate AQM algorithm at a node, while in the other case they are used to select the best AQM parameters as a function of changing network state.
(3) Lastly we describe an algorithm called Google Congestion Control (GCC), that is part of the WebRTC project in the IETF, and is used for controlling real time in-browser com- munications.
This algorithm has some interesting features such as a unique use of Kalman Filtering at the receiver, to estimate whether the congestion state at the bottleneck queue in the face of a channel capacity that is widely varying.
1.7FURTHER READING The books by Comer
Fall and Stevens [11] are standard  for TCP from the pro- tocol point of view.
The book by Srikant
[12] discusses congestion control from the optimization and control theory point of view, and Hassan and Jain
[13] provide a good overview of applications of congestion control to various domains.
Congestion avoidance and control.
[2] Chiu DM, Jain R. Analysis of increase and decrease algorithms for congestion avoidance in computer networks.
[3] Jain R. Congestion control and traffic management in ATM networks: recent advances and a survey.
[4] Bird R, Brotman C, Case R, Dudley G. Advances in APPN architecture.
[5] Floyd S, Jacobsen V. Random early detection gateways for congestion avoidance.
[6] Floyd S. TCP and explicit congestion notification.
End-to-end congestion avoidance on a global Internet.
[9] Mo J, La RJ, Anantharam V, Walrand J. Analysis and comparison of TCP Reno and Vegas.
Upper Saddle River, NJ: Pearson Prentice Hall; 2006.
TCP/IP illustrated, volume 1: the protocols.
The mathematics of internet congestion control.
[13] Hassan M, Jain R. High performance TCP/IP networking: concepts, issues and solutions.
Upper Saddle River, NJ: Pearson Prentice Hall;
2004.24  INTRODUCTION CHAPTER 2ANALYTIC MODELING OF CONGESTION CONTROL 2.1INTRODUCTION The past 2 decades have witnessed a great deal of progress in the area of analytic modeling of con- gestion control algorithms, and this chapter is devoted to this topic.
This work has led to explicit formulae for the average throughput or window Size of a TCP connection as a function of para- meters such as the delay-bandwidth product, node buffer capacity, and packet drop rate.
More advanced models, which are discussed in , can also be used to analyze congestion control and Active Queue Management (AQM) algorithms using tools from optimization and control theo- ries.
This type of analysis useful for a number of reasons:  By explicitly describing the dependence of congestion control performance on a few key system parameters, they provide valuable guidelines on ways the congestion control algorithm can be improved or adapted to different network conditions.
The analysis of congestion control algorithms provides fundamental insight into their ultimate performance limits.
For example, the development of the theory led to the realization that TCP becomes unstable if the link speed or the round trip delay becomes very large.
This has resulted in innovations in mechanisms that can be used to stabilize the system under these conditions.
A lot of the early work in congestion control modeling was inspired by the team working at the Lawrence Berkeley Lab, who published a number of very influential papers on the simulation-based anal- ysis of TCP congestion control
A number of more theoretical papers [4/C06]followed their work and led to the development of the theory described here.
One of the key insights that came out of this work is the discovery that at lower packet drop rates , TCP Reno’s throughput is inversely proportional to the square root of the drop rate (also known as the squa re-root formula).
This formula was independently arrived at by several researchers
[5],w h o showed that at higher packet drop rates, the thr oughput is inversely proportional to the drop rate.
TCP models can be classified into single connection models and models for a multinode net- work.
This chapter mostly discusses single connection models, but toward the end of the chapter, the text also touches on models of multiple connections passing through a common bottleneck node.
introduces models for congestion control in the network context.
The rest of this chapter is organized as follows: In  , we derive formulae for TCP throughput as a function of various system parameters.
In particular, we consider of influence of the delay-bandwidth product, window size, node buffer size, packet drop rates, and so on.
sider the case of connections with no packet drops (  ),
connections in which all packet drops are attributable to buffer overflow (  ), and connections in which packet drops are attributable to random link errors (  ).
In  , we develop a fluid-flow model based technique for computing the throughput that is used several times later in this book.
We also generalize TCP’s window increment rules to come up with an additive increase/multiplicative decrease (AIMD) system and analyze its system throughput in  . analyzes a more sophisticated model in which the packet loss process features correlated losses.
In  , we consider the case of multiple TCP connections that share a common bottleneck and analyze its performance.
The analysis of AQM schemes such as Random Early Detection (RED) is best pursued using tools from optimal control theory, and this is done in .
The results in  are used extensively in the rest of the book, especially the general proce- dure for deriving formulae for the throughput of congestion control algorithms.
More advanced readers can venture into Sections 2.2.3, 2.4, 2.5, and 2.6 , which cover the topics of mean value analysis, stochastic analysis, and multiple parallel connections.
2.2TCP THROUGHPUT ANALYSIS 2.2.1 THROUGHPUT AS A FUNCTION OF WINDOW SIZE Consider the model shown in  , with a single TCP Reno controlled traffic source passing through a single bottleneck node.
Even though this is a very simplified picture of an actual Data sourceTCP Send congestion controlTCP ReceiveC, T d B Tu ACK Delay  Model for a TCP connection.28  ANALYTIC MODELING OF CONGESTION CONTROL network, Low and Varaiya
[8]have shown that there is no loss of generality in replacing all the nodes along the path by a single bottleneck node.
Define the following: W(t): TCP window size in packets at time t Wmax: Maximum TCP window size Wf: Window size at which a packet is lost MSS: TCP packet size in bytes C: The bottleneck node capacity in packets/sec B: Buffer count at the bottleneck node in packets
Td: Propagation delay in the forward (or downstream) direction in seconds
Tu: Propagation delay in the backward (or upstream) direction in seconds T: Total propagation 1Transmission delay for a segment bmax: The maximum buffer occupancy at the bottleneck node in packets R(t): Throughput of the connection in packets/sec at time t Ravg: Average throughput of a connection in packets/sec Note that T, which is the fixed part of the round trip latency for a connection, is given by T5Td1Tu11 C We first consider the simplest case when the window size is in equilibrium with size W max, there is sufficient buffering at the bottleneck node, so no packets get dropped because of overflows, and the link is error free.
These assumptions are relaxed in the following sections as the sophistication of the model increases.
The diagram illustrates the fact that in this case the sender transmits the entire window fu ll of packets before it receives the ACK for first packet back.
Hence, the average connection throughput R avgis limited by the window size and is given by Ravg5Wmax Twhen W max,CT (1)
Because the rate at which the bottleneck node is transmitting packets is greater than or equal to the rate at which the source is sending packets into the network, the bottleneck node queue size is zero.
Hence, at any instant in time, all the packets are in one of two places: either in the process of being transmitted in the downstream direction ( 5Ravg.Tdpackets), or their ACKs are in the pro- cess of being transmitted in the upstream direction ( 5Ravg.TuACKs).
As the figure illustrates, the ACKs for the first packet in the window arrive at the sender before it has fin- ished transmitting the last packet in that window.
As a result, unlike in  , the sender is continuously transmitting.
The system throughput is no longer limited by the window size, and is able to attain the full value of the link capacity, that is, Ravg5Cwhen W max.
Pk 2 Pk WmaxACK 2ACK 1T 1/C Wmax/C  Packet transmissions with T ,Wmax/C.30  ANALYTIC MODELING OF CONGESTION CONTROL
In this case, C.T dpackets are the process of being transmitted in the downstream direction, and C.T uACKs are in the process of being transmitted in the upstream direction, for a total of C. (Td1Tu)5CT packets and ACKs.
Because CT ,Wmax, this leaves (W max2CT) packets from the TCP window unaccounted for, and indeed they are to be found queued up in the bottleneck node!
Hence, in equilibrium, the maximum buffer occupancy at the bottleneck node is given by bmax5maxðWmax2CT ;0Þ (4)
Using equations 3 and 4 ,Ravgcan also be written as Ravg5Wmax T1bmax C(5)
This shows that the average throughput is given by the window size divided by the total round trip latency, including the queuing delay.
This discussion illustrates the rule of thumb that the optimum maximum window size is given by W max5CT, i.e., it should equal the delay-bandwidth product for the connection because at this point the connection achieves its full bandwidth (equal to the link rate).
Any increase in W max beyond CT increases the length of the bottleneck queue and hence end-to-end delay, without doing anything to increase the throughput.
plots R avgvs W maxbased on  , and as the graph shows, there a knee at Wmax5CT, which is the ideal operating point for the system.
plots the maximum buffer occupancy
b maxas a function of W maxbased on  .
When W maxincreases to B 1CT, then b max5B, and any increase of W maxbeyond
this value will cause the buffer to overflow.
Note that until this point, we have made no assumptions about the spe- cific rules used to control the window size.
Hence, all the formulae in this section apply to any window-based congestion control scheme in the absence of packet drops.
In , we continue the analysis for TCP Reno for the case W max.
B1CT, when packets get dropped because of buffer overflows.
B1CT, then it results in packet drops because of buffer overflows.
In the presence of overflows, the variation of the window size W(t) as a function of time is shown in  for TCP Reno.
We made the following assumptions in coming up with this figure: 1.Comparing  with  in , we assume that we are in the steady phase of a long file transfer, during which the system spends all its time in the congestion avoidance phase, i.e., the initial Slow-Start phase is ignored, and all packet losses are recovered using duplicates ACKs (i.e., without using timeouts).bmax WmaxB CT B+CTbmax = Wmax–CT  Maximum buffer occupancy as a function of W max. W(t) tWf = B + CT Wf/2 tAtBPhase 1: Linear IncreasePhase 2: Sublinear IncreaseFluid Approximation  Variation of window size in steady state for TCP Reno.32  ANALYTIC MODELING OF CONGESTION CONTROL 2.We also ignore the Fast Recovery phase because it has a negligible effect on the overall throughput, and including it considerably complicates the analysis.
3.Even though the window size W increases and decreases in discrete quantities, we replace it by a continuous variable W(t), thus resulting in what is known as a fluid approximation.
This simplifies the analysis of the system considerably while adding only a small error term to the final results.
TCP Reno then goes into the congestion recovery mode, during which it retrans- mits the missing packet and then reduces the window size to W f/2.
Using  ,i fW f/2 exceeds the delay-bandwidth product for the connection, then R avg5C. Note thatWf 2$CTimplies that B1CT 2$CT which leads to the condition B$CT (6)
Hence, if the number of buffers exceeds the delay-bandwidth product of the connection, then the periodic packet loss caused by buffer overflows does not result in a reduction in TCP throughput, which stays at the link capacity C. Combining this with the condition for buffer overflow, we get Wmax2CT$B$CT as the conditions that cause a buffer overflow but do not result in a reduction in throughput.
is a well-known rule of thumb that is used in sizing up buffers in routers and switches.
Later in this chapter, we show that  is also a necessary condition for full link utilization in the presence of multiple connections passing through the link, provided the connections are all synchronized and have the same round trip latency.
When B ,CT, then the reduction in window size to W f/2 on packet loss leads to the situation where all the packets are either in transmission or being ACK’d, thus emptying the buffer periodi- cally, as a result of which the system throughput falls below C (  ).
An analysis of this scenario is carried out next.33 2.2TCP THROUGHPUT ANALYSIS We compute the throughput by obtaining the duration and the number of packets transmitted during each cycle.
The increase of the TCP window size happens in two phases, as shown in  :
In phase 1, W(t) ,CT, which implies that the queue at the bottleneck node is empty during this phase.
Hence, the rate at which ACKs are returning to the sender is equal to the TCP throughput R(t)5W(t)/T.
This implies that the rate at which the window is increasing is given by dWðtÞ dt5dW daUda dt51 WðtÞUWðtÞ T51 T(7) where a(t) is the number of ACKs received in time t and da/dt is the rate at which ACKs are being received.
Hence, WðtÞ2Wf 25ðt 0dt T5t T This results in a linear increase in the window size during phase 1, as shown in  .
so that the bottleneck node has a persistent backlog.
Hence, the rate at which ACKs are returning back to the sender is given by C.
tB  Window and buffer occupancy evolution for B ,CT.35 2.2TCP THROUGHPUT ANALYSIS Hence, ðW CTWðtÞUdW5ðt 0CUdt W2ðtÞ2ðCTÞ252Ct so that WðtÞ5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2Ct1ðCTÞ2q (11)  implies that the increase in window size during phase 2 happens in a sublinear fashion, as illustrated in  .
Because of the persistent backlog at the bottleneck node, the number of packets n Btransmitted during phase 2 is given by nB5CtB5W2 f2ðCTÞ2 2(13)
The average TCP throughput is given by Ravg5nA1nB tA1tB(14)
Thus, the number of packets transmitted during a cycle is proportional to the square of the maximum window size, which is an interesting result in itself and is used several times in Part 2 of this book.
Later in this chapter, we will see that this relation continues to hold even for the case when the packet losses are because of random link errors.
If B exceeds CT, then the throughput converges to C as expected.
However, the fact that the throughput is able to get up to 0.75C even for very small buffer sizes is interesting and has been recently used in the design of the CoDel AQM Scheme (see ).
2.2.3 THROUGHPUT AS A FUNCTION OF LINK ERROR RATE In the presence of random link errors, TCP may no longer be able to attain the maximum window size of B 1CT; instead, the window increases to some value that is now a random variable.
This is clearly illustrated in  , with the maximum window and consequently the starting window varying randomly.
To analyze this case, we have to resort to probabilistic reasoning and find the expected value of the throughput.
In  , we consider the case when the lost packets are recovered using the duplicate ACKs mechanism, and in  , we consider the more com- plex case when packet losses result in timeouts.
To take the random losses into account, we need to introduce a second model in to the system, that of the packet loss process.
The simplest case is when packet losses occur independently of one another with a loss probability p; this is the most common loss process used in the literature.
ANALYTIC MODELING OF CONGESTION CONTROL Wireless links lead to losses that tend to bunch together because of a phenomenon known as fading.
In , we analyze a system in which the packet loss intervals are correlated, which can be used to model such a system.
The type of analysis that is carried out in the next section is known as mean value analysis (MVA) in which we work with the expectations of the random variables.
This allows us to obtain closed form expressions for the quantities of interest.
There are three other alternative models that have been used to analyze this system:  Fluid flow models: The fluid flow model that is introduced in  can also be extended to analyze the system in the presence of random losses, and this is discussed in  .
 Markov chain models [9,10] : These work by defining a state for the system that evolves according to a Markov process, with the state transition probabilities derived as a function of the link error rate.
The advantage of this approach is that it is possible to obtain the full distribution of the TCP window size, as opposed to just the mean value.
However, it is not possible to get closed form solutions for the Markov chain, which has to be solved numerically.
[6]: These models replace the simple iid model for packet loss by a more sophisticated stochastic process model.
As a result, more realistic packet loss models, such as bursty losses, can also be analyzed.
We describe such a model in  .
2.2.3.1 Link Errors Recovered by Duplicate ACKs The TCP model used in this section uses discrete states for the window size, as opposed to the fluid model used in  [5] .
We define a cycle as the time between two loss events ( ) and a subcycle as the time required to transmit a single window of packets followed by reception of their ACKs.
In addition to assumptions 1 and 2 from  , we make the following additional assumptions: 4.All the packets in a window are transmitted back to back at the start of a subcycle, and no other packets are sent until the ACK for the first packet is received, which marks the end of the subcycle and the start of the next one.
5.The time needed to send all the packets in a window is smaller than the round trip time.
This assumption is equivalent to stating that the round trip time T(t) is fixed at T(t) 5T and equals the length of a subcycle.
This is clearly an approximation because we observed in  that the T(t) increases linearly after the window size exceeds CT because of queuing at the bottleneck node.
Another consequence of this assumption is that the window increase process stays linear throughout, so we are approximating the sublinear increase for larger window sizes shown in  (which is caused by the increase in T(t)), by a straight line.
6.Packet losses between cycles are independent, but within a cycle when a packet is lost, then all the remaining packets transmitted until the end of the cycle are also lost.
This assumption can be justified on the basis of the back-to-back transmission assumption and the drop-tail behavior of the bottleneck queue.39 2.2TCP THROUGHPUT ANALYSIS Define the following:
Wi: TCP window size at the end of the i thcycle Ai: Duration of the ith cycle Xi: Number of subcycles in the i thcycle Yi: Number of packets transmitted in the i thcycle Ravg: Average TCP throughput p: Probability that a packet is lost because of random link error given that it is either the first packet in its round or the preceding packet in its round is not lost.
As shown in  , cycles are ed using the variable i, and in the ithcycle, the initial TCP window is given by the random variable W i-1/2, and the final value at the time of packet loss is given by W i. Within the ithcycle, the sender transmits up to X iwindows of packets, and the time required to transmit a single window of packets is constant and is equal to the duration of the (con- stant) round trip latency T, so that there are X isubcycles in the ithcycle.
Within each subcycle, the window size increases by one packet as TCP goes through the congestion avoidance phase, so that by the end of the cycle, the window size increases by X ipackets.
We will assume that the system has reached a stationary state, so that the sequences of random variables {W i}, {A i}, {X i} and {Y i} converge in distribution to the random variables W, A, X and Y, respectively.
An-1 AnCycle n-1 Cycle n Cycle n+1Subcycles T  Evolution of TCP window size under random losses.40  ANALYTIC MODELING OF CONGESTION CONTROL
Our objective is to compute R avg, the average throughput.
After a packet is lost, W i/C01 additional packets are transmitted before the cycle ends.
We will ignore these packets, so that Y iequals the number packets transmitted until the first drop occurs.
Under this assumption, the computation of E(Y) is straightforward, given that the number of pack- ets transmitted during a cycle is distributed according to the Bernoulli distribution, that is, PðYi5kÞ5ð12pÞk21p;k51;2;... (22)
Next, note that E(A) is given by EðAÞ5EðXÞUT (24) where T is the fixed round trip delay for the system.
is the well-known “square root” law for the expected throughput of a TCP connec- tion in the presence of random link errors that was first derived by Mathis and colleagues [7].
Our derivation is a simplified version of the proof given by Padhye et al.
[5] 2.2.3.2 Link Errors Recovered by Duplicate ACKs and Timeouts In this section, we derive an expression for the average TCP throughput in the presence of duplicate ACK as well as timeouts .
As in the previous section, we present a simplified version of the analysis in the paper by Padhye and colleagues [5].
As illustrated in  , we now consider a supercycle that consists of a random number of cycles (recall that cycles terminate with a duplicate ACK /C0based packet recovery) and a single timeout interval.
Define the following random variables: ZDA i: Time duration during which missing packets are recovered using duplicate ACK triggered retransmissions in the ithsupercycle.
This interval is made of several cycles, as defined in the previous section.
ni: Number of cycles within a single interval of length ZDA iin the ithsupercycle Yij
: Number of packets sent in the jthcycle of the ithsupercycle42  ANALYTIC MODELING OF CONGESTION CONTROL Xij: Number of subcycles in the jthcycle of the ithsupercycle Aij: Duration of the jthcycle of the ithsupercycle ZTO i: Time duration during which a missing packet is recovered using TCP timeouts Mi: Number of packets transmitted in the i thsupercycle Pi: Number of packets transmitted during the timeout interval ZTO i Si: Time duration of a complete supercycle, such that Si5ZDA i1ZTO i (33)
As in the previous section, we assume that the system is operating in a steady state in which all the random variables defined above have converged in distribution.
We will denote the expectedvalue of these random variables with respect to their limit distribution by appending an E beforethe variable name as before.
Note that the average value of the system throughput R avg, is given by Ravg5EðMÞ EðSÞ(34)
We now proceed to compute the expected values E(M) and E(S).
Ai2Cycle 1 Cycle 2T SiZiDAZiTOSupercycle i T0 2T0 4T0Timeout Interval  Evolution of TCP window size in the presence of duplicate ACKs and timeouts.43 2.2TCP THROUGHPUT ANALYSIS Si5Xni j51Aij1ZTO i (36)
The same packet is transmitted one or more times during the timeout interval ZTO.
Hence a sequence of k TOs occurs when there are (k 21) consecutive losses followed by a successful transmission.
This formula follows from the fact that the value of the timeout is doubled for the first six timer expiries and then kept fixed at 64T 0for further expirations.
For smaller values of p, this can be approximated as Ravg51 Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2 3p1T0pð1132p2Þmin 1 ;3ﬃﬃﬃﬃ 3p 8q/C16/C17r (52)
This formula was empirically validated by Padhye et al.
[5]and was found to give very good agreement with observed values for links in which a significant portion of the packet losses are attributable to timeouts.
2.3A FLUID FLOW MODEL FOR CONGESTION CONTROL
Some of the early analysis work for TCP throughput in the presence of packet drops was done by Floyd
[7], using a simple heuristic model based on the fluid flow model of the45 2.3A FLUID FLOW MODEL FOR CONGESTION CONTROL window size evolution (  ).
This is a simplification of the fluid flow model used in  (see  ) because the model assumes that the window growth is strictly linear within each cycle.
With reference to  , consider a single sample path W(t) of the window size process, such that the value of the maximum window size is W m, so that the value of the window after a packet loss is W m/2.
During the period in which the window size increases from W m/2 to Wm(called a cycle), it increases by 1 for every round trip duration T; hence, it takes W m/2 round trips to increase from W m/2 to W m.
This implies that the length of a cycle is given by TW m/2 seconds.
Note that the number of packets that are transmitted during a cycle is given by Y5ðTWm=2 0RðtÞdt and after substituting R(t)
It is an interesting coincidence that this formula for the number of packets transmitted in a cycle is identical to the one derived in  (see  ) because in the earlier case, the der- ivation took the nonlinear evolution of the window size into account, but in the current case, we made a simplifying linear approximation.
It follows that the throughput for a single cycle of the sample path is given by R5Y TWm=253 4Wm TW(t)
tWm/2Wm TWm/2  Fluid model for Reno in the congestion avoidance phase.46  ANALYTIC MODELING OF CONGESTION CONTROL
Taking the expectations on both sides, it follows that Ravg51 Tﬃﬃﬃ 3 2r Eﬃﬃﬃﬃ Yp/C16/C17 (55) The usual way of analyzing this system implicitly makes this assumption that the expectation and the square root operations can be interchanged.
Note that this can be done only if Y is a con- stant, say Y 5N, and under this assumption,  reduces to Ravg51 Tﬃﬃﬃ 3 2rﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ EðYÞp 51 Tﬃﬃﬃﬃﬃﬃ 3N 2r (56) Because the system drops one packet of every N that are transmitted, the packet drop rate p is given by p 51/N. Substituting this in  , we obtain Ravg51 Tﬃﬃﬃﬃﬃ 3 2ps (57) and we have recovered the square-root formula.
However, this analysis makes it clear that this equation only holds for the case of deterministic number of packets per cycle and is referred to as the “deterministic approximation” in the rest of this book.
If we do not make the deterministic approximation, then for the case when the packet drop vari- ables are iid with probability p, the number of packets per cycle follows the following geometric distribution. PðY5nÞ5ð12pÞn21pn51;2;...
In the absence of a closed form expression for this sum, the final expression for the iid packet loss case is given by Ravg51 Tﬃﬃﬃﬃﬃ 3p 2rXN n51ﬃﬃﬃnpð12pÞn21(60)
If we approximate the geometric distribution (  ) by an exponential distribution with density function given by fYðyÞ5λe2λyy$0 (61) and to match the two distributions, we set λ5p.
Hence, we have succeeded in recovering the square- root formula using the more exact calculation.
The constant in  computes to 1.08, as opposed to 1.22 in  , so it results in a slightly lower average throughput.
In the calculation done in  , the assumption following  that the {X i} and {W i} sequences are independent also results in implicitly introducing the deterministic assump- tion into the computation, so that the final result coincides with that of the deterministic case.
As we will see later in Part 2, the most important information in the formula for the average throughput is not the constant but the nature of the functional dependence on p and T. Because the deterministic approximation gives the correct functional dependence while using fairly straightfor- ward computations, we will follow the rest of the research community in relying on this technique in the rest of this book.
can be used to derive an expression of the distribution of the maximum TCP window size W m, and this is done next.
Because Wm5ﬃﬃﬃﬃ 8Y 3q , it follows that PðWm#xÞ5PY#3x2 8/C18/C19 Again, using the exponential distribution (  ) for the number of packets per cycle, it follows that PðWm#xÞ512e23px2 8 so that the density function for W mis given by fWmðxÞ53px 4e23px2 8x$0: A GENERAL PROCEDURE FOR COMPUTING THE AVERAGE THROUGHPUT Basing on the deterministic approximation, we provide a procedure for computing the average throughput for a congestion control algorithm with window increase function given by W(t) and48  ANALYTIC MODELING OF CONGESTION CONTROL maximum window size W m.
This procedure is used in the next section as well as several times in Part 2 of this book: 1.Compute the number of packets Y transmitted per cycle by the formula YαðWmÞ51 TðT 0Wðt;αÞdt In this equation, T is the round trip latency, and αrepresents other parameters that govern the window size evolution.
2.Equate the packet drop rate p to Y, using the formula YαðWmÞ51 p If this equation can be inverted, then it leads to a formula for W mas a function of p, that is, Wm5Y21 α1 p/C18/C19 3.Compute the cycle duration τðWmÞfor a cycle as a function of the maximum window size.
4.The average throughput is then given by Ravg51=p τðWmÞ51=p τðY21 αð1 pÞÞ The formula for the window increase function Wðt;αÞcan usually be derived from the window increase-decrease rules.
Note that there are algorithms in which the inversion in step 2 is not feasible, in which case we will have to resort to approximations.
There are several examples of this in .
2.3.1 THROUGHPUT FOR GENERAL ADDITIVE INCREASE/MULTIPLICATIVE DECREASE (AIMD) CONGESTION CONTROL Recall that the window increase /C0decrease rules for TCP Reno can be written as: W’W11per RTT on ACK receipt W 2on packet loss8 < :(65) This rule can be generalized so that the window size increases by a packets per RTT in the absence of loss, and it is reduced by a fraction bW on loss.
This results in the following rules: W’W1a per RTT on ACK receipt ð12bÞW on packet loss/C26 (66)
This is referred to as an AIMD congestion control [11], and has proven to be very useful in exploring new algorithms; indeed, most of the algorithms in this book fall within this framework.
In general, some of the most effective algorithms combine AIMD with increase decrease para- meters (a,b) that are allowed to vary as a function of either the congestion feedback or the window size itself.
Here are some examples from later chapters: 
By choosing the value of a, the designer can make the congestion control more aggressive (if a.1) or less aggressive (if a ,1) than TCP Reno.
The choice a .1 is used in high-speed49 2.3A FLUID FLOW MODEL FOR CONGESTION CONTROL networks because it causes the window to increase faster to take advantage of the available bandwidth.
Protocols such as High Speed TCP (HSTCP), TCP BIC, TCP FAST and TCP LEDBAT use an adaptive scheme in which they vary the value of a depending on the current window size, the congestion feedback, or both.
The value of b influences how readily the connection gives up its bandwidth during congestion.
TCP Reno uses a rather aggressive reduction of window size by half, which causes problems in higher speed links.
By choosing b ,0.5, the reduction in window size is smaller on each loss, which leads to a more stable congestion control algorithm.
But this can lead to fairness issues because existing connections don’t give up their bandwidth as readily.
Using the deterministic approximation technique from the previous section, we derive an expression for the throughput of AIMD congestion control.
With reference to  , the TCP window size reduces from W mto (1-b)W mon packet loss (i.e., a reduction of bW mpackets).
On each round trip, the window size increases by a packets, so that it takes bW m/a round trips for the window to get back to W m. This implies that the length of a cycle is given by τ5TWmb aseconds : The average number of packets transmitted over a single cycle is then given by: Y5ðTWmb
The average response time R avgis then given by Ravg51=p TWmb=a51 Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ að22bÞ 2bps (69)
In , we generalize the notion of AIMD congestion control by introducing nonlineari- ties in the window increase /C0decrease rules, which is called generalized AIMD or GAIMD conges- tion control.
Note that  holds only for the case when the parameters a and b are constants.
If either of them is a function of the congestion feedback or the current window size, it leads to a nonlinear evolution of the window size for which this analysis does not hold.
2.3.2 DIFFERENTIATING BUFFER OVERFLOWS FROM LINK ERRORS
The theory developed in Sections 2.2 and 2.3 can be used to derive a useful criterion to ascertain whether the packet drops on a TCP connection are being caused because of buffer overflows or because of link errors.
When buffer overflows are solely responsible for packet drops, then N packets are transmitted per window cycle, where N is given by  N53W2 f 8: Because one packet is dropped per cycle, it follows that packet drop rate is given by p51 N58 3W2 f: (71)
Because Wf/C25CT(ignoring the contribution of the buffer size to W f), it follows that pðCTÞ2/C258 3(72) when buffer overflows account for most packet drops.
On the other hand, when link errors are the predominant cause of packet drops, then the maxi- mum window size W machieved during a cycle may be much smaller than W f.
This criterion was first proposed by Lakshman and Madhow
[4]and serves as a useful test for the prevalence of link errors on a con- nection with known values for p, C, and T. 2.4A STOCHASTIC MODEL
In this section, following Altman and colleagues [6], we replace the simple packet loss model that we have been using with a more sophisticated loss model with correlations among interloss intervals.
Define the following model for the packet loss process: fτng1N n52N: Sequence of time instants at which packets are lost Sn5τn112τn: Sequence of intervals between loss instants d5EðSnÞ: The average interloss interval UðkÞ5E½SnSn1k/C138: The correlation function for the interloss interval process.
Rn: Value of the TCP throughput just before nthpacket loss at T n Recall that the throughput at time t is given by RðtÞ5WðtÞ Tso thatdR dt51 TdW dt(74)
For TCP Reno, dW/dt 51/T under the assumption that the round trip delay remains constant during a cycle, so that dR dt51 T2(75)
so that R(t) increases linearly with slope 1/T2.
is a stochastic linear differential equation, with a solution given by Brandt
To compute the average throughput R avg, we start with its definition given by Ravg5lim t-N1 tðt 0RðtÞdt52  ANALYTIC MODELING OF CONGESTION CONTROL Using the inversion formula from Palm calculus
The expectation E0 is with respect to the Palm measure P0that is conditioned on sample paths for which τ050.
For the case of deterministic interpacket loss intervals, it follows that ^UðkÞ51k50;1;2;...
so that Ravg51 Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1 p22b 2bs (81) which coincides with  in the previous section.
Substituting b 51/2, we again recover the square-root formula.
implies that a correlation between packet loss intervals leads to an increase in the average throughput.
2.5WHY DOES THE SQUARE-ROOT FORMULA WORK WELL?
From  in the previous section, it follows that increasing the variance or the presence of correlation between the interpacket loss intervals increases the average throughput.
Because in53 2.5WHY DOES THE SQUARE-ROOT FORMULA WORK WELL? practice, interpacket loss intervals have a finite variance and probably are correlated as well, it begs the question of why  work so well in practice even though it was derived under the assumption of deterministic interloss intervals.
Recall that we made the assumption in  that the rate of increase of the conges- tion window is linear, as shown in the dashed curve in  .
This is equivalent to assum- ing that the round trip latency remains constant over the course of the window increase.
This is clearly not the case in reality because the late ncy increases with increasing window size attributable to increasing congestion in the netw ork.
Hence, the actual increase in window size becomes sublinear as shown in the solid curve in  ; indeed, we analyzed this model in  .
We will assume that the bottleneck node for both cases has the same buffer size, so that both of these windows increase to the same value W mat the end of their cycles.
Let R Lbe the throughput of the linear model and let R NLbe the throughput of the nonlinear model.
It follows that RL5NL tLand RNL5NNL tNL where N Lis the number of packets transmitted in the linear model and N NLis the number for the nonlinear model.
In equations 15 and 53 , we computed N NLand N L, respectively, and it turns out that N NL5NL.
However because t L,tNL, it follows that R L.RNL.
Hence, the reason why the lin- ear model with deterministic number of packets per cycle works well is because of the cancellation of the following two error sources: R L.RNLdue to the linear increase assumption, as explained earlier 
We will con- sider two cases: I n  , we discuss the homogeneous case when all connections have the same round trip latency.
I n  , we discuss the heterogeneous case in which the round trip latencies are different.
In general, the presence of multiple connections increases the complexity of the model consider- ably, and in most cases, we will have to resort to approximate solutions after making simplifying assumptions.
However, this exercise is well worth the effort because it provides several new insights in the way TCP functions in real-world environments.
The analysis in this section assumes that there are no packet drops due to link errors so that all losses are attributable to buffer overflows.
Consider the following system: There are K TCP c onnections passing thr ough the bottleneck link.
The total window size attributable to all the K sessions.
The constants W max, C, and B and the variable b(t) are defined as in  .
2.6.1 HOMOGENEOUS CASE We will initially consider the homogeneous case in which the propagation 1transmission delays for all the sessions are equal (i.e., T 5Ti5Tj,i ,j51, 2, ...,K).
The basic assumption that is used to simplify the analysis of this model is that of session syn- chronization.
This means that the window size variations for all the K sessions happen together (i.e., their cycles start and end at the same time).
This assumption is justified from observations made by Shenker et al.
[3]in which they observed that tail drop causes the packet loss to get syn- chronized across all sessions.
This is because packets get dropped because of buffer overflow at the bottleneck node; hence, because typically multiple sessions encounter the buffer overflow condition at the same time, their window size evolution tends to get synchronized.
This also results in a bursty packet loss pattern in which up to K packets are dropped when the buffer reaches capacity.
This implies that dWiðtÞ dt5dWjðtÞ dt’i;j (82) because the rate at which a window increases is determined by the (equal) round trp latencies.
ANALYTIC MODELING OF CONGESTION CONTROL
As in  , the time evolution of the total window W(t) shows the characteristic saw- tooth pattern, with the sum of the window sizes W(t) increasing until it reaches the value Wf5B1CT.
This results in synchronized packet losses at all the K connections and causes W(t) to decrease by half to W f/2.
This condition holds for a network with a small delay-bandwidth product.
In this case, the bottleneck queue is always occupied, hence RðtÞ5Cand RiðtÞ5C Ki51;2;...;K
This condition holds for a network with a large delay-bandwidth product.
(88) Note that the total throughput Ravgis independent of the number of connections K.  also implies that as a result of the lack of buffers in the system and the synchronization of the TCP sessions, the link does not get fully used even with a large number of sessions.
Case 1 also leads to the interesting result that for the fully synchronized case, the amount of buffering required to keep the link continuously occupied is given by CT and is independent of the number of connections passing through the link.
Hence, for the case when the bottleneck link is fully used, we have XK i51RiðtÞ5C; so that XK i51WiðtÞ bðtÞ1CTi51 (90)57 2.6THE CASE OF MULTIPLE PARALLEL TCP CONNECTIONS
The following equation follows by reasoning similar to that used to derive  .
Following the analysis in  , the average throughput for the ithsession is given by Ravg i53 8ðWf iÞ2 tA i1tB ii51;2;...;K
(92) where, as before, tA iis the average time interval after a packet loss when the buffer is empty, and tB iis the average time interval required for the buffer to fill up.
In general, it is difficult to find closed form solutions for W i(t) and an expression for Wf ifrom these equations, even if synchronization among sessions is assumed.
Hence, following Bonald [14], we do an asymptotic analysis, which provides some good insights into the problem.
1.The case of large buffers Define βi5B CTi,i51,2, ...,K and assume that βiis very large, that is, the delay bandwidth product for all sessions is negligible compared with the number of buffers in the bottleneck node.
This means that the bottleneck buffer is continuously occupied, so that tA i50.
Hence, when the number of buffers in the bottleneck node is large, each connection gets a fair share of the capacity despite any differences in propagation delays.
2.The case of large delay-bandwidth product We assume that βi,i51,2, ...,K is very small, that is, the delay-bandwidth product for each session is very large compared with the number of buffers in the bottleneck node.
We also make the assumption that the K connections are synchronized, so that they drop their packets at the bottleneck node at the same time.
(96)58  ANALYTIC MODELING OF CONGESTION CONTROL Hence, for large delay-bandwidth products, TCP Reno with tail drop has a very significant bias against connections with larger propagation delays.
At first glance, it may seem that  contradicts the formula for the average throughput (60) because the latter shows only an inverse relationship with the round trip latency T. However, note that by making the assumption that the K sessions are synchronized, their packet drop probabilities p become a function of T as well, as shown next.
Using heuristic agreements,  can be extended to the case when the average through- put is given by the formula: Ravg5h Tepd(100) where h, e, and d are constants.
Equations such as  for the average throughput arise commonly when we analyze congestion control algorithms in which the window increase is no lon- ger linear (see  for several examples of this).
Substituting  into  leads to  .
This equation was earlier derived by Xu et al.
It has proven to be very useful in analyz- ing the intraprotocol fairness properties of congestion protocols, as shown in .
is critically dependent on the assumption that the packet drops are synchronized among all connections, which is true for tail-drop queues.
A buffer management policy that breaks this synchronization leads to a fairer sharing of capacity among connections with unequal latencies.
One of the objectives of RED buffer management was to break the synchronization in packet drops by randomizing individual packet drop decisions.
Simulations have shown that RED indeed achieves this objective, and as a result, the ratio of the throughputs of TCP connections using RED varies as the first power of their round trip latencies rather than the square.
READING TCP can also be analyzed by embedding a Markov chain at various points in the window size evo- lution and by using the packet loss probability to control the state transitions.
The stationary distri- bution of the window size can then be obtained by numerically solving the resulting equations.
This program has been carried out by Casetti and Meo
[9], Hasegawa and Murata
Because this technique does not result in closed form expressions, the impact of various para- meters on the performance measure of interest are not explicitly laid out.
[17] carry out an analysis of the congestion control system with multiple connections and RED by setting up a discrete time recursion equation between quan- tities of interest and then show that by using limit theorems the equations converge to a simpler more robust form.
[18] analyze the system by directly solving a stochastic differen- tial equation for the window size and thereby derive the distribution of the window size in steady state.
Recall that Q is the probability that a packet loss that ends a cycle results in a timeout.
Note that the timeout happens in the last cycle of the supercycle (see  ).
We will focus on the last subcycle of the last cycle, which ends in a timeout, as well as the subcycle just before it, which will be referred to as the “penultimate” subcycle.
Let w be the window size during the penultimate subcycle and assume that packets f 1,...,fware transmitted during the penultimate subcycle.
Because f 1,...,fkare ACKed, this will result in the transmission of k packets in the last subcycle, say s 1,...,sk.
Assume that packet s m11is dropped in the last round; this will result in the dropping of packets s m12,...,sk.
Note that packets60  ANALYTIC MODELING OF CONGESTION CONTROL s1,...,smthat are successfully transmitted in the last round will result in the reception of m dupli- cate ACKs at the sender.
Hence, a TCP timeout will result if m 50,1, or 2 because in this case, there are not enough ACKs received to trigger a duplicate ACK /C0based retransmission.
In all the three cases above, not enough duplicate ACKs are generated in the last subcycle to trigger a retransmission.
Congestion avoidance and control.
[2] Floyd S. Connections with multiple congested gateways in packet switched networks.
Part 1: one-way traffic.
Some observations on the dynamics of a congestion control algorithm.
The performance of TCP/IP for networks with high bandwidth-delay pro- ducts and random loss.
[5] Padhye J, Firoiu V, Towsley D, Kurose J. Modeling TCP throughput: a simple model and its empirical validation.
[6] Altman E, Avrachenkov K, Barakat C. A stochastic model of TCP/IP with stationary random losses. ACM SIGCOMM 2000.
The macroscopic behavior of the TCP congestion avoidance algo- rithm.
[8] Low S, Varaiya P. A simple theory of traffic and resource allocation in ATM.
[9] Casetti C, Meo M. A new approach to model the stationary behavior of TCP connections.
[10] Kumar A. Comparative performance analysis of versions of TCP in a local network with a lossy link.
A comparison of equation based and AIMD congestion control.
The stochastic equation Y n115AnYn1Bnwith stationary coefficients.
[13] Baccelli F, Bremaud P. Elements of queueing theory: palm-martingale calculus and stochastic recur- rences.
[14] Bonald T. Comparison of TCP Reno and TCP Vegas via fluid approximations.
[15] Xu L, Harfoush K, Rhee I. Binary increase congestion control (BIC) for fast long distance networks.
[16] Hasegawa G, Murata M. Analysis of dynamic behaviors of many TCP connections sharing tail-drop/ RED routers.
[17] Tinnakornsrisuphap P, Makowski AM.
TCP traffic modeling via limit theorems.
[18] Budhiraja A, Hernandez-Cmpos F, Kulkarni VG, Smith FD.
Stochastic differential equation for TCP window size.
ANALYTIC MODELING OF CONGESTION CONTROL SUGGESTED READING Appenzeller G. Sizing router buffers.
PhD thesis, Stanford University, 2005 Barakat C. TCP/IP modeling and validation.
Firoiu V, Yeom I, Zhang X. A framework for practical performance evaluation and traffic engineering in IP networks.
Promoting the use of end-to-end congestion control in the Internet.
The window distribution of multiple TCPs with random loss queues.
Misra V, Gong W, Towsley D. Fluid based analysis of a network of AQM routers supporting TCP flows with an application to RED.
SUGGESTED READING CHAPTER 3OPTIMIZATION AND CONTROL THEORETIC ANALYSIS OF CONGESTION CONTROL 3.1INTRODUCTION
In this chapter, we introduce a network wide model of congestion control in the fluid limit and use it to ask the following questions: Can the optimal congestion control laws be derived as a solution to an optimization problem?
If so, what is the utility function being optimized?
What is meant by stability of this system, and what are the conditions under which the system is stable?
This line of investigation was pursued by Kelly
[6,7] , among others, and their results form the subject matter of this chapter.
As in , we continue to use fluid flow models for the system.
Unlike the models used in , the models in this chapter are used to represent an entire network with multiple con- nections.
These models are analogous to the “mean field” models in physics, in which phenomena such as magnetism are represented using similar ideas.
They enable the researcher to capture the most important aspects of the system in a compact manner, such that the impact of important sys- tem parameters can be analyzed without worrying about per-packet level details.
It has been shown that by applying Lagrangian optimization theory to a fluid flow model of the network, it is possible to decompose the global optimization problem into independent local optimi- zation problems at each source.
Furthermore, the Lagrangian multiplier that appears in the solution can be interpreted as the congestion feedback coming from the network.
This is an elegant theoreti- cal result that provides a justification for the way congestion control protocols are designed.
Indeed, TCP can be put into this theoretical framework by modeling it in the fluid limit, and then the theory enables us to compute the global utility function that TCP optimizes.
Alternately, we can derive new congestion control algorithms by starting from a utility function and then using the theory to compute the optimal rate function at the source nodes.
The fluid flow model can also be used to answer questions about TCP’s stability as link speeds and round trip latencies are varied.
This is done by applying tools from classical optimal control theory to a linearized version of the differential equations obeyed by the congestion control algo- rithm.
This technique leads to some interesting results, such as the fact that TCP Reno with Active Queue Management (AQM) is inherently unstable, especially when a combination of high band- width and large propagation delay is encountered.
This analysis has been used to analyze the Random Early Detection (RED) controller and discover suitable parameter settings for it.
It has also been used to find other controllers that perform better than RED.
The techniques developed in this chapter for analyzing TCP constitute a useful toolkit that can be used to analyze other congestion control algorithms.
In recent years, algorithms such as Data Canter TCP (DCTCP) and the IEEE 802.1 Quantum Congestion Notification (QCN) have been ana- lyzed using these methods.
We end the chapter with a discussion of a recent result called the aver- aging principle (AP), which shows the equivalence of a proportional-integral (PI) type AQM and a special type of rate control rule at the source.
The rest of this chapter is organized as follows: In  , we use Lagrangian optimization theory to analyze the congestion control problem and derive an expression for the utility function for TCP Reno.
In  , we introduce and analyze Generalized additive increase/multiplicative decrease (GAIMD) algorithms,  discusses the application of control theory to the conges- tion control problem and the derivation of system stability criteria, and  explores a recent result called the AP that has proven to be very useful in designing congestion control algorithms.
The most important concepts covered in these sections are that of the formulation of congestion control as the solution to an optimization problem and the application of classical Nyquist stability criteria to congestion control algorithms.
3.2CONGESTION CONTROL USING OPTIMIZATION THEORY Consider a network with L links and N sources (  ).
This is a natural assumption to make because if the congestion along a source’s path increases, then it should lead to a decrease in its data rate.
Define UiðriÞ5ð rif21 iðriÞdriso thatdUiðriÞ dri5f21 iðriÞfor 1#i#N (5)
Because U ihas a positive increasing derivative, it follows that it is monotone increasing and strictly concave.67 3.2CONGESTION CONTROL USING OPTIMIZATION THEORY By construction, the equilibrium rate r iis the solution to the maximization problem max ri½UiðriÞ2riqi/C138: (6) This equation has the following interpretation: If U i(ri) is the utility that the source attains as a result of transmitting at rate r i, and q iis price per unit data that it is charged by the network, then  leads to a maximization of a source’s profit.
Note that  is an optimization carried out by each source independently of the others (i.e., the solution r iisindividually optimal ).
We wish to show that r iis also the solution to the fol- lowing global optimality problem: max r$0XN i51UiðriÞ;subject to (7) Xr#C: (8) Equations 7 and 8 constitute what is known as the primal problem.
A unique maximizer, called the primal optimal solution, exists because the objective function is strictly concave, and the feasi- ble solution set is compact.
A fully distributed implementation to solve the optimality problem described by equations 7 and 8 is not possible because the sources are coupled to each other through the constraint  ).
There are two ways to approach this problem: 1.By modifying the objective function (  ) for the primal problem, by adding an extra term called the penalty or barrier function, or 2.By solving the problem that is dual to that described by equations 7 and 8 .
In this chapter, we pursue the second option and solve the dual problem (we will briefly describe the solution based on the primal problem in  ).
It can be shown that the solution to the primal problem leads to a direct feedback of buffer-related data without any averaging at the nodes, and the solution to the dual problem leads to processing at the nodes before feedback of more explicit information
In general, we will show that the dual problem leads to congestion feedback that is proportional to the queue size at the congested node, and hence is more appropriate for modeling congestion control systems with AQM algorithms operating at the nodes.
The duality method is a way to solve equations 7 and 8 (Appendix 3.D ), which naturally leads to a distributed implementation, as shown next.
;1#i#N (11) that maximize Lðr;λÞ, can be computed separately by each source without the need to coordinate with the other sources, in N separate subproblems.
However, as  shows, a source needs information from the network, in the form of qi, before it can compute its optimum rate.
Hence, to complete the solution we need to solve the dual problem (i.e., find λl;1#l#L) such that min λ$0DðλÞ (12) and substitute them into the  for rmax i.
The convex duality theorem then states that the optimum rmax icomputed in  also maximize the original primal problem (  ).
The dual problem (  ) can be solved using the gradient projection method
Note that the Lagrange multipliers λlbehave as a congestion measure at the link because this quantity increases when the aggregate traffic rate at the link y l(rmax) exceeds the capacity C lof the link and conversely decreases when the aggregate traffic falls below the link capacity.
Hence, it makes sense to identify the Lagrange multipliers λlwith the link congestion measure p l, so that λl5pl;1#l#L, and pn11 l5½pn l1γðylðrmax iÞ2ClÞ/C1381(16a) or in the fluid limit dpl dt5γðylðtÞ2ClÞif p lðtÞ.0 γðylðtÞ2ClÞ1if p lðtÞ50/C26 (16b)69 3.2CONGESTION CONTROL USING OPTIMIZATION THEORY Equations 11 and 16 constitute the solutions to the dual problem.
This solution can be imple- mented in a fully distributed way at the N sources and L links, in the following way:
At link l, 1 #l#L 1.Link l obtains an estimate of the total rate of the traffic from all sources that pass through it, y l. 2.It periodically computes the congestion measure plusing a , and this quantity is communicated to all the sources whose route passes through link l.
This communication can either explicit as in ECN schemes or implicit as in random packet drops with RED.
At source i, 1 #i#N 1.Source i periodically computes the aggregate congestion measure for all the links which lie along its route given by qn i5XL l51Xlipn l (17) 2.Source i periodically chooses its new rate using the formula rn i5Ui021ðqn iÞ (18)
From equations 17 and 18 , we obtain the rate control equations for the congestion control prob- lem, if the utility function U iis known.
This distributed procedure is strongly reminiscent of the way TCP congestion control operates, and hence it will not come as a surprise that it can be put into this framework.
Hence, TCP congestion control can be interpreted as a solution to the problem of maximizing a global network utility function (  ) under the constraints (  ).
In the next section, we obtain expressions for the utility function U for TCP Reno.
Note that for the case γ51,b is precisely the equation satisfied by the queue size process at the node; hence, the feedback variable p l(t) can be identified as the queue size at link l. AQM type schemes can also be put in this framework, as explained next.
The RED algorithm can be described as follows: Let b lbe the queue length at node l and let blavbe its average; then they satisfy the following equations: bn11 l5½bn l1yn l2Cl/C1381 bav;n11 l5ð12αlÞbav;n l1αlbn l(19)
Then the dropping probability p lis given by pn l50 if bav;n l,Bmin Kðbav;n l2BminÞif B min,bav;n l,Bmax 1 if bav;n l.Bmax8 < :(22)70  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS If we ignore the queue length averaging and let B min50 and consider only the linear portion of  , then the dropping probability becomes pn l5Kbn l (23)
This is referred to as a proportional controller and is discussed further in  of this chapter.
Taking the fluid limit and using  , we get dplðtÞ dt5KdblðtÞ dt 5KðylðtÞ2ClÞif b lðtÞ.0 K½ylðtÞ2Cl/
But is exactly in the form ( b ) that was derived from the gradient projec- tion method for minimizing the dual problem.
Hence, a proportional controller /C0type RED arises as a natural consequence of solving the dual problem at the network nodes.
3.2.1 UTILITY FUNCTION FOR TCP RENO There are two ways in which the theoretical results from  can be used in practice: 
Given the system dynamics and the source rate function f (  ), compute the network utility function U (  ) that this rate function optimizes.
 Given a network utility function U, find the system dynamics and the source rate function that optimizes this utility function.
In this section, we use the first approach, in which a fluid model description for TCP Reno is used to derive the network utility function that it optimizes.
We start by deriving an equation for the TCP Reno’s window dynamics in the fluid limit.
Note that the total round trip delay is given by TiðtÞ5Di1X lXliblðtÞ Cl(25) where Di5Tid1Tiuis the total propagation delay and b l(t) is the ithqueue length at time t.
The source i rate R i(t) at time t is defined by RiðtÞ5WiðtÞ TiðtÞ(26)
Using the notation from  , the aggregate congestion measure Q i(t) at a source can be written as QiðtÞ512L lALið12Plðt2τb liðtÞÞÞ /C25X lALiPlðt2τb liðtÞÞ (27) because it takes τb liðtÞseconds for the ACK feedback from link l to reach back to source i. Note that is in the form required in  .
The aggregate flow rate at link l is given by YlðtÞ5X lXliRiðt2τf liðtÞÞ (28) because it takes τf liðtÞseconds for the data rate at source i to reach link l.71 3.2CONGESTION CONTROL USING OPTIMIZATION THEORY The rate of change in window size at source i for TCP Reno is then given by the following equation in the fluid limit: dWiðtÞ dt5Riðt2TiðtÞÞ½12QiðtÞ/C138 WiðtÞ2Riðt2TiðtÞÞQiðtÞ1 24WiðtÞ 3(29)
The first term on the Right Hand Side (RHS) of  captures the rate of increase in window size, which is the rate at which positive ACKs return to the source multiplied by the increase in window size caused by each such ACK during the congestion avoidance phase.
The sec- ond term captures the rate of decrease of the window size when the source gets positive congestion indications from the network (the factor 4/3 is added to account for the small-scale fluctuation of Wi(t); see Low
Note that  ignores the change in window size caused by timeouts.
We now make the following approximations: We ignore the queuing delays in  so that the round trip delay is now fixed and approximated by Ti5Di1X lXli Cl(30)
Note that we ignored the propagation delays in the simplifications that led to  .
However, modeling these delays is critical in doing a stability analysis of the system, which we postpone to  .
In steady state, it follows from  that ri51 Tiﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 3 212qi qis (34) so that we again recovered the square-root formula.
(36)72  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS Note that for small values of q i, reduces to the expression that is familiar from  ri51 Tiﬃﬃﬃﬃﬃﬃﬃ 3 2qis (37)
Utility functions of the form  are known to lead to rates that minimize “potential delay fairness” in the network ( Appendix 3.E ), that is, they minimize the overall potential delay of transfers in progress.
If the utility function is of the form UiðriÞ5wilogri (40)
then it maximizes the “proportional fairness” in the network.
It can be shown that TCP Vegas’s utility function is of the form (40)[9] hence it achieves proportional fairness.
Max-min fairness is in some sense the ideal way of allocating bandwidth in a network; however, in general, it is difficult to achieve max-min fairness using AIMD type algorithms.
It can be achieved by using explicit calculations at the network nodes, as was done by the ATM ABR scheme.
The theory developed in the previous two sections forms a useful conceptual framework for designing congestion control algorithms.
In , we discuss the recent application of these ideas to the design of congestion control algorithms, which also uses techniques from machine learning theory. 3.3GENERALIZED TCP /C0FRIENDLY
In this section, we use the theory developed in  to analyze a generalized version of the TCP congestion control algorithm with nonlinear window increment /C0decrement rules, called GAIMD.
We then derive conditions on the GAIMD parameters to ensure that if TCP GAIMD and73 3.3GENERALIZED TCP /C0FRIENDLY ALGORITHMS Reno pass through a bottleneck link, then they share the available bandwidth fairly.
This line of investigation was originally pursued to design novel congestion control algorithms for traffic sources such as video, which may not do well with TCP Reno.
Define the following: αsðRsðtÞÞ: Rule for increasing the data rate in the absence of congestion βsðRsðtÞÞ: Rule for decreasing the rate in the presence of congestion Then the rate dynamics is governed by the following equation: dRsðtÞ dt5ð12QsðtÞÞRsðt2TsÞαsðRsðtÞÞ2QsðtÞRsðt2TsÞβsðRsðtÞÞ (42)
From  , it follows that in equilibrium, qs5αsðrsÞ αsðrsÞ1βsðrsÞ5fsðrsÞ (44) so that the utility function for GAIMD is given by UsðrsÞ5ðαsðrsÞ αsðrsÞ1βsðrsÞdrs (45)
Define an algorithm to be TCP-friendly if its utility function coincides with that of TCP Reno.
From equations 44 and 35 , it follows that an algorithm with increase /C0decrease functions given by ðαs;βsÞis TCP friendly if and only if αsðrsÞ αsðrsÞ1βsðrsÞ52 21r2 sT2 si:e:αsðrsÞ βsðrsÞ52 r2 sT2 s(46)
Following Bansal and Balakrishnan [11], we now connect the rate increase /C0decrease rules to the rules used for incrementing and decrementing the window size.
W’W2βWlOn packet drop (47b) Equations 47a and 47b are a generalization of the AIMD algorithm to nonlinear window increase and decrease and hence are known as GAIMD algorithms.
Using the same arguments as for TCP Reno, the window dynamics for this algorithm are given by dWsðtÞ dt5ð12QsðtÞÞRsðt2TÞα Wk11 sðtÞ2QsðtÞRsðt2TsÞβWl sðtÞ74  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS Substituting WsðtÞ5RsðtÞTs, it follows that dRsðtÞ dt5ð12QsðtÞÞRsðt2TsÞα Rk11 sðtÞTk12 s2QsðtÞRðt2TsÞβRl sðtÞTl21 s (48)
Comparing this equation with  , we obtain the following expressions for the rate increase /C0decrease functions for the GAIMD source that corresponds to equations 47a and 47b : αsðrsÞ5α rk11 sTk12 sand βsðrsÞ5βrl sTl21 s (49)
Substituting  back into the TCP friendliness criterion from  yields α β1 ðrsTsÞk1l1152 r2 sT2 s(50)
Hence, the algorithm is TCP friendly if and
only if k1l51 andα β52 (51) Note that if l ,1, then this implies that window is reduced less drastically compared with TCP on detection of network congestion.
Using  , it follows that in this case, 0 ,k,1, so that the window increase is also more gradual as compared with TCP.
3.4STABILITY ANALYSIS OF TCP WITH ACTIVE QUEUE MANAGEMENT
The stability of a congestion control system is de fined in terms of the behavior of the bottleneck queue size b(t).
If the bottleneck queue size fluct uates excessively and v ery frequently touches zero, thus leading to link under utilization, then the system is considered to be unstable.
Also, if the bottleneck queue size grows and spends all its time completely full, which leads to excessive packet drops, then again the system is unstable .
Hence, ideally, we would like to control the system so that the bottleneck queue size stays in the neighborhood of a ta rget length, showing only small fluctuations.
It has been one of the achievements of fluid modeling of congestion control systems that it makes it possible to approach this problem by using the tools of classical control system theory.
This program was first carried out by Vinnicombe
[6,7] The latter group of researchers modeled TCP Reno, for which they analyzed the RED controller, and another controller that they introduced called the PI controller.
Since then, the technique has been applied to many75 3.4STABILITY ANALYSIS OF TCP WITH ACTIVE QUEUE MANAGEMENT other congestion control algorithms and constitutes one of the basic techniques in the toolset for analyzing congestion control.
To analyze the stability of the system, we will follow Holot and colleagues
In particular, we assume that there are N homogeneous TCP sources, all of which pass through single bottleneck node.
Initially, let’s consider the open-loop system shown in  .
Later in this and subsequent sections, we will connect the queue length process b(t) with congestion indicator process Q(t), using a variety of controllers.
QueueingRate ControlN Homogeneous Sources  The open loop system.76  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS The fluid approximation for the queue length process at the bottleneck can be written as dbðtÞ dt5WðtÞ TðtÞNðtÞ2C (55) Equations 54 and 55 give the nonlinear dynamics for the rate control and queuing blocks, respectively, in  .
To simplify the optimal control problem so that we can apply the tech- niques of optimal control theory, we now proceed to linearize these equations.
System stability can be studied with the help of various techniques from optimal control theory, including Nyquist plots, Bode plots, and root locus plots [13].
Of these, the Nyquist plot is the most amenable to analyzing systems with delay lags of the type seen in these systems and hence is77 3.4STABILITY ANALYSIS OF TCP WITH ACTIVE QUEUE MANAGEMENT used in this chapter.
Appendix 3.B has a quick introduction to the topic of the Nyquist stability cri- terion that the reader may want to consult at this point.
Assuming initially that there is no controller present, so that there is no AQM stabilization and the queue size difference is fed back to the sender with no delay, then the system is as shown in  , so that Qδ5Pδ5bδ:
Next let’s introduce the propagation delay into the system, so that QδðtÞ5Pδðt2TÞ, as shown in .
In this case,  changes to dWδðtÞ dt522N CT2 0WδðtÞ2C2T0 2N2Pδðt2TÞ This introduction of the delay in t he feedback loop can lead to sys tem instability, as shown next.
The open loop and closed loop transfer functions for this system are given by U0ðsÞ5UðsÞe2sT05Ke2sT0 ðas11Þðbs11Þ
W0ðsÞ5Ke2sT0 ðas11Þðbs11Þ1Ke2sT0 From the Nyquist plot for U0ðjωÞshown in B , note the following: 
The addition of the delay component has led to a downward spiraling effect in the locus of U0ðjωÞ.
This is because U0ðjωÞ5jU0ðjωÞje2jargðU0ðjωÞÞwhere the point ðω;U0ðjωÞÞon the curve makes the following angle with the real axis: argðU0ðjωÞÞ5ωT01tan21aω1tan21bω Hence, as ωincreases, the first term on the RHS increases linearly, and the other two terms are limited to at mostπ 2each.
The system is no longer unconditionally stable, and as the gain K increases, it may encircle the point (21,0) in the clockwise direction, thus rendering it unstable.
Thus, this proves that high link capacity or high round trip latency can cause system instability.
Although the onset of instability with increasing C or T 0is to be expected, the variation with N implies that the system gains in stability with more sessions.
The intuitive reason for this is that more sessions average out the fluctuations because of the high variations in TCP window size (especially the decrease by half on detecting congestion).
With only a few sessions, these variations affect the stability of the system because even a single packet drop causes a large drop in window size.
On the other hand, with many connections, the decrease in window size of any one of them does not have as large an effect.
Some of the high-speed variations of TCP that we will meet in  reduce their windows by less than half on detecting packet loss, thus reducing K and sub- sequently increasing their stability in the presence of large C and T 0.
3.4.1 THE ADDITION OF CONTROLLERS TO THE MIX Because the system shown in  can become unstable, we now explore the option of adding a controller into the loop, as shown in  , and then adjusting the controller parameters in order to stabilize the system.
We will first analyze the RED controller, and then based on the learn- ing from its analysis, introduce two other controllers, the Proportional and the PI controllers, that are shown to have better performance than the RED controller.
3.4.1.1 Random Early Detection (RED) Controllers The RED controller is shown in  , and its Laplace transform is given by (see Appendix 3.
The other RED parameters are defined in  in .
As  shows, the RED controller consists of a Low Pass Filter (LPF) followed by a constant gain.
The objective of the optimal control problem is to choose RED parameters L redand K to stabilize the system shown in  .
Note that the open loop transfer function for the system is given by TðsÞ5UtcpðsÞUqueueðsÞVredðsÞe2sT0 5LredðCT0Þ3 ð2NÞ2e2sT0 s K110 @1
To apply the Nyquist criterion, set s5jω, so that TðjωÞ5jTðjωÞje2jargðTðjωÞÞwhere jTðjωÞj5LredðCT0Þ3 ð2NÞ2 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ω K/C0/C1211qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ω 2N=CT2 0/C16/C172 11ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ω 1=T0/C16/C172 11rs and (66) argðTðjωÞÞ5ωT01tan21ω K1tan21ω 2N=CT2 01tan21ω 1=T0(67)
[6]proved the following result: Let L redand K satisfy:
To prove this, we start with the following observation: From  , it follows that jTðjωÞj#LredðCT0Þ3 ð2NÞ2 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ω K/C0/C1211q (70)
Furthermore, if ω5ωc;N$N2;T0#T1, then it follows from equations 68 and 70 that jTðjωcÞj#1 (71) From this, we conclude that the critical frequency ω/C3at which jTðjω/C3Þj51, satisfies the relation ω/C3#ωc(this is because jTðjωÞjis monotonically decreasing in ω).
Hence, it follows from the Nyquist criterion that if we can show that the angle arg TðjωcÞsatisfies the condition argTðjωcÞ,πradians (72) then the system has a positive phase margin (PM) given by PM$π2argTðjωcÞ.0 and hence is stable.
Note that the fact that PM .0 implies that TðjωÞ,1, where ωis the frequency at which arg ðTðjωÞÞ5π, that is, the gain margin is also positive.
This proves  and consequently the stability result.
The following example from Holot et al.
Choosing max p50.1 and because Lred5max p=ðmax th2min thÞ, it follows that max th2min th/C25540 packets.
A shows the case when the RED parameters are set without taking  into account, and we can observe the large buffer oscillations that result.
When the parameters are set as per  , then they result in a smoother buffer variation as shown in B .
Note the large amount of time that it takes for the queue to settle down in B , which is a conse- quence of the small value of ωcthat was used to get a good phase margin.
As the link capacity C or the round trip latency increases or the number N of connections decreases, the RED parameters L redand K have to be changed to maintain the inequality (  ).
We can do the following: 
Adapt L red: This can be done by changing the value of the marking probability max pwhile keeping the queue thresholds the same.
Note that reducing max phas the effect of reducing the number of dropped or marked packets when the average queue size is between the threshold values.
This causes the queue size to increase beyond max th, thus leading to increased packet drops because of buffer overflow.
On the other hand, increasing max phas the effect of increasing the number of RED drops and thus decreasing throughput.
By using  as a guide, it is possible to adapt L redmore intelligently as a function of changing parameters.
An alternative technique for adapting RED was given by Feng et al.
[14], in which they used the behavior of the average queue size to adapt max p.
Thus, if the queue size spent most of its time near max th, then max pwas increased, and if it spent most of its time at min thor under, max p was reduced. 
Adapt K: From  , K can be reduced by decreasing the averaging constant w q.
However, note that decreasing w qcauses the smoothing function to become less responsive to queue length fluctuations, which again leads to poorer regulation of the queue size.
We can compensate by increasing the constant from 0.1, but the earlier analysis showed that this has the effect of decreasing the PM and thus increasing the queue oscillations.
3.4.1.2 Proportional Controllers As shown in  , the RED controller from  reduces to a proportional controller if K is chosen to be very large, which also corresponds to removing the low-pass filter entirely.
This implies that the feedback from the bottleneck node is simply a scaled version of the (slightly delayed) queue length delta rather than the average queue length.
If we denote the proportionality factor by K P, then from  , it follows that the transfer function is given by TPðsÞ5UtcpðsÞUqueueðsÞVPðsÞe2sT0 5KPðCT0Þ3 ð2NÞ2e2sT0 s 2N=CT2 0110 @1
As 1=T0110 @1 A(74) and in the frequency domain TPðjωÞ5jTPðjωÞje2jargðTPðjωÞÞ We now carry out a stability analysis for this system using the same procedure that was used for the RED controller.
Hence, we will choose a critical frequency ωcsuch that with the appropriate choice of the controller parameters jTpðωcÞj#1, and then show that the PM at ω5ωcis positive, which is sufficient to prove stability.
Note that this choice of K pprecisely cancels out the high loop gain caused by TCP’s window dynamics.
The cancellation of the loop gain using the Proportional controller constant K p(using equation 76) is cleaner than the cancellation using L redor K in RED controllers because the latter can lead to large queue length variations as we saw in the previous section.
Recently proposed protocols such as DCTCP (see ) use a proportional controller.
However, a proportional controller by itself does not work well in certain situations; in particular, just like the RED controller, it may lead to a steady-state error between the steady state output and the reference value.
In the next sec- tion, we discuss the PI controller, which corrects for this problem.
compares the performance of the RED and proportional controllers, and we can see that the latter has much faster response time and also lower queue size fluctuations in equilibrium. 0
It is pos- sible to design an integral controller for AQM that can clamp the queue size to some reference value regardless of the load value.
The simplest of such integral controllers is the PI controller, which is given by the transfer function (  ): VPIðsÞ5KPIs z11/C16/C17 s(80) The transfer function for the system with the PI controller is given by TPIðsÞ5UtcpðsÞUqueueðsÞVPIðsÞe2sT0 5KPIðCT0Þ3 ð2NÞ2s z110 @1 Ae2sT0 ss 2N=CT2 0110 @1
[7] shows the result of a simulation with a bottleneck queue size of 800 packets, in which the reference b 0of the PI controller was set to 200 packets and the traffic consisted of a mixture of http and ftp flows.
It clearly shows the faster response time PI compared with the RED controller as well as the regulation of the buffer occupancy to the target size of 200 packets.
B shows the case when the number of flows has been increased to the point that the sys- tem is close to its capacity.
Neither RED nor the proportional controllers are able to stabilize the queue because high packet drop rates have pushed the operating point beyond the size of the queue.
The PI controller continues to exhibit acceptable performance, although its response time is a little slower.
The transfer function of the PI controller is described in the s domain in  .
For a digi- tal implementation, this description needs to be converted into a z-transform.
The input into the controller is bδ5b2b0, and the output is Pδ.
This equation can be converted into a difference equation, so that at time t 5kS, where S 51/fs PðkSÞ5abδðkSÞ2cbδððk21ÞSÞ1Pððk21ÞSÞ (88) This equation can also be written as PðkSÞ5ða2cÞbδðkSÞ1cðbδðkSÞ2bδððk21ÞSÞ1Pððk21ÞSÞ (89)
By  , a zero derivative implies that that input rate of the flows to the bottleneck node exactly matches the link capacity so that there is no up or down fluctuation in the queue level.
The idea of providing difference 1derivative AQM feedback has proven to be very influential and constitutes part of the standard toolkit for congestion control designers today.
Despite its superior stability properties, the implementation of PI controllers in TCP/IP networks faces the hurdle that the TCP packet header allows for only 1 bit of congestion feedback.
The more recently designed IEEE QCN protocol has implemented a PI controller using 6 bits of congestion feedback in the packet header
3.5THE AVERAGING PRINCIPLE (AP)
The discussion in  showed that to improve stability for the congestion control algorithm, the network needs to provide information about both the deviation from a target buffer occupancy and the derivative of the deviation.
This puts the burden of additional complexity on the network nodes.
In practice, this is more difficult to implement than when compared with adding additional functionality to the end systems instead.
In this section, we will describe an ingenious way the source algorithm can be modified without touching the nodes, which has the same effect as using a PI-type AQM controller.
This is done by applying the so-called Averaging Principle (AP) to the source congestion control rules [16].
The AP is illustrated in  .
It shows a congestion control algorithm, in which the rate reacts to two factors: 1.Periodically once every τseconds, the source changes the transmit rate R C, either up or down, based on the feedback it is getting from the network.
2.At the midpoint between two network-induced rate changes, the source changes its rate according to the following rule: Let R Tbe the value of the rate before the last feedback-induced88  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS change.
Then τ=2 time units after every feedback induced change, the AP controller performs an “averaging change” when the rate R Cis changed as follows: RC’RC1RT 2 It can be shown that there exists an AQM controller of the type that was derived in  , given by (see  ) PðkÞ50:5KbδðkÞ10:25KðbδðkÞ2bδðk21ÞÞ1Pðk21Þ (90) such that the AP controller is exactly equivalent to this controller.
Both con- trollers map a sequence of sampled queue error samples bδðnÞto an input signal P(n) that drives the congestion control rate determination function.
Controller 1 is an AP controller, and the upper branch of controller 2 is the AQM controller from  .
In the following, we will show that P(n)5U(n) and that the lower branch of controller 2 can be ignored, so that PðnÞ/C25U1ðnÞ. The following equations describe the input-out relations for the two controllers.
This can be done by showing that U 1(n) and P m(n) satisfy the same recursions.
Because the contribution of U 2(n) can be ignored [16], it follows that PðnÞ/C25U1ðnÞ, and the AQM and AP controllers are almost identical.
We will encounter congestion control protocols such a TCP BIC later in the book (see ) that are able to operate in a stable manner in very high-speed networks even with tradi- tional AQM schemes.
These protocols may owe their stability to the fact that their rate control rules incorporate averaging of the type discussed here. 3.6IMPLICATIONS FOR CONGESTION CONTROL ALGORITHMS From the analysis in the previous few sections, a few broad themes emerge that have influenced the design of congestion control algorithms in recent years:  AQM schemes that incorporate the first or higher derivatives of the buffer occupancy process lead to more stable and responsive systems.
This was shown to be the case in the analysis of the PI controller.
Also, because the first derivative of the buffer occupancy process can be written as dbðtÞ dt5C2X iRi it also follows that knowing the derivative is equivalent to knowing how close the queue is to its saturation point C.
Some protocols such as QCN (see ) feed back the value of db/dt directly, and others such as XCP and RCP (see ) feed back the rate difference in the RHS of the equation.
If the network cannot accommodate sophisticated AQM algorithms (which is the case for TCP/ IP networks), then an AP-based algorithm can have an equivalent effect on system stability and performance as the derivative-based feedback.
Examples of algorithms that have taken this route include the BIC and CUBIC algorithms (see ) and QCN (see ).
To impose the constraint that the sum of the data rates at a link should be less than the link capacity, the barrier function can be chosen in a way such that it increases to infinity when the arrival rate approaches the link capacity.
U(r 1,...,rN) is a strictly concave function and it can be shown that the problem max ri$0Uðr1; :::;rNÞ (104) decomposes into N separate optimization subproblems that can be solved independently at each of the sources.
The solution to the optimization problem at each source node is given by U0 iðriÞ5X l:lAAiflX s:sABlrs !
(105) where A iis the set of nodes that connection i passes through and B lis the set of connections that pass through node l.
This can be interpreted as a congestion feedback of flP srs/C0/C1 back to the source from each of the nodes that the connection passes through, and is the equivalent of equation 11for the dual problem.
If k r.0, then in equilibrium setting, dr i/dt50 leads back to  . is known as the primal algorithm for the congestion control problem.
A very good description of the field of optimization and control theory applied to congestion control is given in Srikant’s book
It has a detailed discussion of the barrier function approach to system optimization, as well as several examples of the application of the Nyquist criterion to various types of congestion control algorithms.
For cases when the system equilibrium point falls on a point on nonlinearity, the linearization technique is no longer applicable.
Substituting equation A4 back into equations A2 and A3 and then into the original differential equation, we obtain the linearized equations dWδðtÞ dt522N CT2 0WδðtÞ2C2T0 2N2Pδðt2T0Þ dbδðtÞ dt5N T0WδðtÞ21 T0bδðtÞ(A5)94  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS APPENDIX 3.B THE NYQUIST STABILITY CRITERION
The Nyquist criterion is used to study the stability of systems that can modeled by linear differen- tial equations in the presence of a feedback loop.
In .B1 , G(s) is the Laplace transform of the system being controlled, and H(s) is the Laplace transform of the feedback function.
The Nyquist criterion is a technique for verifying this condition that is often easier to apply than finding the roots, and it also provides addi- tional insights into the problem.
Asωis varied from 2Nto1N, the function HðjωÞGðjωÞmaps these points to the target complex plane, where it describes a curve.
For the system to be stable, we require that Z 50, and because M 5Z/C0P, it follows that: I f P50, then stability requires that M 50, that is, there should no clockwise encirclements of the point ( 21,0) by the locus of HðjωÞGðjωÞ. I f P.0, then M 52 P, i.e., the locus of HðjωÞGðjωÞshould encircle the point ( 21,0) P times in the counterclockwise direction.
.B2 shows an example of the application of the Nyquist criterion to the function GðjωÞ along with some important features.
Note that curve for GðjωÞgoes to 2Nasω-0, and approaches 0 as ω-0.
the angle that this line makes with the real axis.
Note the following features in the graph: 
The point ( 2a, 0) where the GðjωÞcurve intersects with the real axis is called the phase crossover point because at this point arg ðGðjωÞÞ5π.
As the Gain Margin decreases, the system tends towards instability.
The point where the GðjωÞcurve intersects with the unit circle is called the gain crossover point because at this point jGðjωÞj51.
The phase margin (PM) of the system is defined by the angle between the crossover point and the negative real axis.
Just as for the GM, a smaller PM means that the system is tending toward instability.
Generally a PM of between 30 and 60 degrees is preferred.
In the applications of the Nyquist criterion in this book, we usually find a point ωcsuch that jGðjωcÞj#1.
This implies that the gain crossover point ω/C3is such that 0 #ω/C3#ωc.
Hence, if we can prove that arg ðGðjωcÞÞ,π, then the system will be stable.
APPENDIX 3.C TRANSFER FUNCTION FOR THE RED CONTROLLER In this appendix, we derive the expression for the transfer function of the RED controller
Assume that this difference equation can be put in the form of the following dif- ferential equation dx dt5KxðtÞ1LbðtÞ
Then, in a sampled data system, x(t k11) can be written xðtk11Þ5eKðtk112tkÞxðtkÞ1ðtk11 tkeKðtk112τÞLbðtkÞdτ 5eKδxðtkÞ2ð12eKδÞLbðtkÞ K(C2)
Comparing (C1) with (C2), it follows that L 52 K and 12wq5eKδso that K5logeð12wqÞ δ Hence, the differential equation for the averaging process is given by dx dt5KxðtÞ2KbðtÞ, which in the transform space translates into XðsÞ bðsÞ5K s1K This signal is further passed through the thresholding process as shown in .B1 before yielding the final output that is fed back to the source.
However, too high a value for K will result in the situation in which the filter starts tracking the instantaneous value of the buffer size, resulting in continuous oscillations.
APPENDIX 3.D CONVEX OPTIMIZATION THEORY Definition 1 :
A set C is said to be convex if the following property holds:
The function is said to be strictly convex (concave) if the above inequalities are strict when x6¼y and a Að0;1/C138.
The values of x that sat- isfy the constraints (D3b) form a convex set.
The Lagrangian function L for this problem is defined by Lðx;λ;μÞ5fðxÞ2λTðPx2cÞ2μTQx (D4) where λ$0;μare called the dual variables or the Lagrangian multipliers for this problem.
Karush-Kuhn-Tucker theorem: x is a solution to the optimization problem (D3a /C0b)if and only if it satisfies the following conditions: ΔfðxÞ2PTλ2QTμ50 λTðPx2bÞ50 Px#b Qx50 λ$0(D5)98  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS Definition 3 :
Define the dual function Dðλ;μÞby Dðλ;μÞ5max xACLðx;λ;μÞ (D6) where C is the set of all x that satisfy the constraints (D3b) .
Strong Duality Theorem: Let ^xbe the point that solves the constrained optimization problem (equations D3a and D3b ).
If the Slater constraint qualification conditions are satisfied, then inf λ$0;μDðλ;μÞ5fð^xÞ (D7)
APPENDIX 3.E A GENERAL CLASS OF UTILITY FUNCTIONS Consider a network with K connections and define the following utility function for connection k [10], given by UkðrkÞ5wkr12αk 12αkk51;...;K (E1) for some αk.0, where r is the steady state throughput for connection k. Note that the parameters αkcontrols the trade-off between fairness and efficiency.
Consider the following special cases for 1#k#K:
This utility function assigns no value to fairness and simply measures total throughput.
αk51:UkðrkÞ5lim αk-1wkr12αk 12αk5wklogrk (E3) This network resource allocation that minimizes this utility function is known as weighted pro- portionally fair.
The logarithmic factor means that an algorithm that minimizes this function will cut down one user’s allocation by half as long as it can increase another user’s allocation by more than half.
The TCP Vegas algorithm has been shown to minimize this utility function.
αk52:UkðrkÞ5wk rk(E4) The network resource allocation corresponding to this utility function is called weighted mini- mal potential delay fair, and as shown in this chapter, this is function TCP Reno minimizes.
This metric seeks to minimize the total time of fixed-length file transfers.
Minimizing this metric is equivalent to achieving max-min fairness in the network
[9].99 APPENDIX 3.E A GENERAL CLASS OF UTILITY FUNCTIONS
Fairness and stability of end-to-end congestion control.
Rate control for communication networks: shadow pricing, propor- tional fairness and stability.
A duality model of TCP and queue management algorithms.
Optimization flow control I: basic algorithm and convergence.
[5] Kunniyur S, Srikant R. End-to-end congestion control schemes: utility functions, random losses and ECN marks.
[6] Holot CV, Misra V, Towsley D, Gong W-B. A control theoretic analysis of RED.
On designing improved controllers for AQM routers sup- porting TCP flows.
[9] Srikant R. The Mathematics of Internet Congestion Control.
[10] Mo J, Walrand J. Fair end-to-end window based congestion control.
[11] Bansal D, Balakrishnan H. Binomial congestion control algorithms.
[12] Vinnicombe G. On the stability of networks operating TCP-like congestion control.
Proc IFAC World Congress, Barcelona, 2002.
[13] Ogata K. Modern control engineering.
A self-configuring RED gateway.
Data center transport mechanisms: congestion control theory and IEEE standardization.
Prabhakar B. Stability analysis of QCN: the averaging principle. ACM SIGMETRICS, 2011.
[17] Misra V, Gong W, Towsley D. Fluid based analysis of a network of AQM routers supporting TCP flows with an application to RED.
Rate adaptation, congestion control and fairness: a tutorial.
Ecole Lausanne: Polytechnique F´ed´erale de Lausanne; 2012.
Gibbens RJ, Kelly FP.
Resource pricing and the evolution of congestion control.
Linear stability of TCP/RED and a scalable control.
Massoulie L, Roberts J. Bandwidth sharing: objectives and algorithms.
IEEE/ACM Trans Netw 2002;10 (3):320 /C08.100  OPTIMIZATION AND CONTROL THEORETIC ANALYSIS CHAPTER 4CONGESTION CONTROL IN BROADBAND WIRELESS NETWORKS 4.1INTRODUCTION
When users access the Internet using cable, DSL, or wireless mediums, they are doing so over access networks.
Over the course of the past 20 years, these networks have been deployed worldwide and today constitute the most common way to access the Internet for consumers.
These networks are characterized by transmission mediums such as air, cable, or copper lines, which presented formidable barriers to the reliable transmission of digital data.
These problems have been gradually overcome with advances in networking algorithms and physical media modulation technology so that reliable high-speed communication over access networks is a reality today.
In this chapter, the objective is to discuss congestion control over broadband wireless networks.
Common examples of broadband wireless networks deployed today include WiFi networks that operate in the unlicensed frequency bands and broadband cellular networks such as 2G, 3G, and LTE that operate in service provider /C0owned licensed bands.
When TCP is used as the transport protocol over these networks, it runs into a number of problems that are not commonly found in wireline networks; some of these are enumerated in  .
As a result, a lot of effort over the past 10 to 15 years has gone into finding ways in which the wireless medium can be made more data friendly.
A lot of this work has gone into improving the wireless physical layer with more robust modulation and more powerful error correction schemes, but equally important have been the advances in the Medium Access Control (MAC) and Transport Layers.
Some of this effort has gone into modifying the congestion control algorithm to overcome wireless-related link impair- ments, and this is the work that described in this chapter.
The fundamental problem that hobbles TCP over wireless links is the fact that it cannot differ- entiate between congestion and link-error related packet drops.
As a result, it reacts to link errors by reducing its transmit window, but the more appropriate reaction would be to retransmit the lost packet.
By using the expression for the average TCP throughput that was derived in , Ravg51 Tﬃﬃﬃﬃﬃ 3 2ps 103Internet Congestion Control.
this problem can be solved in several ways: 
By making the wireless link more robust by using channel coding: This leads to a reduction in the wireless link error rate p. 
By retransmitting the lost packets quickly at lower layers of the stack (called Automatic Retransmissions [ARQ]), so that TCP does not see the drops.
This also leads to a reduction in p at the cost of an increase in T because of retransmissions.
By introducing special algorithms at the TCP source that are able to d ifferentiate between congestion drops and link-error related drops.
We will discus s one such algorithm called TCP Westwood, which we will show has the same effect on the throughput as a reduction in the link error rate p. 
By splitting the end-to-end TCP connection into two parts, such that the wireless link falls into a part whose congestion control has be en specially designed for wirele ss environments.
This technique works because by splitting the connection, we re duce the latency T for each of the resulting two connections.
Furthermore, the use of one of the three techniques mentioned above can reduce p. Modern broadband wireless systems such as LTE have largely solved the link error caused packet drop problem through a combination of strong channel coding and retransmissions.
In fact, recent studies of wireless link performance show very few packet losses.
However, link errors have been replaced by a new problem in these networks, namely, a large amount of variability in the link latency, leading to a problem that has been christened “bufferbloat.”
The rest of this chapter is organized as follows.
In  , we introduce the broadband wireless architecture and enumerate the problems that wireless networks present for TCP. InSections 4.3 and 4.4 , we describe several techniques that are used to improve TCP performance in wireless networks, including Split Connections, Bandwidth Estimators, Loss Discrimination Algorithms, and Zero Receive Window (ZRW) ACKs.
We illustrate these ideas with a description and analysis of TCP Westwood, which was specifically designed for wireless environments.
In , we use the analytical tools developed in  to do performance modeling of TCP running over wireless links that implement techniques such Forward Error Correction (FEC) and Automatic Retransmissions (ARQ) to improve robustness.
In  , we analyze the bufferbloat issue, its causes, and some of the solutions that have been suggested.
has some useful rules that can be used to test whether link errors are indeed causing packet drops and the effectiveness of various techniques to combat it.
An initial reading of this chapter can be done in the sequence 4.1 -4.2-4.3-4.4-4.7 (for those interested in techniques to overcome wireless related link errors) or in the sequence 4.1-4.2-4.6-4.7 (for those interested in the bufferbloat problem and its solutions).
has more advanced material on the analysis of FEC and ARQ algorithms.
4.2WIRELESS ACCESS ARCHITECTURE AND ISSUES  is a simplified picture of typical cellular net work architecture.
Mobile users are connected to ab a s es t a t
i r e l ess connection, and their traffic i s backhauled to a wireless gateway, which serves as the demarcation point between the cellular network and the rest of the Internet.
The cellular base station is responsibl e for transforming the digital signal into an analogue waveform for104  CONGESTION CONTROL IN BWN over-the-air transmission, as well as for efficientl y sharing the shared wireless medium among multiple users.
Modulation techniques have improved over t he years, and today both WiFi and LTE networks use an algorithm called OFDMA (orthogon al frequency-division multiplexi ng).
OFDMA is extremely robust against multipath fading, which is the main cause of signa l distortion in cellular wir eless.
All traffic going to the mobiles is constrained to pass through the gatew ay, which is responsible for higher level functions such as enabling user mobility between base stations an d user authentication, sleep mode control, and so on.
Some of the issues that cause performance problem s for TCP in wireless networks are the following:  Higher link error rates: This has traditionally been the biggest problem with wireless networks and is caused by difficult propagation conditions in the air, leading to high link attenuations andmultipath creating reflections from surrounding objects.
With the improvement in modulationand network coding techniques over the past 20 years, the error rates have decreasedsignificantly, but they are still much higher compared with wired transmission mediums.
High link latency: Higher link latency in access networks is partially caused by the physical layer technology used to overcome wireless medium /C0related transmission problems.
A good example of this coding technology is FEC, which helps to ma ke the medium more reliable but leads to higher latency.
Also, the MAC layer algorithm operating on top of the physical laye r ,w h
su s e dt os h a r e the access medium more efficiently and provide qu ality of service (QoS) gu arantees, introduces latencies of its own.
This problem was particularly onerous in the f irst generation cellular data networks, namely GPRS, which h ad link latencies of the order of 1 second or more.
The latency decreased to about 100 ms in 3G networks and is about 50 ms in LTE.
This is still a fairly large number considering that coast-to-coast latency across the continental United States is about 23 ms. 
Large delay variations: This is common in wireless cellular networks and is caused by the base station adapting its physical layer transmission parameters such as modulation and coding, as afunction of link conditions, on a user by user basis.
This causes the effective channel capacityto fluctuate over a wide range, leading to a phenomenon called bufferbloat.
This is caused bythe fact the TCP window can never be perfectly matched with the varying link capacity, and attimes when the capacity is low and the window is much bigger, the buffer occupancy increases,as shown in .Server
Wide Area Network ServerGatewayWireless Backhaul Base  StationMobile MobileWireless Access  Cellular wireless architecture.105 4.2WIRELESS ACCESS ARCHITECTURE AND ISSUES  Random disconnects: This problem can be caused by one of several reasons.
For example, mobile users are momentarily disconnected from the network when they are doing hand-offs between cells.
Alternatively, if the wireless signal fades because of multipath or gets blocked because of an obstruction, then it can cause a temporary disconnect.
During this time, the TCP source stops receiving ACKs, and if the disconnect time is of the order of 100s of milliseconds, the retransmission timer expires, thus affecting TCP throughput.
Asymmetric link capacities: Wired mediums are fu ll duplex and have symmetric bandwidth, neither of which is true for wireless access networks.
N etworks such as Asymmetric Digital Subscriber Loop (ADSL) and Cable Hybrid Fiber Coax (HFC) posses much more capacity in the downlink direction compared with the uplink, and this is a lso true for cellular wireless.
This problem has lessened with the introduction of newer technologi es, but it still cannot be ignored.
When the uplink has much lower bandwidth compared with the downlink, then the inter-ACK time interval is no longer a good estimate of the bandwidth of the d ownlink bottleneck capacity; hence, TCP’s self-clocking mechanism breaks down.
The analy sis in  has been extended to asymmetric links by Lakshman and Madhow [1]and Varma [2].
4.3SPLIT-CONNECTION TCP This section describes and analyzes Split-Connection TCP.
This is not a new congestion control algorithm but a way to change the structure of the congestion control system so that the problematic part of the network can be isolated.
This is done by abandoning the end-to-end nature of the TCP transport layer in favor of multiple TCP-controlled segments in series (  ).
As shown, the end-to-end connection traverses two segments in series, the wide area network (WAN) network and the wireless network, with the gateway node that was introduced in  at the boundary between the two.
The TCP connection from a server in the WAN to a mobile client is split into two parts, with the first TCP connection (called TCP1) extending from the server to the gateway, and the second TCP connection (called TCP2) extending from the gateway to the client node.
The system operates as follows:  When the server initiates a TCP connection with a client, the connection set-up message is intercepted by the gateway, which then proceeds to do the following: (1) reply to the connection set-up back to the server, as if it were the client, thus establishing the TCP1 connection, and (2) establish the TCP2 connection with the client, with itself as the end point.
Data SourceTCP 1 SendTCP 2  ReceiveNetwork 1   CoreTCP 2 SendNetwork 2 WirelessTCP 1  Receive ACKs ACKsGateway  Split TCP connection.106  CONGESTION CONTROL IN BWN  During the data transfer phase of the connection, the gateway intercepts all packets coming from the server over TCP1 and immediately ACKs them back.
At the same time, it forwards the packet on TCP2 to the client and discards it after it rece ives the corresponding ACK from the client.
If the TCP2 is operating at a slower speed than TCP1, then the data buffers in the gateway start to fill up, and the gateway then proceed to backpressure the server by reducing the size of the receive window in TCP1’s ACK packets.
The design described differs from the one in the original paper on split TCP [3]in the following way: Bakre and Badrinath assumed that the TCP connection would be split at the wireless base station rather than at the gateway.
Doing so introduces some additional complications into the design because if the mobile client moves from one base station to another, then the entire state information about the connection and the buffered packets have to be transferred to the new base station.
In the design shown in  , on the other hand, this situation is avoided because the split happens at the gateway node, which remains the same even as the client changes base stations.
The benefits of a split-connection TCP design are the following: 
The TCP congestion control algorithms operating on each segment of the network can be tailored to the specific characteristics of the network.
The core network is typically constructed from highly reliable optical connections and hence has very low link error rates.
The access network, on the other hand, may present several transmission-related problems that were described earlier.
As a result, the congestion control algorithm over the wireless network can be specially tailored to overcome these link problems.
Note that this can be done without making any changes to TCP1, which is typically running on some server in the Internet and hence cannot be modified easily 
Because of the way that TCP throughput varies as a function of the round trip latency and link error rates, the split-connection design can result in a significant boost to the throughput even in the case in which legacy TCP is used on both connections.
This claim is proven below.
Rns: Throughput of the end-to-end non-split TCP connection Rsp: End-to-end throughput for the split-TCP connection R1: Throughput of TCP1 in the split-connection case R2:
Throughput of TCP2 in the split-connection case T: Round trip latency for the non-split connection T2: Round trip latency for TCP2 in the split-connection case q2: Error rate for the bottleneck link in the wireless network For the non-split case, we will assume that the bottleneck link is located in the wireless network.
Using the formula for TCP throughput developed in , it follows that the TCP throughput for the non-split case is given by Rns51 Tﬃﬃﬃﬃﬃﬃﬃ 3 2q2s (1)
For the split-TCP connection, the end-to-end throughput is given by107 4.3SPLIT-CONNECTION TCP Rsp5minðR1;R2Þ (2)
Because the end-to-end bottleneck for the non-split connection exists in the wireless network, it follows that even for the split case, this node will cause the throughput of TCP2 to be lower than that of TCP1, so that Rsp5R2 (3) Note that the throughput over the access network in the split-connection network is given approximately by R2/C251 T2ﬃﬃﬃﬃﬃﬃﬃ 3 2q2s (4)
This formula is not exact because it was derived in  under the assumption of a persis- tent data source that never runs short of packets to transmit.
In the split-connection case, the data source for TCP2 is the packets being delivered to it by TCP1, which may run dry from time to time.
However, for the case when R 2,R1, the higher speed core network will feed packets faster than the wireless network can handle it, thus leading to a backlog in the receive buffers in the gate- way node.
Hence,  serves as good approximation to the actual throughput.
It follows that Rsp Rns/C25T T2.1 (5) which implies that simply splitting the TCP connection causes the end-to-end throughput to increase.
The intuitive reason for this result is the following.
By splitting the connection, we reduced the round trip latency of each of the resulting split connections compared with the original, thus boosting their throughput.
If the connection over the wireless network is completely lost over extended periods of time, as is common in cellular networks, and then TCP2 stops transmitting, and the gateway ultimately runs out of buffers and shuts down the TCP1 by reducing its receive window size to zero.
This shows that TCP1 is not completely immune to problems in the wireless network even under the split-connection design.
Hence, it is necessary to introduce additional mechanisms to boost TCP2’s performance over and above what TCP Reno can deliver.
Techniques to do this are described in the remainder of this chapter.
From  , it is clear that when T and T 2are approximately equal (as in satellite access networks), then R nsis approximately equal to R sp, that is, there are no performance benefits to splitting the connection.
However,  assumes that plain TCP Reno is being used for TCP2.
If instead we modify TCP2 so that it is more suited to the access network, then we may still see a performance benefit by splitting the connection.
For example, in satellite networks, if we modify the window increment rule for TCP2 and use a much larger window size than the one allowed for by TCP Reno, then there will be a boost in the split network performance.
The main issues with the split TCP design are the following: 
Additional complexity: Split TCP requires that the gateway maintain per-connection state.
In the original design, the splitting was done at the base station, where processing power could be an issue; however, the gateway-based implementation makes this less of an issue because most108  CONGESTION CONTROL IN BWN gateways are based on high-end routers.
Migrating the state from one base station to another on client handoff was another complication in the original design that is avoided in the proposed design.
The end-to-end throughput for split TCP may not show much improvement if the connection over the wireless network is performing badly.
Hence split TCP has to go hand in hand with additional mechanisms to boost the performance of TCP2.
Split TCP violates the end-to-end semantics for TCP because the sending host may receive an ACK for a packet over TCP1 before it has been delivered to the client.
4.4ALGORITHMS TO IMPROVE PERFORMANCE OVER LOSSY LINKS The analysis in  showed that by simply splitting the TCP connection, we got an improvement in TCP throughput because the round trip delay over the wireless network is lower than the end-to-end latency for the non-split connection.
For the case when link conditions in the wireless network are so bad that performance boost caused by the lower latency in the access is overwhelmed by the performance impact of link losses, we need to modify TCP2 to combat the link-specific problems that exist there.
Note that these enhancements can also be implemented for the non-split TCP connection, which will also result in improvement in its performance.
In practice, though, it may be easier to deploy these changes locally for TCP2 at the gateway node because this is under the operator’s control.
The suggestions that have been made to improve TCP performance in the presence of wireless link impairments fall into the following areas:
1.Using available bandwidth estimates (ABEs) to estimate transmit rates: The TCP sender does not keep an estimate of the bottleneck link bandwidth; as a result, when it encounters congestion, it simply cuts down its window size by half.
However, if the sender were able to keep an accurate estimate of the bottleneck link bandwidth (or ABE), then it can reduce its window size in a more intelligent fashion.
This not only helps to reduce convergence time for the algorithm to the correct link rate, but also if the packet loss occurs because of link errors (rather than congestion), then the ABE rate is not reduced, which is reflected in the fact that the resulting sending rate remains high.
As a result, these types of algorithms exhibit better performance in the presence of wireless link losses.
2.Loss Discrimination Algorithm (LDA): On detection of packet loss, if the TCP sender were given the additional information by an LDA about whether the loss was caused by congestion or link errors, then it can improve the congestion control algorithm by reducing its window size only if the loss was caused by congestion and simply retransmitting the missing packet if the loss was caused by link errors. 3.ZRW ACKs:
In case the wireless link gets disconnected over extended periods of time, the TCP sender can undergo multiple time-outs, which will prevent transmission from restarting even after the link comes back up.
To prevent this from occurring, the TCP receiver (or some other node along the path) can shut down the sender by sending a TCP ACK with a zero size receive window, which is known as a ZRW ACK.109 4.4ALGORITHMS TO IMPROVE PERFORMANCE OVER LOSSY LINKS 4.4.1 AVAILABLE BANDWIDTH ESTIMATION AND TCP WESTWOOD A number of TCP algorithms that use ABE to set their window sizes have appeared in the literature.
The original one was proposed by Casetti et al.
[4], and the resulting TCP congestion algorithm was named TCP Westwood.
This still remains one of the more popular algorithms in this category and is currently deployed in about 3% of servers worldwide (see Yalu
The TCP Westwood source attempts to measure t he rate at which its traffic is being received, and thus the bottleneck rate, by f iltering the stream of returning ACKs.
This algorithm does not use out-of-band probing packets to measure thr oughput because they have been shown to signifi- cantly bias the resulting measurement.
It was later shown that the original Westwood proposal overestimates the connection bandwidth be cause of the phenomenon of ACK compression [6].I n this section, we will present a slightly modified version called TCP Westwood 1, which corrects for this problem.
In the above pseudocode, ABE is the estimated connection bandwidth at the source, RTT_min is the estimated value of the minimum round tri p delay, and seg_size is the TCP packet size.
The key idea in this algorithm is to set the windo w size after a congestion event equal to an esti- mate of the bottleneck rate for the connection r ather than reducing the window size by half as Reno does.
Hence, if the packet loss is attributable to link errors rather than congestion, then the window size remains unaffected because the ABE value does not change significantly as a result of packet drops.
Note that from the discussion in , ABE/C3RTT is an estimate of the ideal window size that the connection should use, given estimated bandwidth ABE and round trip latency RTT.
However, by using RTT mininstead of RTT, Westwood sets the window size to a smaller value, thus helping to clear out the backlog at the bottleneck link and creating space for other flows.110  CONGESTION CONTROL IN BWN The key innovation in Westwood was to estimate the ABE using the flow of returning ACKs, as follows:
As pointed out by Mascolo et al.
[7], low-pass filtering is necessary because congestion is caused by the low-frequency components and because of the delayed ACK option.
The filter coeffi- cients are time varying to counteract the fact that the sampling intervals Δkare not constant.
It was subsequently shown [6,7] that the filter (  ) overestimates the available band- width in the presence of ACK compression.
ACK compression is the process in which the ACKs arrive back at the source with smaller interarrival times that do not reflect the bottleneck bandwidth of the connection.
To correct this problem, Mascolo et al.
[7]made the following change to the fil- tering algorithm to come up with Westwood 1: Instead of computing the bandwidth R kafter every ACK is received, compute it once every RTT seconds.
Hence, if D kbytes are acknowledged during the last RTT interval Δk, then Rk5Dk Δk(7) By doing this, Westwood 1evenly spreads the bandwidth samples over the RTT interval and hence is able to filter out the high-frequency components caused by ACK compression.
Note that the window size after a packet loss is given by W’ ^R3T where ^RðtÞ5WðtÞ Ts where T is the minimum round trip latency and T sis the current smoothed estimate of the round trip latency.
This can also be written as W’WT Ts Hence, the change in window size is given by W(T s2T)/T s.
This equation shows that Westwood uses a multiplicative decrease policy, where the amount of decrease is proportional to the queuing delay in the network.
It follows that TCP Westwood falls in the class of additive increase/multiplicative decrease (AIMD) algorithms; however, the amount of decrease may be more or less than that of TCP Reno, depending on the congestion state of the network.
Hence, it is important to investigate its fairness when competing with other Westwood connections and with111 4.4ALGORITHMS TO IMPROVE PERFORMANCE OVER LOSSY LINKS Reno connections.
[4]carried out one such simulation study in which they showed that when competing with Reno connections, Westwood gets 16% more bandwidth than its fair share, which they deemed to be acceptable.
shows the throughput of Westwood and Reno as a function of link error rate for a link of capacity 2 mbps.
As can be seen, there is a big improvement in Westwood throughput over that of Reno when the loss rates are around 0.1%.
Other schemes to estimate the connection bandwidth include the work by Capone et al.
[8]as part of their TCP TIBET algorithm, in which they use a modified version of the filter used in the Core Stateless Fair Queuing Estimation algorithm
[9]and obtained good results.
Another filter was proposed by Xu et al.
This filter was derived from the Time Sliding Window (TSW) estimator that was proposed by Clarke and Fang [11].
4.4.1.1 TCP Westwood Throughput Analysis It has been shown by Greico and Mascolo
[12] that the expected throughput of TCP Westwood in the presence of link error rate p is given by Ravg~1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ pTsðTs2TÞp (8)  can be derived used the machinery developed in  of , and we proceed to so next using the same notation that was used there.
The second term changes because the window decrease rule is different, and the resulting equation is given by dWðtÞ dt5Rðt2TðtÞÞ½12QðtÞ/C138 WðtÞ1Rðt2TðtÞÞQðtÞð^RðtÞT2WðtÞÞ (10)
In we replacedWðtÞ 2, which is window decrease rule for Reno by ^RðtÞT2WðtÞ, which the window change under Westwood.
Assuming that the queuing delay (T s/C0T) can be written as a fraction k of the average round trip latency, Ts2T5kTssuch that 0
#k#1: Then  can be written as Ravg51 Tsﬃﬃﬃﬃﬃ 1 kps On comparing this equation with that for TCP Reno, it follows that TCP Westwood has the effect of a net reduction in link error rate by the fraction k. T
his reduction is greatest when the queuing delay is small compared with the end-to-end latency.
From th is, it follows that Westwood is most effective over long-distance links and least effective in local area network (LAN) environments with a lot of buffering.
4.4.2 LOSS DISCRIMINATION ALGORITHM (LDA) Loss Discrimination Algorithms are useful tools that can be used to differentiate between packet losses caused by congestion versus those caused by link errors.
This information can be used at sender to appropriately react on receiving duplicate ACKs by not reducing the window size if link errors are the cause of the packet drop.
We will describe an LDA that was proposed by Fu and Liew as part of their TCP Veno algorithm
This mechanism was inspired by the rules used by TCP Vegas to detect an increase in queue size along the connection’s path.
Readers may recall from the description of TCP Vegas from  that this algorithm uses the difference between expected throughput and the actual throughput values to gauge the amount of congestion in the network.
In ,Tsis a smoothed value of the measured round trip time.
By Little’s law, D(t) equals the queue backlog along the path.
The congestion indicator in the LDA is set to 0 if D(t) is less than 3 and is set to 1 otherwise.
[13] used this rule in conjunction with Reno’s regular multiplicative decrease rule and a slightly modified additive increase rule and observed a big increase in TCP performance in the presence of random errors.
Another LDA was proposed by Xu et al.
[10] and works as follows: Unlike the LDA described earlier, this LDA requires that the network nodes pass on congestion information back to the source by appropriately marking the ACK packets traveling in the reverse direction if they observe that their average queue length has exceeded some threshold.
The average queue length is computed using a low-pass filter in exactly the same way as for marking packets in the RED algorithm.
The only difference is that a much more aggressive value for the smoothing parameter is used.
Using this LDA, they showed that their algorithm, TCP Jersey, outperformed TCP Westwood for the case when the link error rate equaled or exceeded 1%, and they attributed this improvement to the additional boost provided by the LDA.
4.4.3 ZERO RECEIVE WINDOW (ZRW) ACKs A problem that is unique to wireless is that the link may get disconnected for extended periods, for a number of different reasons.
When this happens, the TCP source will time out, perhaps more than once, resulting in a very large total time-out interval.
As a result, even after the link comes back up, the source may still be in its time-out interval.
The situation described can be avoided by putting the TCP source into a state where it freezes all retransmit timers, does not cut down its congestion window size, and enters a persist mode.
This is done by sending it an ACK in which the receive window size is set to zero.
In this mode, the source starts sending periodic packets called Zero Window Probes (ZWPs).
It continues to send these packets until the receiver responds to a ZWP with a nonzero window size, which restarts the transmission.
There have been two proposals in the literature that use ZRW ACKs to combat link disconnects: 
The Mobile TCP or M-TCP protocol by Brown and Singh [14]: M-TCP falls within the category of split-TCP designs, with the connection from the fixed host terminated at the gateway and a second TCP connection between the gateway and the client.
However, unlike I-TCP, the gateway does not ACK a packet from the fixed host immediately on reception but waits until the packet has been ACK’d by the client, so that the system maintains end-to-end TCP semantics.
In this design, the gateway node is responsible for detecting whether the mobile client is in the disconnect state by monitoring the ACKs flowing back from it on the second TCP connection.
When the ACKs stop coming, the gateway node sends a zero window ACK back to the source node, which puts it in the persist mode.
When the client gets connected again, the gateway sends it a regular ACK with a nonzero receive window in response to a ZWP to get the packets flowing again.
To avoid the case in which the source transmits a114  CONGESTION CONTROL IN BWN window full of packets, all of which get lost over the second connection, so that there is no ACK stream coming back that can be used for the zero window ACK, Brown and Singh suggested that the gateway modify the ACKs being sent to the fixed host, so that the last byte is left un-ACKed.
Hence, when the client disconnect does happen, the gateway can then send an ACK back to the fixed host with zero window indication.
The Freeze TCP protocol by Goff et al.
[15]: In this design, the zero window ACK is sent by the mobile client rather than the gateway.
They also borrowed a feature from an earlier design by Cacares and Iftode
[16] by which when the disconnect period ends, the client sends three Duplicate ACKs for the last data segment it received before the disconnection for the source to retransmit that packet and get the data flowing again.
Note that Freeze TCP can be implemented in the context of the split-TCP architecture, such that the zero window ACKs can be sent by the mobile client to the gateway node to freeze the second TCP connection over the access network.
In general, client disconnect times have been steadily reducing as the mobile hand-off protocols improve from GPRS onward to LTE today.
Hence, the author is not aware of any significant deployment of these types of protocols.
They may be more relevant to wireless local area networks (WLANs) or ad-hoc networks in which obstructions could be the cause of link disconnects.
4.4.4 TCP WITH AVAILABLE BANDWIDTH ESTIMATES AND LOSS DISCRIMINATION ALGORITHMS
The following pseudocode, adapted from Xu et al.
[10], describes the modified congestion control algorithm in the presence of both the ABE and LDA algorithms.
Note that the variable LDA is set to 1 if the sender decides that the packet loss is attributable to congestion and set to 0 if it is attributable to link errors.
Also, SS, CA, fast_recovery, and explicit_retransmit are the usual TCP Reno Slow Start, Congestion Avoidance, Fast Recovery, and packet retransmission procedures as described in .
An example of a rate_control procedure is the one given for TCP Westwood in  .
On the other hand, even if there is no packet loss but congestion is detected, then the source invokes rate_control without doing any retransmits (lines 6 /C09).
4.5LINK-LEVEL ERROR CORRECTION AND RECOVERY A more direct way of solving the problem of link layer reliability is by reducing the effective error rate that is experienced by TCP.
There are several ways this can be done: 
The problem can be attacked at the physical layer by using more powerful error-correcting codes.
For example, LTE uses a sophisticated error correction scheme called Turbo Coding to accomplish this.
Other error correction schemes in use include Reed-Solomon and convolutional coding [17].
Link layer retransmissions or ARQ: As the name implies, the system tries to hide packet loss at the link layer by retransmitting them locally.
This is a very popular (and effective) method, widely used in modern protocols such as LTE and WiMAX.
 Hybrid schemes: Both physical layer error correction and link layer retransmissions can be combined together in an algorithm known as Hybrid ARQ (HARQ).
In these systems, when a packet arrives in error at the receiver, instead of discarding it, the receiver combines its signal with that of the subsequent retransmission, thereby obtaining a stronger signal at the physical layer.
Modern wireless systems such as LTE and WiMAX have deployed HARQ.
From theoretical studies as well as p ractical experience from deploy ed wireless networks, link layer techniques have proven very effectiv e in combating link errors.
As a resu lt, all deployed cellular data net- works, including 3G, LTE, and WiMAX, include powerful ARQ and FEC components at the wireless link layer.
4.5.1 PERFORMANCE OF TCP WITH LINK-LEVEL FORWARD ERROR CORRECTION (FEC) Link-level FEC techniques include algorithms such as co nvolutional coding, turbo coding, and Reed- Solomon coding, and all modern cellular protocols, including 3G and LTE, incorporate them as part of the physical layer.
FEC algorithms were discove red in the 1950s, but until 1980 or so, their use was con- fined to space and military communications becaus e of the large processing power they required.
With116  CONGESTION CONTROL IN BWN the advances in semiconductor technol ogy, link-level FEC is now incorpor ated in every wireless commu- nication protocol today and has started to move up the s tack to the transport and even application layers.
The algorithms have also become better with time; fo r example, turbo coding, w hich was discovered in the 1990s, is a very powerful algorithm that takes the system close to the theoretical Shannon bound.
When compared with ARQ, FEC has some advantages; ARQ results in additional variable latency because of retransmissions that can throw off the TCP r etransmit timer, but this is not an issue for FEC.
This is especially problematic for long satellite l inks, for which ARQ may be an inappropriate solution.
Following Barakat and Altman
[18], we present an analysis of the TCP throughput over a link that is using a link-level block FEC scheme such as Reed-Solomon coding.
X: Number of link-level transmission units (called LL PDUs) formed from a single TCP packet T: Minimum round trip latency for TCP q: Probability that a LL PDU is received in error in the absence of FEC qT: Probability that a LL PDU is received in error in the presence of FEC (N,K): Parameters of the FEC scheme, which uses a block code
This consists of K FEC units of data, to which the codec adds (N-K) FEC units of redundant coding data, such that each LL PDU consists of N units in total.
Because there are KX FEC data units that make up a TCP packet, it follows that the average TCP throughput in the presence of FEC with parameters (N,K) is given by RavgðN;KÞ5minKX Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 3 2pFECðN;KÞs ;K NC !
We now derive an expression for the TCP packet loss probability p(N,K).
A TCP packet is lost when one or more of the X LL PDUs that constitute it arrive in error.
An LL PDU is lost when more than (N-K) of its FEC block units are lost because of transmis- sion errors, which happens with probability qT5XN i5N2K11N i/C18/C19 qið12qÞN2i(16)
#X (17) From  , it is clear that the addition of FEC reduces the loss probability of LL PDUs (and as a result that of TCP packets), which results in an increase in TCP throughput as long as the first term in the minimum in  is less than the second term.
As we increase N, we reach a point where the two terms become equal, and at this point, the FEC entirely eliminates all link errors.
Any increase in FEC beyond this point results in the deterioration of TCP throughput (i.e., there is more FEC than needed to clean up the link).
This implies that the optimal amount of FEC is the one that causes the two terms in  to be equal, that is, NX Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 3 2pFECðN;KÞs 5C (18)
Increasing N/K while keeping K fixed leads to more FEC blocks per LL PDU, which increases the resulting throughput.
also plots (K/N)C and the intersection of this curve with the throughput curve is the point that is predicted by  with the optimal amount of FEC.
The analysis presented assumed that the LL PDU loss probabilities form an iid process.
A more realistic model is to assume the Gilbert loss model, in which errors occur in bursts.
This reflects the physics of an actual wireless channel in which burst errors are common because of link fading conditions.
However, this complicates the analysis considerably, and a closed form simple expres- sion is no longer achievable.
Interested readers are referred to Barakat and Altman [18] for details of the analysis of this more complex model.
4.5.2 PERFORMANCE OF TCP WITH LINK-LEVEL AUTOMATIC RETRANSMISSIONS AND FORWARD ERROR CORRECTION Link-level ARQ or retransmission is an extremely effective way of reducing the link error rate.
(B)Variation of TCP throughput with number of ARQ retransmissions.119 4.5LINK-LEVEL ERROR CORRECTION AND RECOVERY system
[19] that has been commercially deployed.
As shown in A , even with an error rate as high as 10%, the TCP throughput remains at 40% of the maximum.
B shows that the throughput performance improves as the number of ARQ retransmissions is increased, but after six or so retransmissions, there is no further improvement.
This is because at high error rates, the number of retransmissions required to recover a packet grows so large that the resulting increase in end-to-end latency causes the TCP retransmit timer to expire, thus resulting in a time-out.
This observation also points to an important issue when using ARQ at the link layer.
Because of its interaction with TCP’s retransmissions, ARQ can in fact make the overall performance worse if not used carefully.
Hence, some guidelines for its use include: 
The maximum number of ARQ retransmissions should be capable of being configured to a finite number, and if the LL-PDU is still received in error, then it should be discarded (and let TCP recover the packet).
The max number of retransmissions should be set such the total time to do so is significantly less than the TCP retransmit time-out (RTO) interval.
ARQ should be configurable on a per connection basis, so that connections that do not need the extra reliability can either turn it off or reduce the number of retransmissions.
Modern implementations of ARQ in wireless protocols such as LTE or WiMAX follow these rules in their design.
In this section, following Barakat and Fawal [20], we extend the analysis in  to analyze the performance of TCP over a link implementing a Selective Repeat ARQ scheme (in addition to FEC) and in-order delivery of packets to the IP layer at the receiver.
We will reuse the definitions used in  , so that a TCP packet of size KX FEC blocks is split into X LL PDUs, each of which is made up of K FEC blocks (see  ).
To each of these K FEC blocks we add (N-K) redundant FEC blocks to obtain a Reed Solomon /C0encoded LL PDU of size N FEC blocks.
If the FEC does not succeed in decoding an LL PDU, the SR-ARQ scheme is invoked to recover it by doing retransmissions.
The retransmissions will be done a maximum number of times, denoted by M. If the LL PDU fails to get through even after M retransmissions, the ARQ-SR assumes that it cannot be recovered and leaves its recovery to TCP.
Recall from  in  that the expression for average TCP throughput in the presence of random loss and time-outs is given by Ravg5minKX EðTARQÞﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2pARQ 3q 1RTO UpARQð1132p2
ARQÞmin 1 ;3ﬃﬃﬃﬃﬃﬃﬃﬃﬃ 3pARQ 8q/C18/C19 ;αC0 BB@1 CCA(19) where the throughput is in units of FEC units per second and αis the fraction of the link bandwidth lost because of the FEC and LL PDU retransmissions, p ARQ is the probability that a TCP packet is lost despite ARQ, E(T ARQ) is the average round trip latency, C is the capacity of the link, and RTO is the TCP timeout value.
Note that in  , we have replaced T, which is the nonqueuing part of the round trip latency in  of , by the average value E(T ARQ).
The reason for this is that in the presence of ARQ, the nonqueuing part of the round trip latency is no longer fixed but varies randomly depending on the number of link retransmissions.120  CONGESTION CONTROL IN BWN As in the previous section, we will assume that q is the probability that an LL PDU transmission fails because of link errors in the absence of FEC and ARQ.
Note that to find R avg, we need to find expressions for E(T ARQ), p ARQ, and αas functions of the link, ARQ, and FEC parameters q, N, K, M, and X. The ARQ-SR receiver acknowledges each LL PDU either with an ACK or a NACK.
The LL PDUs that are correctly received are resequenced before being passed on to the IP layer.
A TCP packet is lost if one or more of its constituent LL PDUs is lost.
Under the assumption that LL PDUs are lost independently of each other with loss rate q F, we obtain the probability of a TCP packet loss as pARQ512ð12qFÞX(20)
We now obtain an expression for q F. Note that an LL PDU is lost if all M retransmissions fail, in addition to the loss of the original transmission, for a total of (M 11) tries.
The probability that a single LL PDU transmission fails in the absence of ARQ but with FEC included is given by qT5XN i5N2K11N i/C18/C19 qið12qÞN2i(21) so that the probability that an LL PDU is lost after the M ARQ retransmissions is given by qF5qM11 T (22)
By substituting  in , we obtain the following expression for the TCP packet loss probability p ARQin
M112 43 5X (23) To obtain an expression for α, which is the amount of link bandwidth left over after the FEC and ARQ overheads are accounted for, note the following: 
A fraction (K/N) of the link bandwidth is lost because of the FEC overhead.
A fraction (1 2qT) of the link bandwidth is lost because of dropping of LL PDUs that arrive in error.
A fraction (1 2qF)X21of the link bandwidth is lost because of LL PDUs that arrive intact but are dropped anyway because one or more of the other (X 21) LL PDUs that are part of the same TCP packet do not arrive intact, despite the M retransmissions.
Hence, the fraction of the link bandwidth that is available for TCP packet transmission is given by α5ð12qFÞX21ð12qTÞK N(24)
We next turn to the task of finding the average round trip latency E(T ARQ) for the TCP packets.
Define the following: τ: The time interval from the start of transmission of an LL PDU and the receipt of its acknowledgement121 4.5LINK-LEVEL ERROR CORRECTION AND RECOVERY D: One-way propagation delay across the link Mi: Number of retransmissions for the i thpacket, 1 #i#X, until it is delivered error free.
Note thatMi#M. T0: Average round trip latency for the TCP packet, excluding the delay across the bottleneck link t, t0:
Random variables whose average equals T and T 0, respectively Then τis given by τ52D1N C(25) Assume that all the X LL PDUs that make up a TCP packet are transmitted back to back across the link.
Then the ith LL PDU starts transmission at timeði21ÞN C, and it takesN C12D1Miτseconds to finish its transmission and receive a positive acknowledgement.
Hence, it follows that t5t012D1max 1#i#XiN C1Miτ/C18/C19 (26) We will now compute the expected value of t under the condition that the TCP packet whose delay is being computed has been able to finish its transmission.
Next we derive a formula for the probability distribution for M i. Note that the probability that all X LL PDUs that make up the packet are received successfully (after ARQ) is given by ð12qM11 TÞXbecause the qM11 Tis the probability that an LL PDU is received in error even after M retransmissions.
This allows us to evaluate T in  and subsequently the average TCP throughput by using  .
4.6THE BUFFERBLOAT PROBLEM IN CELLULAR WIRELESS SYSTEMS
Until this point in this chapter, we have focused on the random packet errors as the main cause of performance degradation in wireless links.
With advances in channel coding and smart antenna technology, as well as powerful ARQ techniques such as HARQ, this problem has been largely solved in recently designed protocols such as LTE.
However, packet losses have been replaced by another link related problem, which is the large variability in wireless link latency.
This can also lead to a significant degradation in system performance as explained below.
Recall from  that for the case when ther e are enough buffers to accommodate the maximum window size W max, the steady-state maximum buffer occup ancy at the bottleneck link is given by: bmax5Wmax2CT (33)
From this we concluded that if the maximum window size is kept near CT, then the steady state buffer occupancy will be close to zero, which is the ideal operating case.
In reaching this conclu- sion, we made the critical assumption that C is fixed, which is not the case for the wireless link.
If C varies with time, then one of the following will happen: I f C .Wmax/T, then b max50,
i that is, there is no queue build at the bottleneck, and in fact the link is underused I f C ,, Wmax/T, then a large steady-state queue backlog will develop at the bottleneck, which will result in an increase in the round trip latency if the buffer size B is large.
This phenomenon is known as bufferbloat.
Note that buffer size B at the wireless base station is usually kept large to account for the time needed to do ARQ and to keep a reservoir of bits ready in case the capacity of the link suddenly increases.
Because C is constantly varying, the buffer will switch from the empty to the full state frequently, and when it is in the full state, it will cause a degradation in the performance of all the interactive applications that are using that link.
Queuing delays of the order of several seconds have been measured on cellular links because of this, which is unacceptable for applications such as Skype, Google Hangouts, and so on.
Also, the presence of a large steady-state queue size means that transient packet bursts, such as those generated in the slow-start phase of a TCP session, will not find enough buffers, which will result in multiple packet drops, leading to throughput degradation.
There are two main causes for the variation in link capacity: 
ARQ related retransmissions: If the ARQ parameters are not set properly, then the retransmissions can cause the link latency to go up and the capacity to go down.123 4.6THE BUFFERBLOAT PROBLEM IN CELLULAR WIRELESS SYSTEMS 
Link capacity variations caused by wireless link adaptation: This is the more important cause of capacity changes and is unique to wi reless links.
To maintain the robustness of transmissions, the wireless base station const antly monitors the quality of the link to each mobile separately, and if it detects problems, then it tries to make the link more robust by changing the modulation, coding or both.
For instance, in LTE, the modulation can vary from a high of 256-QAM to a low of QPSK, and in the latter case, the link capacity is less than 25% of the former.
This link adaptation is done on a user-by-user basis so that any point in time different mobiles get different performance depending on their link condition.
The base station implements this system, by g iving each mobile its own buffer space and a fixed time slot for transmission, and it serves the mobiles on a round robin basis.
Depending on the current modulation and coding, different mobiles transmit different amounts of data in their slot, which results in the varying link capacity (  ).
In  mobile 1 has the best link, so it uses the best modulation, which enables it to fit 100 bytes in its slot; mobile 2 has the worst link and the lowest modulation, so it can fit only 50 bytes in its slot.
From the description of the link scheduling that was given earlier, it follows that the traffic belonging to different mobiles does not get multiplexed together at the base stationSlot 1 Mobile 1 100 bytesSlot 2 Mobile 2 50 bytesSlot 3 Mobile 3 75 bytes Mobile 1 Buffer Mobile 2 Buffer Mobile 3 BufferMobile 1 Mobile 2 Mobile 3  Downlink scheduling at a wireless base station.124  CONGESTION CONTROL IN BWN (i.e., as shown in  , a buffer only contains traffic going to single mobile).
Hence, the congestion that a connection experiences is “sel f-inflicted” and not attributable to the conges- tive state of other connections.
Note that the rule of thumb that is used in wireline links to size up the buffer size (i.e., B 5CT) does not work any longer because the optimal size is constantly varying with the link capacity.
The bufferbloat problem also cannot be solved by simply reducing the size of the buffer because this will cause problems with the other functions that a base station has to carry out, such as reserving buffer space for retransmissions or deep packet inspections.
4.6.1 ACTIVE QUEUE MANAGEMENT (AQM) TECHNIQUES TO COMBAT BUFFERBLOAT AQM techniques such as Random Early Detection (RED) were designed in the mid 1990s but have not seen widespread deployment because of the difficulty in configuring them with the appropriate parameters.
Also, the steadily increasing link speeds in wireline networks kept the network queues under control, so operators did not feel the need for AQM.
However, the bufferbloat problem in cellular networks confronted them with a situation in which the size of the bottleneck queue became a serious hindrance to the system performance; hence, several researchers have revisited the use of traditional RED in controlling the queue size; this work is covered in  .
[21] have recently come up with a new AQM scheme called Controlled Delay (CoDel) that does not involve any parameter tuning and hence is easier to deploy.
It is based on using delay in the bottleneck buffer, rather than its queue size, as the indicator for link conges- tion.
This work is covered in  .
4.6.1.1 Adaptive Random Early Detection Recall from the description of RED in , that the algorithm requires that the operators choose the following parameters: the queue thresholds min thand max th, the maximum dropping probability max pand the averaging parameter w q.
If the parameters are not set properly, then the analysis in  showed that it can lead to queue length oscillations, low link utilizations, or buffer saturation, resulting in excessive packet loss.
Using the formulae in , it is possible to choose the appropriate parameters for a given value of the link capacity C, the number of connections N, and the round trip latency
T. However, if any of these parameters is varying, as C is in the wireless case, then the optimal parameters will also have to change to keep up with it (i.e., they have to become adaptive).
Adaptive RED (ARED) was designed to solve the problem of automatically setting RED’s parameters [22].
It uses the “gentle” version of RED, as shown in  .
The operator has to set a single parameter, which is the target queue size, and all the other parameters are automatically set and adjusted over time, as a function of this.
The main idea behind the adaptation in ARED is the following: With reference to  , assuming that averaging parameter w qand the thre- sholds min thand max thare fixed, the packet drop probability increases if the parameter max p is increased and conversely decreases when max pis reduced.
Hence, ARED can keep the buffer125 4.6THE BUFFERBLOAT PROBLEM IN CELLULAR WIRELESS SYSTEMS occupancy around a target level by increasing max pif the buffer size is more than the target and decreasing max pif the buffer size falls below the target.
This idea was originally proposed by Feng et al.
[23] and later improved upon by Floyd et al.
[22], who used an AIMD approach to vary- ing max p, as opposed to multiplicative increase/multiplicative decrease (MIMD) algorithm used by Feng et al.
Decrease max p: max p’max p*b 1 The parameters in this algorithm are set as follows:
The algorithm is run periodically every interval seconds, where interval 50.5 sec a: The additive increment factor is set to a 5min(0.01, max p/4).minth maxthmaxp1.0 Average Queue Sizepb 2maxth  Random Early Detection (RED) packet drop/marking probability for “gentle” RED.126  CONGESTION CONTROL IN BWN b: The multiplicative decrement factor is set to 0.9.
target: The target for the average queue size is set to the interval target5½min th10:4ðmax th2min thÞ;min th10:6ðmax th2min thÞ/C138 max th: Is set to 3/C3min th min th: This is the only parameter that needs to be manually set by the operator.
ARED was originally designed for wireline links, in w hich the capacity C is fixed and the variability is attributable to the changing load on the link (i. e., the number of TCP connections N).
The algorithm needs to be adapted for the wireless case, in particular the use of  to choose w qneeds to be clarified because C is now varying.
If w qis chosen to be too large relative to the link speed, then it causes queue oscillations because t he average tends to follow the instantan eous variations in queue size.
Hence, one option is to choose the maximum possible value for the link capacity in  ,w h
correspond to the mobile occupying the entire channel at the maximum modulation.
Also, the value of interval needs to be revisited because the time intervals during which the capacity changes are likely to be different than the time intervals during which the load on the link changes.
[22] showed through simulations that with interval 50.5 sec, it takes about 10 sec for ARED to react to an increase in load and bring the queue size back to the target level.
This is likely too long for a wireless channel, so a smaller value of interval is more appropriate.
4.6.1.2 Controlled Delay (CoDel) Active Queue Management The CoDel algorithm is based on the following observations about bufferbloat, which were made by Nichols and Jacobsen [21]: 
Networks buffers exist to absorb transient packet bursts, such as those that are generated during the slow-start phase of TCP Reno.
The queues generated by these bursts typically disperse after a time period equal to the round trip latency of the system.
They called this the “good” queue.
In a wireless system, queue spikes can also be caused to temporary decreases in the link capacity.
 Queues that are generated because of a mismatch between the window size and the delay bandwidth product, as given by  , are “bad” queues because they are persistent or long term in nature and add to the delay without increasing the throughput.
These queues are the main source of bufferbloat, and a solution is needed for them.
Based on these observations, they gave their CoDel algorithm the following properties: 
Similar to the ARED algorithm, it is parameter-less 
It treats the “good” queue and “bad” queues, differently, that is, it allows transient bursts to pass through while keeping the nontransient queue under control.
 Unlike RED, it is insensitive to round trip latency, link rates, and traffic loads.
 It adapts to changing link rates while keeping utilization high.
The main innovation in CoDel is to use the packet sojourn time as the congestion indicator rather than the queue size.
This comes with the following benefits: Unlike the queue length, the sojourn time scales naturally with the link capacity.
Hence, whereas a larger queue size is acceptable if the link rate is high, it can become a problem when the link rate decreases.
This127 4.6THE BUFFERBLOAT PROBLEM IN CELLULAR WIRELESS SYSTEMS change is captured by the sojourn time.
Hence, the sojourn time reflects the actual congestion expe- rienced by a packet, independent of the link capacity.
CoDel tracks the minimum sojourn time experienced by a packet, rather than the average sojourn time, because the average can be high even for the “good” queue case (e.g., if a maximum queue size of N disperses after one round trip time, the average is still N/2).
On the other hand, if there is even one packet that has zero sojourn time, then it indicates the absence of a persistent queue.
The minimum sojourn time is tracked over an interval equal to the maximum round trip latency over all connections using the link.
Moreover, because the sojourn time can only decrease when a packet is dequeued, CoDel only needs to be invoked at packet dequeue time.
To compute an appropriate value for the target minimum sojourn time, consider the following equation for the average throughput that was derived in  of : Ravg50:75 12β ð11βÞ2hi C where β5B CT(35) Even at β50:05, gives Ravg50:78C. Because βcan be also interpreted as β5(Persistent sojourn time)/T, it follows that the above choice can also be interpreted as: The per- sistent sojourn time threshold should be set to 5% of the round trip latency, and this is what Nichols and Jacobsen recommend.
The sojourn time is measured by time stamping every packet that arrives into the queue and then noting the time when it leaves the queue.
When the minimum sojourn time exceeds the target va lue for at least one round trip interval, a packet is dropped from the tail of the queue.
The next dropping interval is decreased in inverse proportion to the square root to the number of drops since the dropping state was entered.
As per the analysis in , this leads to a gradual linear decrease in the TCP t hroughput, as can be seen as follows: Let N(n) and T(n) be the number of packets transmitted in nthdrop interval and the duration of the nthdrop interval, respectively.
Then the TCP throughput R(n) during the nthdrop interval is given by RðnÞ5NðnÞ TðnÞn51;2;...
Because the increase in window size during each drop interval is proportional to the size of the interval, it fol- lows that WmðnÞ~1ﬃﬃnp, from which it follows from  thatNðnÞ~1 n. Plugging these into  , we finally obtain that RðnÞ~1ﬃﬃnp.
The throughput decreases with n until the minimum sojourn time falls below the threshold at which time the controller leaves the dropping state.
In addition, no drops are carried out if the queue contains less than one packet worth of bytes.
One way of understanding the way CoDel works is with the help of a performance measure called Power, defined by Power5Throughput Delay5Rav Ts128  CONGESTION CONTROL IN BWN Algorithms such as Reno and CUBIC try to max imize the throughput without paying any attention to the delay component.
As a result, w e see that in variable capacity links, the delay blows up because of the large queues that result.
AQM algorithms such as RED (and its variants such as ARED) and CoDel, on the other hand, can be used to maximize the Power instead by trading off some of the throughput for a lower delay.
On an LTE link, simulations have shown that Reno achieves almost two times the throughput when CoDel is not used; hence, the tradeoff is quite evident here.
The reasons for the lower throughputs with CoDel are the following:  As shown using  , even without a variable link capacity, the delay threshold in CoDel is chosen such that the resulting average throughput is about 78% of the link capacity.
In a variable capacity link, when the capacity increases, it is better to have a large backlog (i.e., bufferbloat) because the system can keep the link fully utilized.
With a scheme such as CoDel, on the other hand, the buffer soon empties in this scenario, and as a result, some of the throughput is lost.
Hence, CoDel is most effective when the application needs both high throughput as well as low latency (e.g., video conferencing) or when lower speed interactive applications such as web surfing are mixed with bulk file transfers in the same buffer.
In the latter case, CoDel considerably improves the latency of the interactive application at the cost of a decrease in the throughput of the bulk transfer application.
If Fair Queuing is combined with CoDel, so that each application is given its own buffer, then this mixing of traffic does not happen, in which CoDel can solely be used to improve the performance of applications of the video conferencing type.
The analysis presented above also points to the fact that the most appropriate buffer management scheme to use is a function of the application.
This idea is explored more fully in .
4.6.2 END-TO-END APPROACHES AGAINST BUFFERBLOAT AQM techniques against bufferbloat can be deployed only if the designer has access to the wireless base station whose buffer management p olicy is amenable to being changed.
If this is not the case, then one can use congestion control schemes that work on an end-to-end basis but are also capable of controlling delay along the ir path.
If it is not feasible to change the congestion algorithm in the remote-end server, then the split-TCP architecture from  can be used (see  ).
In this case, the new algorithm can be deployed on the gateway (or some other box along the path) while the leg acy TCP stack continues to run from the server to the gateway.
In this section, we will describe a congestion control algorithms called Low Extra Delay Background Transport (LEDBAT) that is able to regulate the connections’ end-to-end latency to some target value.
We have already come across another algorithm that belongs to this category (i.e., TCP Vegas; see ).
In general, protocols that use queuing delay (or queue size) as their congestion indicator, have an intrinsic disadvantage when competing with protocols such as Reno that use packet drops as the congestion indicator.
The former tends to back off as soon as the queues start129 4.6THE BUFFERBLOAT PROBLEM IN CELLULAR WIRELESS SYSTEMS to build up, allowing the latter to occupy the a ll the bandwidth.
However, if the system is designed in the split-connection approach, then the operator has the option of excluding the Reno-type algorithm entirely from the wireless subnetwork, so the unfairness issue will not arise in practice.
4.6.2.1 TCP LEDBAT LEDBAT has been Bit Torrent’s default transport protocol since 2010
[24] and as a result now accounts for a significant share of the total traffic on the Internet.
LEDBAT belongs to the class of Less than Best Effort (LBE) protocols, which are designed for transporting background data.
The design objective of these protocols is to grab as much of the unused bandwidth on a link as possible, and if the link starts to get congested, then quickly back off.
Even though LEDBAT was designed to be a LBE protocol, its properties make it suitable to be used in wireless networks suffering from bufferbloat.
This is because LEDBAT constantly monitors its one-way link latency, and if it exceeds a configurable threshold, then it backs off its congestion window.
As a result, if the wireless link capacity suddenly decreases, then LEDBAT will quickly detect the resulting increase in latency caused by the backlog that is created and decrease its send- ing rate.
Note that if the split-TCP design is used, then only LEDBAT flow will be present at the base station (and furthermore, they will be isolated from each other), so that LEDBAT’s reduction in rate in the presence of regular TCP will never be invoked. LEDBAT requires that that each packet carry a timestamp from the sender, which the receiver uses to compute the one-way delay from the sender, and sends this computed value back to the sender in the ACK packet.
The use of the one-way delay avoids the complications that arise in other delay-based protocols such as TCP Vegas that use the round trip latency as the congestion signal, and as a result delays experienced by ACK packets in the reverse link introduce errors in the forward delay estimates.
Define the following: θðtÞ: Measured one-way delay between the sender and receiver τ: The maximum queuing delay that LEDBAT may itself introduce into the network, which is set to 100 ms or less T: Minimum measured one-way downlink delay γ: The gain value for the algorithm, which is set to one or less.
The window increase /C0decrease rules for LEDBAT are as follows (per RTT):
If the current queuing delay given by θðtÞ2T, exceeds the target τ, then the window is reduced in proportion to130  CONGESTION CONTROL IN BWN their ratio.
When the measured queuing delay less that the target τ, then LEDBAT increases its window, but the amount of increase per RTT can never exceed 1.
There are some interesting contrasts between LEDBAT and Westwood even though their designs are similar: 
Whereas Westwood uses a smoothed estimate of the round trip latency to measure congestion, LEDBAT uses the latest unfiltered 1-way latency.
 LEDBAT is less aggressive in its window increase rule because Westwood follows Reno in increasing its window by 1 every RTT, but LEDBAT uses a window increase that is less than 1. 
The rule that LEDBAT uses to decrease its window size in congestion is similar to the one that Westwood uses on encountering a packet loss, as seen below: LEDBAT: ΔW5γθ2T τ21/C20/C21 Westwood: ΔW5WTs2T Ts/C18/C19 Hence, in both cases, the decrease is proportional to the amount of congestion delay, but in LEDBAT, the decrease is additive, but in Westwood, it is multiplicative.
Note that even though two different clocks are used to measure the one-way delays in LEDBAT because only the difference θðtÞ2Tappears in  , any errors attributable to a lack of syn- chronization get cancelled out.
LEDBAT has been shown to have intraprotocol fairness issue if multiple LEDBAT connections are competing for bandwidth at a single node.
If the bottleneck is at the wireless base station, then interaction between LEDBAT flows from different mobiles should not be an issue, since the per mobile queues are segregated from each other.
The following rules, which are based on the material in this chapter, can be used for designing con- gestion control systems that traverse wireless links: 
Compute the quantity p(CT)2for the system where p is the packet drop rate.
If this number is around 8/3, then most of the packet drops are to buffer overflows rather than link errors, so no extra steps are needed to bring down the link error rate (refer to the discussion in  of  for the justification for this rule).
If p(CT)2..8/3, then the packet drops are being caused predominantly because of link errors.
Furthermore, the link error rate is too high, and additional steps need to be taken to bring it down.
If the wireless link supports Reed-Solomon FEC, then use  to compute p FEC, which is the link error rate in the presence of FEC.
If p FEC(CT)2B8/3, then FEC is sufficient for good performance.
If the wireless link supports ARQ, then use  to compute p ARQ, which is the link error rate with both ARQ131 4.7SOME DESIGN RULES and FEC.
If p ARQ(CT)2B8/3 for a certain number of retransmissions M, then link-level ARQ1FEC is sufficient for good performance.
If FEC or ARQ are not available on the wireless link or if they do not result in a sufficient reduction in the error rate, then use the Transport layer techniques from Sections 4.3 and 4.4 . /C0If
the TCP stack at the source is under the designer’s control, then techniques such TCP Westwood with ABE, LDA, or ZRW can be implemented.
/C0If the TCP stack at the source is not under the designer’s control, then use a split-level TCP design, with the splitting point implemented at a gateway-type device.
In this case, techniques such as Westwood, ABE, or LDA can be implemented on the TCP2 connection.
If the link error rate is zero but the link shows large capacity variations, then bufferbloat may occur, and the following steps should be taken to limit it: 
If the base station software can be modified, then AQM schemes such ARED or CoDel should be used.
If AQM is not an option, then the delay-based TCP protocols such as LEDBAT or Vegas should be used.
This should be done in conjunction with a split-connection TCP architecture so that regular TCP continues to be used in the core packet network.
4.8FURTHER READING The techniques discussed in this chapter to mitigate packet losses work at either the transport layer (e.g., Westwood) or at the link layer (e.g., FEC or ARQ).
However, some algorithms work cross-layer; the most well known among these is an algorithm called Snoop
Snoop can be implemented at an intermediate point along the session path, either at the gateway or the base station.
It works as follows: Downlink packets from the source are intercepted by Snoop and stored until it detects an ACK for that packet from the client.
If a packet is lost, then Snoop intercepts the duplicate ACKs and suppresses them.
It then retransmits the missing packet from its local cache, so that the source does not become aware of the loss.
Hence, in some sense, Snoop operates like a TCP-aware link-level ARQ protocol.
Wireless links sometimes exhibit a large increase in their end-to-end latency because of link- level ARQ retransmissions or because of the extra latency during handoffs.
This causes the TCP retransmit timer to time out even though no packets were lost.
When the ACK for the retransmitted packet arrives, the source is not able to tell whether this ACK was for the original transmission or for the retransmission (this is called the ACK ambiguity problem) and assumes the latter.
This leads to the retransmission of the entire window of packets.
To correct this problem, Ludwig and Katz
[27] designed an algorithm called TCP Eifel, which solves the ACK ambiguity problem by time stamping every TCP data packet and then having the receiver time stamp returning ACKs with the time stamp of the packet being ACK’d.132  CONGESTION CONTROL IN BWN
[1] Lakshman TV, Madhow U. Window based error recovery and flow control with a slow acknowledgement channel: a study of TCP/IP performance.
[2] Varma S. Performance and buffering requirements for TCP applications in asymmetric networks.
I-TCP: indirect TCP for mobile hosts.
Proceedings of the fifteenth international conference on distributed computing systems, 1995.
Westwood: end-to-end congestion control for wired/wireless networks.
[5] Yang P, Luo W, Xu L, Deogun J, Lu Y. TCP congestion avoidance algorithm identification.
International Conference on Distributed Computing Systems, 2011.
[6] Greico LA, Mascolo S. End-to-end bandwidth estimation for congestion control in packet networks. Presented at the second international workshop on QoS-IP, 2003.
Performance evaluation of Westwood 1TCP congestion control.
[8] Capone A, Fratta L, Martignon F. Bandwidth estimation schemes for TCP over wireless networks.
[9] Stoica I, Shenker S, Zhang H. Core-stateless fair queueing: achieving approximately fair bandwidth allocation in high speed networks.
[10] Xu K, Tian Y, Ansari N. TCP-Jersey for wireless communications.
[11] Clark DD, Fang W. Explicit allocation of best effort packet delivery service.
[12] Grieco LA, Mascolo S. Mathematical analysis of Westwood 1TCP congestion control.
IEE Proc Control Theory Appl.
TCP Veno: TCP enhancement for transmission over wireless access networks.
[15] Goff T, Phatak DS, Gupta V. Freeze TCP: A true end-to-end TCP enhancement mechanism for mobile environments.
[16] Cacares R, Iftode L. Improving the performance of reliable transport protocols in mobile computing environments.
[18] Barakat C, Altman E. Bandwidth tradeoff between TCP and link-level FEC.
[19] Varma S, Michail T. Early results from next generation wireless broadband systems.
[20] Barakat C, Fawal A. Analysis of link-level hybrid FEC/ARQ for wireless links and long lived TCP traffic.
[21] Nichols K, Jacobsen V. Controlled delay active queue management.
An algorithm for increasing the robustness of RED’s Active Queue Management, Technical Report.
A self configuring RED gateway.
[25] Balakrishnan H., Amir E., Katz R.H. Improving TCP/IP performance over wireless networks.
ACM conference on mobile computing and networking, 1995.
[26] Balakrishnan H, Seshan S, Katz R. Improving reliable transport and handoff performance in cellular wireless networks.
The Eifel algorithm: making TCP robust against spurious retransmissions. ACM SIGCOMM 2000;30(1):30 /C06. SUGGESTED READING Baker F, Fairhurst G. IETF recommendations regarding active queue management.
Balakrishnan H, Padmanabhan VN, Seshan S, Katz RH.
A comparison of mechanisms for improving TCP performance over wireless links.
Chan MC, Ramjee R. Improving TCP/IP performance over third generation wireless networks.
Chaskar HM, Lakshman TV, Madhow U. TCP over wireless with link level error control: Analysis and design methodology.
De Cicco L, Carlucci G, Mascolo S. Experimental investigation of the Google congestion control for real-time flows.
Fighting the bufferbloat: on the coexistence of AQM and low priority congestion control.
Rhee I. Tackling bufferbloat in 3G/4G networks.
The new AQM kids on the block: much ado about nothing?
University of Oslo Research Report, 2014.
Kuhn N, Lochin E, Mehani O. Revisiting old friends: is CoDel really achieving what RED cannot?
Sundararaj S, Duchamp D. Analytical characterization of the throughput of a split TCP connection.
MS Thesis, Dept. of Computer Science, Stevens Institute of Technology, 2002.
Wei W, Zhang C, Zang H, et al. Inference and evaluation of split connection approaches in cellular data networks.
Winstein K, Sivaraman A, Balakrishnan H. Stochastic forecasts achieve high throughput and low delay over cellular networks.
Usenix NSDI 2013.134  CONGESTION CONTROL IN BWN CHAPTER 5CONGESTION CONTROL IN HIGH-SPEED NETWORKS 5.1INTRODUCTION Link speeds have exploded in the past 2 decades from tens of megabits to multiple gigabits per second for both local as well as long-distance networks.
The original congestion control algorithms for TCP, such as Reno and Tahoe, worked very well as long as the link speeds were lower but did not scale very well to the higher speeds and longer propagation delays for several reasons: 
In , , by using a control theory framework, it was shown that TCP Reno becomes unstable if either the link capacity, the propagation delay, or both become large.
This results in severe oscillations of the transmission rates and buffer occupancies, which has been observed in practice in high-speed networks.
For TCP to grow its window from the midpoint (after a multiplicative decrease) to full window size during the congestion avoidance phase, it will require 50,000 RTTs, which is about 5000 seconds (1.4 hours).
If a connection finishes before then, the link will be severely underutilized.
Hence, when we encounter a combination of high capacity and large end-to-end delay, the multiplicative decrease policy on packet loss is too drastic, and the linear increase is not quick enough. 
The square-root formula for TCP throughput (see ) shows that for a round trip latency of T, the packet drop rate has to be less than1:5 ðRTÞ2to sustain an average throughput of R. Hence, using the same link and TCP parameters as above, the packet drop rate has to be less than 10210to sustain a throughput of 10 Gbps.
This is much smaller than the drop rate that can be supported by most links.
A number of alternate congestion control algorithms for high-speed networks were suggested subsequently to solve this problem, including Binary Increase Congestion control (BIC)
The Linux family of operating systems offers users a choice of one of these algorithms to use (in place of TCP Reno) while using TCP CUBIC as the default algorithm.
The Windows family, on the other hand, offers a choice between Reno and CTCP.
As a result of the wide usage of Linux on web servers, it has recently been estimated [8]that about 45% of the servers on the Internet use either BIC or CUBIC, about 30% still use TCP Reno, and another 20% use CTCP.
In this chapter, we provide a description and analysis of the most popular high-speed congestion control algorithms, namely HSTCP, BIC, CUBIC, and CTCP.
Each of these algorithms modifies the TCP window increase and decrease rules in a way such that the system is able to make better utilization of high-speed and long-distance links.
We also discuss two other protocols, namely eXpress Control Protocol (XCP) and Rate Control Protocol (RCP).
These need multi-bit network feedback, which has prevented their deployment in Internet Protocol (IP) networks.
However, sev- eral interesting aspects of their design have influenced subsequent protocol design.
Whereas XCP and RCP approach the high-speed congestion control problem by using an advanced Active Queue Management (AQM) scheme, the other algorithms mentioned are server based and designed to work with regular tail-drop or Random Early Detection (RED) routers.
5.9 contains some observations on the stability of these algorithms.
In networks with large link capacities and propagation delays, the congestion control protocol needs to satisfy the following requirements: 1.It should be able to make efficient use of the high-speed link without requiring unrealistically low packet loss rates.
As poi nted out in the introduction, this is not the case for TCP Reno, because of its conservativ e window increase and aggressive window decrease rules.
2.In case of very high-speed links, the requirement of intraprotocol fairness between the high speed TCP protocol and regular TCP is relaxed because regular TCP is not able to make full use of the available bandwidth.
Hence, in this situation, it is acceptable for the high speed TCP variants to perform more aggressively than standard TCP.
3.If connections with different round trip latencies share a link, then they should exhibit good intraprotocol fairness.
Define the Response Function w of a congestion control algorithm as w 5Ravg.
T packets per round trip time (ppr), where R avgis the average throughput and T is the minimum round trip latency.
This is sometimes also referred to as the average window because it is the average number of packets transmitted per round trip time.
A basic tool used to evaluate high-speed designs is the log-log plot of the Response Function versus p (  ).
Note that the ability of a protocol to use the high amount of bandwidth available in high-speed networks is determined by whether it can sustain high sending rates under low loss rates.
From  , it follows that a protocol becomes more scalable if its sending rate is higher under lower loss rates.136  CONGESTION CONTROL IN HIGH-SPEED NETWORKS From , the Response Function for TCP Reno is given by w5ﬃﬃﬃﬃﬃ 3 2ps or logw50:0920:5 log p (1)
This plot is shown in  , along with the plot for the Response Function of some high- speed algorithms.
 Additive increase/multiplicative decrease AIMD(32, 0.125) corresponds to an AIMD algorithm (introduced in  of ) with an additive increase of 32 packets and a multiplicative decrease factor of 0.125.
Hence AIMD(32, 0.125) has the same slope as Reno on the log-log plot but is able to achieve a higher window size for both high- and lower link speeds.
Even though  plots the response function versus the packet drop rate, it also implies a relationship between the response function and the link capacity as we show next.
We will consider the case when all packet drops are due to buffer overflows, so from  in , it fol- lows that the number of packets N transmitted during a single cycle for TCP Reno is given by N53W2 m 8 where W mis the maximum window size.
But Wm/C25CTif we ignore the contribution caused by the buffer occupancy.
Hence, it follows that the packet drop probability p is given by p51 N/C258 3W2 m58 3ðCTÞ2 which implies that the packet drop probability is inversely proportional to the square of the link speed.
This is also illustrated in  , which shows the frequency of packet drops decreasing as the window size increases for two connections that pass through links with different capacities but with the same round trip latency.
Hence, it follows that small values of p correspond to a large link capacity and vice versa.
Note the following features for the graph in  :
Note that this implies that if Reno and the high-speed protocol compete for bandwidth in a high-speed link, the latter will end up with a much larger portion of the bandwidth (i.e., the system will not be fair).
However, this is deemed to be tolerable because the high-speed protocol is taking up link bandwidth that cannot be used by TCP Reno anyway.
2.At larger values of p (or equivalently lower speed links), it is desirable for TCP friendliness that the point at which the two curves cross over lie as much to the left as possible.
The reason for this is that for lower speed links, the congestion control algorithm should be able to sustain smaller window sizes, which is better suited to coexisting with TCP Reno without taking away bandwidth from it.
To do this, the Response Function for the high-speed algorithm should be equal to or lower than that for TCP Reno after the two curves cross over.
3.To investigate the third requirement regarding intraprotocol RTT fairness, we can make use of the following the result that was derived in : Assume that the throughput of a loss based congestion control protocol with constants A, e, d is given by R5A Tepdpackets =s Then the throughput ratio between two flows with round trip latencies T1 and T2, whose packet losses are synchronized, is given by R1 R25T2 T1/C18/C19 e 12d (4)
Note that as d increases, the slope of the response function and RTT unfairness both increase (i.e., the slope of the response function on the log-log plot determines its RTT unfairness).
Thus, by examining the Response Function graph, one can verify whether the high-speed proto- col satisfies all the three requirements mentioned.
To satisfy the first requirement, many high-speed algorithms use a more aggressive version of the Reno additive increase rule.
Hence, instead of increasing the window size by one packet per round trip time, they increase it by multiple packets, and the increment size may also be a func- tion of other factors such as the current window size or network congestion.
However, to satisfy the second requirement of TCP friendliness, they modify the packet increase rule in one of sev- eral ways.
For example:  HSTCP
[3]reverts to the TCP Reno packet increment rule when the window size is less than some threshold.
 Compound TCP [5]and TCP Africa [9]keep track of the connections’ queue backlog in the network and switch to a less aggressive packet increment rule when the backlog exceeds a threshold.
There is also a tradeoff between requirements 2 and 3 as follows: Even though it is possible to design a more TCP-friendly protocol by moving the point of intersection with the TCP curve to a lower packet drop rate (i.e., to the left in  ), this leads to an increase in the slope of the Response Function line, thus hurting RTT fairness.139 5.2DESIGN ISSUES FOR HIGH-SPEED PROTOCOLS 5.3HIGH SPEED TCP (HSTCP) PROTOCOL HSTCP by Floyd et al.
[3,10] was one of the first TCP variants designed specifically for high- capacity links with significant propagation delays.
The design for HSTCP was based on the plot of the Response Function versus Packet Drop Rate introduced in the previous section and proceeds as follows: The ideal response function for a high-speed congestion control algorithm should have the following properties: 
The HSTCP Response Function should coincide with that of TCP Reno when the packet drop rate exceeds some threshold P (or equivalently when the congestion window is equal to or less than W packets).
Comparison of response functions for TCP Reno versus High-Speed TCP (HSTCP).140  CONGESTION CONTROL IN HIGH-SPEED NETWORKS  can be rewritten as w5W0p p0/C18/C19S packets (7)
To obtain the corresponding a(W), we use the following formula for the response function of AIMD congestion control (this was derived in  in ).
We choose a point WA½W0;W1/C138and compute the corresponding probability P(W) by using  .
Even though HSTCP is not widely used today, it was one of the first high-speed algorithms and served as a benchmark for follow-on designs that were done with the objective of addressing some of its shortcomings.
A significant issue with HSTCP is the fact that the window increment function a(w) increases as w increases and exceeds more 70 for large values of w, as shown in  .141 5.3HIGH SPEED TCP (HSTCP) PROTOCOL
This implies that at the time when the link buffer approaches capacity, it is subject to a burst of more than 70 packets, all of which can then be dropped.
Almost all follow-on high-speed designs reduce the window increase size as the link approaches saturation.
HSTCP is more stable than TCP Reno at high speeds because it is less aggressive in reducing its window after a packet loss which reduces the queue size oscillations at the bottleneck node.
Hence, because of the aggressive window increment policy of HSTCP, combined with small values of the decrement multiplier, connections with a larger round trip latency are at a severe throughput dis- advantage.
There is a positive feedback loop which causes the connection with the large latency to loose throughput, which operates as follows: Even if the two connections start with the same window size, the connection with smaller latency will gain a small advantage after a few round trip delays because it is able to increase its window faster.
However this causes its window size to increase, which further accelerates this trend.
Conversely the window size of the large latency connection goes into a downward spiral as reductions in its throughput causes a further reduction in its window size.
5.4TCP BIC AND CUBIC Both TCP BIC and CUBIC came from the High-Speed Networks Research Group at North Carolina State University [1,11] .
TCP BIC was invented first and introduced the idea of binary search in congestion control algorithms, and TCP CUBIC was later designed to simplify the design of BIC and improve on some of its weaknesses.
Both these protocols have been very successful, and currently TCP CUBIC serves as the default congestion control choice on the Linux operating system.
It has been estimated [8]that TCP BIC is used by about 15% of the servers in the Internet, and CUBIC is used by about 30% of the servers.
Window Size.142  CONGESTION CONTROL IN HIGH-SPEED NETWORKS
The main idea behind TCP BIC and CUBIC, is to grow the TCP window quickly when the cur- rent window size is far from the link saturation point (at which the previous loss happened), and if it is close to the saturation point then the rate of increase is slowed down (see  for BIC and for CUBIC).
This results in a concave increase function, which decrements the rate of increase as the window size increases.
The small increments result in a smaller number of packet losses if the window size exceeds the capacity.
This makes BIC and CUBIC very stable and also very scalable.
If the available link capacity has increased since the last loss, then both BIC and CUBIC increase the window size exponentially using a convex function.
Since this function shows sub-linear increase initially, this also adds to the stability of the algorithm for cases in which the new link capacity did not increase significantly.
5.4.1 TCP BIC The objective of BIC’s design is to satisfy the three criteria of RTT fairness, TCP friendliness, and scalability.
The algorithm results in a concave response time function for BIC as shown in  , from which we can observe the following: 
Because the BIC response time graph intersects with that of Reno around the same point as that of HSTCP, it follows BIC is also TCP friendly at lower link speeds.
The slope of BIC’s response time at first rises steeply and then flattens out to that of TCP Reno for larger link speeds.
This implies that for high-speed links, BIC exhibits the RTT fairness property.
Because of the fast initial rise in its response time, BIC scales well with link speeds.
5.4.1.1 Binary Search Increase The Binary Search window increase rule in BIC was inspired by the binary search algorithm and operates as follows: I f W maxis the maximum window size that was reached just before the last packet loss, and Wminis the window size just after the packet loss, then after 1 RTT, the algorithm computes Wmid, the midpoint between W maxand W min, and sets the current window size to W mid. 
After the resulting packet transmissions: If there are no packet losses, then W minis set to W mid,o r If there are packet losses then W maxis set to W mid.
For the case of no packet losses the process repeats for every RTT until the difference between Wmaxand W minfalls below a preset threshold S min. Note that as a result of this algorithm, the window size increases logarithmically.
Just as for binary search, this process allows the bandwidth probing to be more aggressive when the difference between the current window and the target window is large and gradually becomes less aggressive as the current window gets closer to the target.
This results in a reduction in the number of lost packets as the saturation point is reached.
This behavior contrasts with HSTCP, which increases its rate near the link saturation point, resulting in excessive packet loss.
5.4.1.2 Additive Increase When the distance to W midfrom the current W minis too large, then increasing the window to that midpoint leads to a large burst of packets transmitted into the network, which can result in losses.
In this situation, the window size is increased by a configured maximum step value S max.
This con- tinues until the distance between W minand W midfalls below S max, at which the W minis set directlyAdditive Increase Binary Search Max Probing  Window growth function of TCP BIC.144  CONGESTION CONTROL IN HIGH-SPEED NETWORKS to W mid.
After a large window size reduction, the additive increase rule leads to an initial linear increase in window size followed by a logarithmic increase for the last few RTTs.
For large window sizes or equivalently for higher speed links, the BIC algorithm performs more as a pure additive increase algorithm because it spends most of its time in this mode.
This implies that for high-speed links, its performance should be close to that of the AIMD algorithm, which is indeed the case from  .
5.4.1.3 Max Probing When the current window size grows past W max, the BIC algorithm switches to probing for the new maximum window, which is not known.
It does so in a slow-start fashion by increasing its window size in the following sequence for each RTT: W max1Smin,W max12Smin, Wmax14Smin,...,Wmax1Smax.
The reasoning behind this policy is that it is likely that the new sat- uration point is close to the old point; hence, it makes sense to initially gently probe for available bandwidth before going at full blast.
After the max probing phase, BIC switches to additive increase using the parameter S max. BIC also has a feature called Fast Convergence that is designed to facilitate faster convergence between a flow holding a large amount of bandwidth and a second flow that is starting from scratch.
It operates as follows: If the new W maxfor a flow is smaller than its previous value, then this is a sign of a downward trend.
To facilitate the reduction of the flow’s bandwidth, the new Wmaxis set to (W max1Wmin)/2, which has the effect of reducing the increase rate of the larger window and thus allows the smaller window to catch up.
We now provide an approximate anal ysis of BIC using the sample path /C0based fluid approxi- mation technique introduced in  of Ch apter 2.
This analysis explains the concave behavior of the BIC response time function as illustrated in  .
We consider a typical cycle of window increase (see  ), that is terminated when a packet is dropped at the end of the cycle.
Define the following: Wmax: Maximum window size Wmin: Minimum window size N1: Number of RTT rounds in the additive increase phase N2: Number of RTT rounds in the logarithmic increase phase Y1:
Number of packets transmitted during the additive increase phase Y2: Number of packets transmitted during the logarithmic increase phase As per the deterministic approximation technique, we make the assumption that the number of packets sent in each cycle of the congestion window is fixed (and equal to 1/p, where p is the packet drop rate).
We now proceed to compute the quantities
Note that βis multiplicative decrease factor after a packet loss,
(16)145 5.4TCP BIC AND CUBIC BIC switches to the logarithmic increase phase when the distance from the current window size to W maxis less than 2S max; hence, if this distance less than this, there is no additive increase.
Similarly, to compute Y 2, we need to find the area A 2under the W(t) curve for the logarithmic increase phase.
Unfortunately, a closed-form expression for W maxdoes not exist in general, but it can computed for the following special cases: 1.βWmaxc2Smax This condition implies that the window functi on is dominated by the lin ear increase part, so
that N 1..N2and from equations 17, 21, and 23 , it can be shown that the average throughput is given by Ravg/C251 Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Smax 222β β1 ps (24) which is same as the throughput for a general AIMD congestion control algorithm that was derived in . 2.βWmax.2SmaxandβWmaxis divisible by S max.
5.4TCP BIC AND CUBIC where a5βð22βÞ 2Smaxb5logSmax Smin122β βc5Smax2Smin and the throughput is given by Ravg51 T22β p1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ b214ac11 p/C16/C17 1ð12βÞb1βð22βÞ 2r (26) 3.βWmax#2Smax This condition implies that N 150, and assuming1 pcSmin,W maxis approximately given by Wmax/C251 p1 logβWmax Smin/C18/C19 12ð12βÞ(27)
Because large values of W maxalso correspond to large link capacities (because W max5CT), it follows by a comparison of equations 24 and  in  that for high-capacity links, BIC operates similar to a n AIMD protocol with increase parameter a5Smax, decrease parameter of b5β, and the exponent d 50.5.
This follows from the fact that for high-capacity links, the BIC window spends most of its time in the linear increase portion.
For moderate values of C, we can get some insight into BIC’s behavior by computing the constants in part 2.
As recommended by Xu et al.
Because d 50.5 for large link capacities and d 51 for smaller link capacities, it follows that for intermediate value of capacity 0.5 ,d,1, which explains the concave shape of the BIC response function  .
from Xu et al.
[1]shows the variation of the BIC response function as a function of Smaxand S min.A shows that for a fixed value of S min, increasing S maxleads to an increase in throughput for large link capacities.
B shows that for a fixed value of S max,a s Sminincreases the throughput decreases for lower link capacities.
5.4.2 TCP CUBIC TCP CUBIC is a follow-on design from the same research group that had earlier come up with TCP BIC [11].
Their main motivation in coming up with CUBIC was to improve on BIC in the following areas: (1) BIC’s window increase function is too aggressive for Reno, especially under short RTT or low-speed networks, and (2) reduce the complexity of BIC’s window increment /C0 decrement rules to make the algorithm more analytically tractable and easier to implement.
Recall that BIC implemented the concave and convex functions that govern the window increase by using a binary search technique.
To reduce the resulting complexity of the algorithm, CUBIC replaced the binary search by a cubic function, which contains both concave and convex portions.
Another significant innovation in CUBIC is that the window growth depends only on the real time between consecutive congestion events, unlike TCP BIC or Reno in which the growth depends at the rate at which ACKs are returning back to the source.
This makes the window growth independent of the round trip latency, so that if multiple TCP CUBIC flows are competing for bandwidth, then their windows are approximately equal.
This results in a big improvement in the RTT fairness for CUBIC as shown later in this section.
Define the following: W(t): Window size at time t, given that the last packet loss occurred at t 50
Wm: Window size at which the last packet loss occurred τ: Average length of a cycle α;K: Parameters of the window increase function β: Multiplicative factor by which the window size is decreased after a packet loss, so that the new window is given by ð12βÞWm
The window size increase function for TCP CUBIC is given by WðtÞ5αðt2KÞ31Wm (30)
In short RTT networks, TCP Reno can grow faster than CUBIC because its window increases by one every RTT, but CUBIC’s window increase rate is independent of the RTT.
To keep CUBIC’s growth rate the same as that of Reno in the situation, CUBIC emulates Reno’s window adjustment algorithm after a packet loss event using an equivalent AIMD ðα;βÞ algorithm as follows: Recall that the throughput for TCP Reno is given by R51 Tﬃﬃﬃﬃﬃ 3 2ps and that for an AIMD ðα;βÞalgorithm is given by R51 Tﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ αð22βÞ 2β1 ps Hence, given β, a choice of α53β 22β will ensure that the AIMD algorithm has the same average throughput as TCP Reno.
Because the window size increases by αin every round trip and there are t/T round trips in time t, it fol- lows that the emulated CUBIC window size after t seconds is given by WAIMD ðtÞ5ð12βÞWm13β ð22βÞt T If W AIMD is larger than W CUBIC from  , then W CUBIC is set equal to W AIMD .
Otherwise, W CUBIC is used as the current congestion window size.
2.If W(t) is greater than the corresponding TCP Reno window but less than W m, then CUBIC is in the concave region.
3.If W(t) is larger than W m, then CUBIC is in the convex region.
Steady state behavior Wmax Max Probing  Window growth function for TCP CUBIC.151 5.4TCP BIC AND CUBIC We now proceed to do an analysis of the throughput for TCP CUBIC.
Again using the deter- ministic approximation technique from  , we consider a typical cycle of window increase ( ) that is terminated when a packet is dropped at the end of the cycle.
During the course of this cycle, the TCP window increases from ð12βÞWmtoWm, and we will assume that this takes on the average τseconds.
Following the usual recipe, we equate N to 1/p, where p is the packet drop rate.
The average throughput is then given by Ravg5N τ51=p τ5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ αð42βÞ 4p3Tβ4s (41)
5.4TCP BIC AND CUBIC  clearly shows this piecewise linearity of the CUBIC response function for T510 ms and T 5100 ms, respectively.
If a high-speed algorithm is used over these networks, we would like it to coexist with Reno without overwhelming it, and CUBIC satisfies this requirement very well.
Over high-speed long-distance networks with a large latency, on the other hand, TCP Reno is not able to make full use of the available bandwidth, but CUBIC naturally scales up its per- formance to do so.
shows the ratio between the throughputs of two flows of CUBIC, BIC, HSTCP, and TCP SACK, for a bottleneck link capacity of 400 mbps, and with one of the flows with a fixed RTT of 162 ms; the RTT of the other flow is varied between 16 ms and 162 ms.
This clearly shows that CUBIC has better inter-RTT fairness than the other protocols.
This is because it does not use ACK clocking to time its packet transmissions.
5.5THE COMPOUND TCP (CTCP) PROTOCOL CTCP was invented by Tan et al.
[5,12] from Microsoft Research and serves as the default conges- tion control algorithm in all operating systems from that company, from Windows Vista onward.
In00 2 04 06 08 0 RTT (ms)100 120 140 160 1800.20.40.60.81Throughput RatioTCP-SACK HSTCP BIC-TCP CUBIC  Intraprotocol fairness as a function of the round trip latency.155 5.5THE COMPOUND TCP (CTCP) PROTOCOL addition to making efficient use of high-speed links, the main design objective of CTCP was to be more TCP friendly compared with the existing alternatives such as HSTCP and CUBIC.
To do so, they came up with an innovative way to combine the fairness of delay-based conges- tion control approach (as in TCP Vegas) and the aggressiveness of loss-based approaches.
[5,12] made the observation that the reason protocols such HSTCP and BIC are not very TCP friendly is that they continue to use the loss-based approach to congestion detection and as a result overload the network.
To correct this problem, they suggested that the system should keep track of congestion using the TCP Vegas queue size estimator and use an aggressive window increase rule only when the estimated queue size is less than some threshold.
When the estimated queue size exceeds the threshold, then CTCP switches to a less aggressive window increment rule, which approaches the one used by TCP Reno.
To implement their approach, they used the following decomposition for the window size into two components: WðtÞ5WcðtÞ1WdðtÞ (42)
In ,W c(t) is the part of the window that reacts to loss-based congestion signals and changes its size according to the rules used by TCP Reno.
The component W d(t), on the other hand, reacts to a delay-based congestion signal and a uses a new set of rules for changing its win- dow size.
Specifically, W d(t) has a rapid window increase rule when the network is sensed to be underutilized and gracefully reduces its size when the bottleneck queue builds up.
Note that the expression for θcan also be written as θðtÞ5WðtÞ TsðtÞðTsðtÞ2TÞ (44) which by Little’s law equals the number of queued packets in the network.156  CONGESTION CONTROL IN HIGH-SPEED NETWORKS CTCP specifies that the congestion window should evolve according to the following equations on a per RTT basis:
W’W1αWk;when there are no packet losses or queuing delays and (45) W’ð12βÞWon one or more packet losses during a RTT (46)
Given that the standard TCP congestion window evolves according to (per RTT): Wc’Wc11 with no loss ;and (47a)
The shape of the window increase function in equations 49a to 49c (Figures 5.12 and 5.13 )i s explained as follows: When the netw ork queuing backlog is smaller than γ, the delay-based congestion window grows rapidly as per a .
When the congestion level reaches γ, the congestion window continues to increase at the rate of one packet per r ound trip time, and the delay window starts to ramp down as per b .
If the rate of increase of the congestion window W cequals the rate of decrease of the delay window W d, then the resulting CTCP window W stays constant at W 0until the size of the congestion window W cexceeds W 0.
At that point, the CTCP window W resumes its increase at the same linear rate as the congestion window, and the delay window dwindles to zero (see c ).
Referring to  , we now derive an expression for the throughput of CTCP using the deterministic sample path /C0based technique from  .
The evolution of the window increase is divided into three periods, as explained.
Let the duration of these periods be T DE,TEF, and T FG and let N DE,NEFand N FGbe the number of packets transmitted over these intervals.
Wc Wd W+ =  Evolution of the individual window sizes in CTCP.158  CONGESTION CONTROL IN HIGH-SPEED NETWORKS
Assuming that the CTCP window size at the end of the cycle is given by W m, so that the win- dow size at the start of the next cycle is given by ð12βÞWm, we now proceed to compute the quan- tities in  .
Noting that the queue backlog during the interval T EFis given by γ, it follows from Little’s law that γ5ðTs2TÞW0 Ts where W 0is the CTCP window size during this interval.
During the interval T DE, the CTCP window increases according to dW dt5αWk Ts;so that (52) W12k 12k5αt Ts(53) For the case k 50.75, which is the recommended choice for k as we shall see later, it follows from  that WðtÞ5αt 4Ts/C18/C194 ;tD#t#tE which illustrates the rapid increase in the window size in this region.
Because W increases as a power of 4, we refer to this as quartic increase.
Because the window size increases from ð12βÞWmto W 0during the interval T DE, from equa- tion 53 , it follows that TDE5Ts αð12kÞW12k 02ð12βÞ12kW12k m/C2/C3 (54) and the number of packets transmitted during this interval is given by (with T DE5tD2tE)
can be numerically solved to obtain the value of W m, which can then be substituted back into equations 54, 57, and 59 to obtain the length of the interval T DG.
From  ,i t follows that Ravg51=p TDG(62) If we make the assumption that the packet loss rate is high enough to cause the window size to decrease during the interval T DE, it is possible to obtain a closed-form expression for R.
To make the variation of the throughput with p the same as that for HSTCP, Tan et al.
To make the implementation easier, they used k 50.75 instead.
We can develop some intuition into CTCP’s performance with the following considerations: With reference to  , note that the demarcation between the quartic increase portion of the CTCP window and the linear increase portion is governed by the window size parameter W 0given by W05γTs Ts2T:160  CONGESTION CONTROL IN HIGH-SPEED NETWORKS From this equation, it follows that: 
For networks with large latencies, the end-to-end delay is dominated by the propagation delays; hence, Ts/C25T.
This leads to a large value of W 0.
For networks with small latencies, the queuing delays may dominate the propagation delays, so thatTscT. This leads to small values of W 0.
The interesting fact to note is that W 0is independent of the link capacity, which we now use to obtain some insights into CTCP behavior.
Also note that in the absence of random link errors, the maximum window size W mis approximately equal to the delay bandwidth product CT. Based on the values of the link speed C and the threshold W 0, we can have four different sce- narios (  ): 
As link speed C increases, for a fixed value of end-to-end latency, then W mincreases, and as a result, the number of packets transmitted in the linear portion of window increases (see, e.g., the change from equations 14a to 14b or 14c to 14d).
This can also be seen by the formula for N FG in .
As a result, at higher link speeds, the CTCP throughput converges toward that for TCP Reno. 
This can also be seen by the formula for N DE in .
As a result, an increase in latency causes the CTCP throughput to increase faster compared with that of Reno.
This behavior is reminiscent with that of CUBIC because CUBIC’s throughput also increased with respect to Reno as the end-to-end latency increases.
However, the CTCP throughput will converge back to that for Reno as the link speed becomes sufficiently large, while the CUBIC throughput keeps diverging.
These two effects are clearly evident in the response function plot in  .
plots the response function for CTCP in which the following features are evident:
The response functions for Reno and CTCP are close to each other at lower link speeds.
Then as the link speed increases, CTCP increases at a faster rate and diverges away from Reno.
This can be explained by the fact that the quartic increase portion dominates the linear increase in this region.
Finally, when the link speed assumes very large values, to the left part of the graph, CTCP and Reno again converge together.
This is because the linear portion of the window increase in CTCP is dominant at high link speeds, as explained earlier.
As the lend-to-end latency increases, we can expect this pattern to persist, with the difference that the portion of the curve where the CTCP response time is bigger will move to the left (because of larger values of W 0).
also plots the response function for CUBI C for comparison purposes.
Observe that response functions for CUBIC starts diverg i n ga w a yf r o mt h a to fR e n oa st h el
i n ks p e e d increases, and unlike CTCP, the divergence keeps i ncreasing.
This implie s that at very high link1E-7 1E-6 1E-5 1E-4 1E-3 1E-2 pw 1E+21E+31E+41E+51E+6 TCP RenoCTCPCUBIC Response Function Packet Drop Rate  Response functions for CTCP, CUBIC, and Reno.162  CONGESTION CONTROL IN HIGH-SPEED NETWORKS speeds CTCP will be at a disadvantage with respect to CUBIC or Reno, as has been noted by Munir et al.
From the discussion in the previous few sections, we observe that there is a fundamental differ- ence between the response function for HSTCP on one hand and that for CUBIC or CTCP on the other.
In the latter case, the response function changes as the round trip latency changes and tends to be more TCP Reno friendly at lower latencies.
This is an attractive property to have because lower latencies correspond to LAN environments, in which most transfers still use TCP Reno.
On the other hand, at higher latencies, both CUBIC and CTCP response functions diverge from that of TCP Reno (CUBIC more so than CTCP).
This is tolerable because Reno is not able to make full use of the avail- able link capacity in high-latency and high-capacity networks and may even become unstable.
5.6THE FAST TCP PROTOCOL The design of FAST TCP was inspired by that of TCP Vegas, and in contrast to the other high- speed protocols, it uses end-to-end delay as a measure of congestion rather than dropped packets.
Given the description of TCP Vegas in , the following equation describes the evolution of TCP Vegas’ window size (assuming that the parameters α5β): Wiðt11Þ5WiðtÞ11 TiðtÞsgnðαi2RiðtÞθiðtÞÞ (66) where θiðtÞis the queuing delay that connection i experiences, such that the round trip latency is given by T iðtÞ5Ti1θiðtÞ, where T iis the fixed part.
Hence, RiðtÞθiðtÞis a measure of the number of queued packets from the connection in the network.
Hence, TCP Vegas adjusts its window up or down by one packet per RTT depending on whether the number of queued packets is smaller or greater than its target α.
To create a high-speed version of this algorithm, Wei et al.
changed the window size evolu- tion equation to Wiðt11Þ5WiðtÞ1γiðαi2RiðtÞθiðtÞÞγiAð0;1/C138 (67)
As a result, the window adjustments in FAST TCP depend on the magnitude as well as the sign of the difference
αi2RiðtÞθiðtÞand are no longer restricted to a change of at most 1 every round trip time.
Hence, FAST can adjust its window by a large amount, either up or down, when the num- ber of buffered packets is far away from its target and by a smaller amount when it is closer.
It can be shown that FAST has the following property: The equilibrium throughputs of FAST are the unique optimal vector that maximize the sumP iαilogRisubject to the link constraint that the aggregate flow rate at any link does not exceed the link capacity.
This result was obtained using the Primal-Dual solution to a Lagrangian optimization problem using the methodology described in .
Furthermore, the equilibrium throughputs are given by Ravg i5αi θi(68) Just as for TCP Vegas,  shows that FAST does not penalize flows with large round trip latencies, although in practice, it has been observed that flows with longer RTTs obtain smaller163 5.6THE FAST TCP PROTOCOL throughput than those with shorter RTTs.
also implies that in equilibrium, source i maintains αipackets in the buffers along its path.
Hence, the total amount of buffering in the net- work is at leastP iαito reach equilibrium.
[14] have also proven a stability result for FAST, which states that it is locally asymptotically stable in general networks when all the sources have a common round trip delay, no matter how large the delay is.
5.7THE EXPRESS CONTROL PROTOCOL (XCP) The XCP congestion control algorithm
[15]is an example of a “clean slate” design, that is, it is a result of a fundamental rethink of the congestion control problem without worrying about the issue of back- ward compatibility.
As a result, it cannot deployed in a regular TCP/IP network because it requires multi-bit feedback, but it can be used in a self-contained network that is separated from the rest of the Internet (which can done by using the split TCP architecture from , for example).
XCP fundamentally changes the nature of the feedback from the network nodes by having them provide explicit window increase or decrease numbers back to the sources.
It reverses TCP’s design philosophy in the sense that all congestion control intelligence is now in the network nodes.
As a result, the connection windows can be adjusted in a precise manner so that the total throughput at a node matches its available capacity, thus eliminating rate oscillations.
This allows the senders to decrease their windows rapidly when the bottleneck is highly congested while performing smaller adjustments when the sending rate is close to the capacity (this is similar to the BIC and CUBIC designs and conforms to the averaging principle).
To improve system stability, XCP reduces the rate at which it makes window adjustments as the round trip latency increases.
Another innovation in XCP is the decoupling of efficiency control (the ability to get to high link utilization) from fairness control (the problem of how to allocate bandwidth fairly among com- peting flows).
This is because efficiency control should depend only on the aggregate traffic behav- ior, but any fair allocation depends on the number of connections passing through the node.
Hence, an XCP-based AQM controller has both an efficiency controller (EC) and a fairness controller (FC), which can be modified independently of the other.
In regular TCP, the two problems coupled together because the AIMD window increase /C0decrease mechanism is used to accomplish both objectives, as was shown in the description of the Chiu-Jain theory in . 5.7.1 XCP PROTOCOL DESCRIPTION The XCP protocol works as follows: 
Traffic source k maintains a congestion window W k, and keeps track of the round trip latency Tk.
It communicates these numbers to the network nodes via a congestion header in every packet.
Note that the rate R kat source k is given by Rk5Wk Tk Whenever a new ACK arrives, the following equation is used to update the window: Wk’Wk1Sk (69)164  CONGESTION CONTROL IN HIGH-SPEED NETWORKS where S kis explicit window size adjustment that is computed by the bottleneck node and is con- veyed back to the source in the ACK packet.
Note that S kcan be either positive or negative, depending on the congestion conditions at the bottleneck.
Network nodes monitor their input traffic rate.
Based on the difference between the link capacity C and the aggregate rate Y given by Y5PK k51Rk, the node tells the connections sharing the link to increase or decrease their congestion windows, which is done once every average round trip latency, for all the connections passing through the node (this automatically reduces the frequency of updates as the latencies increase).
This information is conveyed back to the source by a field in the header.
Downstream nodes can reduce this number if they are experiencing greater congestion, so that in the end, the feedback ACK contains information from the bottleneck node along the path.
Operation of the Efficiency Controller (EC): The EC’s objective is to maximize link utilization while minimizing packet drops and persistent queues.
The aggregate feedback φ(in bytes) is computed as follows: φ5αTavC2XK k51Rk !
2βb (70) where α;βare constants whose values are set based on the stability analysis, T avis the average round trip latency for all connections passing through the node, and b is the minimum queue seen by an arriving packet during the last round trip interval (i.e., it’s the queue that does not drain at the end of a round trip interval).
The first term on the RHS is proportional to the mismatch between the aggregate rate and the link capacity (multiplied by the round trip latency to convert it into bytes).
The second term is required for the following reason: When there is a persistent queue even if the rates in the first term are matching, then the second term helps to reduce this queue size.
From the discussion in , the first term on the RHS in  is proportional to the derivative of the queue length, and the second term is proportional to the difference between the current queue size and the target size of zero.
Hence, the XCP controller is equivalent to the proportional-integral (PI) controller discussed in .
Note that the increase in transmission rates in XCP can be very fast because the increase is directly proportional to the spare link bandwidth as per  , which makes XCP suitable for use in high-speed links.
This is in contrast to TCP, in which the increase is by at most one packet per window.
Operation of the Fairness Controller (FC): The job of the FC is to divide up the aggregate feedback among the K connections in a fair manner.
It achieves fairness by making use of the AIMD principle, so that: /C0Ifφ.0, then the allocation is done so that the increase in throughput for all the connections is the same. /C0Ifφ,0, then the allocation is done so that the decrease in throughput of a connection proportional to its current throughput.
When φis approximately zero, then the convergence to fairness comes to a halt.
To prevent this, XCP does an artificial deallocation and allocation of bandwidth among the K connections by using the feedback h given by h5max 0 ;0:1XK k51Rk2jφj ! ; (71)165 5.7THE EXPRESS CONTROL PROTOCOL (XCP) so on every RTT, at least 10% of the traffic is redistributed according to AIMD.
This process is called “bandwidth shuffling” and is the principle fairness mechanism in XCP to redistribute band- width among connections when a new connection starts up.
The per packet feedback to source k can be written as φk5pk2nk (72) where p kis the positive feedback and n kis the negative feedback.
First consider the case when φ.0.
This quantity needs to be equally divided among the throughputs of the K connections.
The corresponding change in window size of the kthconnection is given by ΔWk5ΔRkTk~Tkbecause the throughput deltas are equal.
This change needs to be split equally among the packets from connection k that pass through the node during time T k, say mk.
Note that m kis proportional to Wk=MSS kand inversely proportional to T k.
It follows that the change in window size per packet for connection k is given by pk5KpT2 kMSS k Wk(73) where K pis a constant.
Because the total increase in aggregate traffic rate ish1max ðφ;0Þ Tav,i t follows that h1max ðφ;0Þ Tav5XLpk Tk(74) where the summation is over the average number of packets seen by the node during an average RTT.
Substituting  into , we obtain Kp5h1max ðφ;0Þ TavXLTkMSS k Wk(75)
When φ,0, then the decrease in throughput should be proportional to the current throughput.
It follows that ΔWk5ΔRkTk~RkTk5Wk.
0,α,π 4ﬃﬃﬃ 2p and β5α2ﬃﬃﬃ 2p (78) then the system is stable, independently of delay, capacity, and number of connections.
This analy- sis is done using the techniques introduced in , whereby the system dynamics with166  CONGESTION CONTROL IN HIGH-SPEED NETWORKS feedback are captured in the fluid limit using ordinary differential equations, to which the Nyquist stability criterion is applied.
In this section, we will assume that the round trip latencies for all the k connections are equal, so that T k5Tav5T, k51,...,K.
Using the notation used in the previous section, note that the total traffic input rate into the bottleneck queue is given by YðtÞ5XK k51RkðtÞ5XK k51WkðtÞ T(79) Because the aggregate feedback φis divided up among the K connections, every T seconds, it follows that the total rate of change in their window sizes is given by φ=Tbytes/sec.
Hence, the equations that govern the input traffic rate and buffer dynamics are the following dbðtÞ dt5YðtÞ2Ci f b ðtÞ.0 max ð0;YðtÞ2CÞif b ðtÞ50/C26 (82) dY dt52α TðYðt2TÞ2CÞ2β T2bðt2TÞ (83)
These form a feedback loop, as shown in  .
In this figure, the queue dynamics are governed by  , and the rate allocation is done via  .
These equations have the equilibrium point b eq50 and Y eq5C.
We now choose the critical frequency ωcat which jGðjωcÞj51, as a function of the parameters K1and
To prove stability of the closed-loop system, the Nyquist criterion requires that arg ðGðjωcÞÞ,π radians.
Substituting  into , it follows that argðGðjωcÞÞ5π2π 41β α,πi:e:α,π 4ﬃﬃﬃ 2p which completes the proof.
This analysis shows that unlike for traditional congestion control and AQM schemes, XCP’s sta- bility criterion is independent of the link capacity, round trip latency, or number of connections.
It was later shown by Balakrishnan et al.
[16] that the linearization assumption around the point beq50 may not be a good approximation because the system dynamics are clearly nonlinear at this point (see  ).
They did a more exact stability analysis using Lyapunov functionals and showed that the more exact stability region for α;βvaries as a function of T and is in fact much larger than the region given by  .
However, it is not an exact superset because there are points near the boundary of the curve β5α2ﬃﬃﬃ 2p that lie in the stable region as per the linear analy- sis but actually lie in the unstable region according to the nonlinear analysis (see Balakrishnan et al.
The RCP protocol [17] was inspired by XCP and builds on it to assign traffic rates to connections in a way such that they can quickly get to the ideal Processor Sharing (PS) rate.
RCP is an entirely rate-based protocol (i.e., no windows are used).
Network nodes compute the ideal PS-based rate and then pass this information back to the source, which immediately changes its rate to the mini- mum PS rate that was computed by the nodes that lie along its path.
RCP was designed to overcome the following issue with XCP: When a new connection is initi- ated in XCP and the nodes along its path are already at their maximum link utilizations, then XCP does a gradual redistribution of the link capacity using the bandwidth-shuffling process (see equa- tion 71 ).
Dukkipatti and McKeown [17] have shown that this process can be quite slow because new connections start with a small window and get to a fair allocation of bandwidth in a gradual manner by using the AIMD principle.
However, it takes several round trips before convergence happens, and for smaller file sizes, the connection may never get to a fair bandwidth distribution.
Hence, one of the main innovations in RCP, compared with XCP, is that the bandwidth reallocation happens in one round trip time, in which a connection gets its equilibrium rate.
The RCP algorithm operates as follows: 
Every network node periodically computes a fair-share rate R(t) that it communicates to all the connections that pass through it.
This computation is done approximately once per round trip delay.
 RCP mandates two extra fields in the packet header: Each network node fills field 1 with is fair share value R(t), and when the packet gets to the destination, it copies this field into the ACK packet sends it back to the source.
Field 2, as in XCP, is used communicate the current average round trip latency T k, from a connection to all the network nodes on its path.
The nodes use this information to compute the average round trip latency T avof all the connections that pass through it.
Each source transmits at rate R k, which is the smallest offered rate along its path.
Each network node updates its local rate R(t) according to the equation below: RðtÞ5Rðt2TavÞ1αðC2YðtÞÞ2βbðtÞ Tavhi ^NðtÞ(91) where ^NðtÞis the node’s estimate of the number of connections that pass through it and the other variables are as defined for XCP.
If there is a queue build-up of b(t), then a rate decrease of b(t)/T avwill bring it down to zero within a round trip interval.
It can be easily shown that system dynamic equations for RCP are exactly the same as for XCP (see equations 82 and 83 ).
Hence, the stability conditions derived for XCP in the previous section hold in this case as well. 5.9STABILITY OF HIGH-SPEED TCP ALGORITHMS The algorithms described in this chapter all operate in the region where regular TCP has been shown to exhibit poor performance because of oscillatory behavior.
From the analysis that was described in , the following techniques were found to increase the stability of congestion control algorithms in networks with high link capacity or end-to-end latency:  Sophisticated AQM schemes that feed back the buffer occupancy value, as well as the first or higher derivatives of the buffer occupancy.
Note that the first derivative of the buffer occupancy is given by dbðtÞ dt5C2X iRi so that it provides the same information as the difference between the link capacity and the cur- rent traffic load (i.e., how close the link is to saturation).
 Window size increment rules that incorporate the Averaging Principle (AP) to reduce the rate of the window size increase as the link approaches saturation The natural question to ask is how do the algorithms described in this chapter fare in this regard?
We have not seen any work that applies control theory to models for HSTCP, BIC, CUBIC, or CTCP, but simulations show that they are much more stable than Reno in the high-link speed and large propagation delay environment.
A reason why this is the case can be gleaned from the principles enunciated earlier.
Applying the Averaging Principle, algorithms that reduc e the window increment size as the link reaches saturation, are more stable than those that do not do so.
BIC, CUBIC, CTCP, and FAST all fall into this category of algorithms.
BIC, CUBIC, and FAST reduce their window increment size in proportion to how far they are from the link sa turation point while CT CP changes to a linear rate of increase (from quartic) when the queu e backlog increases beyond a threshold.
XCP and RCP, on the other hand, use a sophisticat ed AQM scheme in which the network nodes feed back the value of the buffer occupancy as well as the difference between the link capacity and the total traffic load.
The Quantum Congest ion Notification (QCN) algorithm that we will meet in  uses a combination of the averag ing principle and first derivative feedback to achieve stability.170  CONGESTION CONTROL IN HIGH-SPEED NETWORKS The HSTCP algorithm, on the other hand, does not fall cleanly into one of these categories.
But note that HSTCP decreases its window size decrement value (on detecting packet loss) as the window size increases, and in general, a lgorithms that do a less drastic reduction of their window size compared with Reno have better stability properties (because this has the effect of reducing the rate oscillations).
This is also true for TCP Westwood (see ), whose window size reduction is proportional to the amount of queuing delay.
The Data Center Congestion Control Protocol (D CTCP) algorithm (see ) also falls in this class because it reduces its window size in proportion to t he amount of time that the queue size exceeds some threshold.
All of these are deployed on fewer than 2% of the servers on the Internet, according to the study done by Yang et al.
A model for steady state throughput of TCP CUBIC.
[3] Floyd S., Ratnasamy S., Shenker S. Modifying TCP’s congestion control for high speed networks.
http://www.icir.org/floyd/papers/hstcp.pdf [4] Liu S, Basar T, Srikant R. TCP-Illinois: a loss and delay based congestion control algorithm for high speed networks.
[5] Tan K, Song J, Zhang Q, Sridharan M. A compound TCP approach for high speed and long distance net- works.
[6] Baiocchi A, Castellani AP, Vacirca F. YeAH-TCP: yet another highspeed TCP.
[7] Wei DX, Jin C, Low SH, Hegde S. FAST TCP: motivation, architecture, algorithms, performance.
TCP congestion avoidance algorithm identification.
[9] King R, Baraniuk R, Riedi R. TCP Africa: an adaptive and fair rapid increase rule for scalable TCP.
[10] Floyd S. Highspeed TCP for large congestion windows.
[12] Tan K, Song J, Zhang Q, Sridharan M. Compound TCP: a scalable and TCP friendly congestion control for high speed networks.
[13] Munir K, Welzl M, Damjanovic D. Linux beats Windows!—or the worrying evolution of TCP in com- mon operating systems.
[15] Katabi D, Handley M, Rohrs C. Congestion control for high bandwidth-delay product networks.
[16] Balakrishnan H, Dukkipati N, McKeown N, Tomlin CJ.
Stability analysis of explicit congestion control protocols.
[17] Dukkipati N, McKeown N. Processor sharing flows in the Internet.
[18] Kelly T. Scalable TCP: Improving performance in highspeed wide area networks.
[19] Leith D, Shorten R. H-TCP: TCP for high speed and long distance networks.
PFLDNet 2004. SUGGESTED READING Blanc A, Collange C,
Avrachenkov K. Comparing some high speed TCP versions under Bernoulli losses.
INRIA Res Rep 2009:9. Blanc A, Avrachenkov K, Collange D, Neglia G. Compound TCP with random losses.
Carofiglio G, Muscariello L, Rossi D. Rethinking Low Extra Delay Background Transport (LEDBAT) Protocols.
Experimental evaluation of TCP protocols for high-speed networks.
IEEE/ACM ToN 2007;15(5):1109 /C022.172  CONGESTION CONTROL IN HIGH-SPEED NETWORKS CHAPTER 6FLOW CONTROL FOR VIDEO APPLICATIONS 6.1INTRODUCTION Video streaming has grown in popularity as the Internet matures and currently consumes more bandwidth than any other application.
Most of this traffic is driven by consumer consumption of video, with services such as Netflix and YouTube leading the pack.
Video traffic comes in two fla- vors, video on demand (VoD) and live-video streaming.
VoD traffic is from stored media and is streamed from servers, and it constitutes the majority of the video traffic.
Video has some fundamental differences in transmission requirements compared with traditional data traffic, such as the fact that there are real-time constraints in the delivery of video traffic to the client player.
As explained in  , this constraint arises because the client video device expects data to be constantly available so that it can keep updating the screen at a constant rate (which is usually 30 frames/s).
If there is a hiccup in this process and no data is available, then this results in a temporarily frozen screen while the network catches up.
Most of the early work on packet video transmission focused on providing real-time transmis- sion by means of new techniques that supported resource reservations and QoS (quality of service) provisioning in the network.
In the Internet Engineering Task Force (IETF), protocols such as RSVP and IntServ were designed during the 1990s with the intent of using them for provisioning network resources for streaming video delivery.
Even though these protocols are now widely used for supporting real-time video in the Internet, most operators balked at supporting these protocols for consumer video transmission because of the extra complexity and cost involved at both the servers and in the network infrastructure.
During the early years of the Web, the conventional wisdom was that video streaming would have to be done over the User Datagram Protocol (UDP) because video did not require the absolute reliability that TCP provided, and furthermore, TCP retransmissions are not compatible with real- time delivery that video requires.
In addition, it was thought that the wide rate variations that are a result of TCP’s congestion control algorithm would be too extreme to support video streams whose packet rate is steady and does not vary much.
Because of UDP does not come with any congestion control algorithm, there was considerable effort expended in adding this capability to UDP so it could use it for carrying video streams and at the same time coexist with regular TCP traffic.
The most significant result from this work was a congestion control algorithm called TCP Friendly Congestion 173Internet Congestion Control.
This was a rate-based algorithm whose design was based on the square-root for- mula for TCP throughput derived in .
The sender kept estimates of the round trip latency and Packet Drop Rate and then used the square root formula to estimate the throughput that a TCP connec- tion would experience under the same conditions.
This estimate was then used to control the rate at which the UDP packets were being transmitted into the network.
The video transmission landscape began to change in the early 2000s when researchers realized that perhaps TCP, rather than UDP, could also be used to transmit video.
TCP’s rate fluctuations, which were thought to be bad for video, could be overcome by using a large receive buffer to dampen them out.
Also because most video transmissions were happening over the Web, using the HyperText Transfer Protocol (HTTP) for video was also very convenient.
The combination of HTTP/TCP for video delivery had several benefits, including:  TCP and HTTP are ubiquitous, and most video is accessed over the Web. 
A video server built on top of TCP/HTTP uses commodity HTTP servers and requires no special (and expensive) hardware or software pieces.
 HTTP has built-in Network Address Translation (NAT) traversal capabilities, which provide more ubiquitous reach.
The use of HTTP means that caches can be used to improve performance.
A client can keep playback state and download video segments independently from multiple servers while the servers remain stateless.
The use of TCP congestion control guarantees that the network will remain stable in the presence of high /C0bit rate video streams.
The initial implementations of video over HTTP/TCP used a technique called Progressive Download (PD), which basically meant that the entire video file was downloaded as fast as the TCP would allow it into the receiver’s buffer.
Furthermore, the client video player would start to play the video before the download was complete.
This technique was used by YouTube, and even today YouTube uses an improved version of the PD algorithm called Progressive Download with Byte Ranges (PD-BR).
This algorithm allows the receiver to request specific byte ranges of the video file from the server, as opposed to the entire file.
A fundamental improvement in HTTP/TCP deliver yo fv i
sw i t ht h e invention of an algorithm called HTTP Adaptive St reaming (HAS), which is also sometimes known by the acronym DASH (Dynamic Adaptive Streaming ove r HTTP).
This algorithm is described in detail in  and was first deployed by Move Networks and then rapidly after that by the other major video providers.
Using HAS, the video receiver is able to adaptively change the video rate so that it matches the bandwidth that the network can currently suppo rt.
From this description, HAS can be considered to be a flow control rather than a congestion control al gorithm because its objective is to keep the video receive buffer from getting depleted rather than to k eep network queues from getting congested.
In this sense, HAS is similar to TCP receive flow control excep t for the fact that the objective of the latter algo- rithm is to keep the receive buffer from overflow ing.
HAS operates on top of TCP congestion control, albeit over longer time scales, and the interaction bet ween the two is rich source of research problems.
HAS remains an area of active research, and the important issues such as the best algorithm to control the video bit rate, the interaction of HAS with TCP, and the interaction of multiple HAS streams at a bottleneck node are still being investigated.
There have been several suggestions to improve the original HAS bit rate adaptation algorithm, some of which are covered in  .174  FLOW CONTROL FOR VIDEO APPLICATIONS Today the reliable delivery of even high-quality high-definition (HD) video has become commonplace over the Web, and some of the credit for this achievement can be attributed to the work described in this chapter.
The rest of this chapter is organized as follows:  introduces the fundamentals of video delivery over packet networks,  describes the HAS protocol,  introduces Adaptive Bit Rate (ABR) algorithms,  describes several ABR algorithms that have been proposed in the literature, and Sections 6.6 and 6.7 discuss the TCP throughput measure- ment problem and the interaction between TCP and ABR.
and 6.7 contain more advanced material and may be skipped during a first reading.
6.2VIDEO DELIVERY OVER PACKET NETWORKS Video pictures, or frames, have to be played at a rate of about 30 frames per second (fps) to create the illusion of motion.
Video compression is done by using the Discrete Cosine Transform (DCT) on the quantized grey scale and color components of a picture frame, and then transmitting the truncated DCT coefficients instead of the original picture.
In addition to the intraframe compression, all compression algorithms also carry out interframe compression, which takes advantage of temporal picture redun- dancy in coding a frame by taking its delta with respect to a previous frame.
A shows a sequence of video frames, encoded using the Moving Picture Experts Group (MPEG) standard.
There are three types of frames shown: I frames are largest because they only use intraframe compression; B and P frames are smaller because they use previous I frames to further reduce their size.
This results in a situation in which the encoded bits per frame is a variable quantity, thus leading to Variable Bit Rate (VBR) video.
B shows the bits per frame for a sample sequence of frames.
A number of video coding standards are in use today: The most widely used is an ITU Standard called H.264 or MPEG-4; others include proprietary algorithms such as Google’s VP-9.
Video cod- ing technology has rapidly advanced in recent years, and today it is possible to send an HD-TV 1080p video using a bit rate of just 2 mbps.
The encoded video results in a VBR stream that is then transmitted into the network (  ).
Because we are not depending on the network to provide a guaranteed bandwidth for the video stream, there arises the problem of matching the video bit rate with the bandwidth that the network can currently provide on a best-effort basis.
If the network bandwidth is not sufficient to support the video bit rate, then the decoder at the receiving end starts to consume the video data at rate that is greater than the rate at which new data is being received from the network.
As a result, the decoder ultimately runs out of video data to decode, which results in a screen freeze and the familiar “buffer loading” message that we often see.
This is clearly not a good outcome, and to avoid this without having to introduce costly and complex guaranteed bandwidth mechanisms into the network, the fol- lowing solutions try to match the video bit rate to the available network bandwidth: 
Use of a large receive buffer: As shown in  , the system can smooth out the variations in network throughput by keeping a large receive buffer.
As a result, temporary reductions in throughput can be overcome by used the video stored in the receive buffer.175 6.2VIDEO DELIVERY OVER PACKET NETWORKS  Transcoding-based solutions ( A ):
These algorithms change one or more parameters of the compression algorithm that operates on the raw video data to vary the resulting bit rate.
Examples include varying the video resolution, compression ratio, or frame rate.
Transcoding is very CPU intensive and requires hardware support to be done at scale, which makes them difficult to deploy in Content Delivery Networks (CDN).
(B)Bits generated per frame.
Encoder Decoder Send BufferReceiver BufferNetworkCongestion Control  End-to-end block diagram of a video transmission system.176  FLOW CONTROL FOR VIDEO APPLICATIONS 
Scalable encoding solutions ( B ):
These can be implemented by processing the encoded video data rather than the raw data.
Hence, the raw video can be encoded once and then adapted on the fly by using the scalability features of the encoder.
Examples of scalable encoding solutions include adapting the picture resolution or frame rate by exploiting the spatial or temporal scalability in the data.
However, even scalable encoding is difficult to implement in CDNs because specialized servers are needed for this.
Stream switching solutions ( C ):
This technique is the simplest to implement and can also be used by CDNs.
It consists of preprocessing the raw video data to produce multiple encoded streams, each at a different bit rate, resulting in N versions.
An algorithm is used at the time of transmission to choose the most appropriate rate given the network conditions.
Stream switching algorithms use the least processing power because after the video is encoded, no further operations are needed.
The disadvantages of this approach include the fact that more storage is needed and the coarser granularity of the encoded bit rates.
The industry has settled on using a large receive buffer and stream switching as the preferred solution for video transmission.
Before the coding rate at the source can be changed, the video server has to be informed about the appropriate rate to use.
Clearly, this is not a function that a congestion control protocol such as TCP provides; hence, all video transmissions systems define a protocol operating on top of TCP.
In some of the early work on video transport, protocols such as Rate Adaptation Protocol (RAP)
(C)  Adaptive streaming techniques.
For example, RAP used an additive increase/multiplicative decrease (AIMD) /C0type scheme that is reminiscent of TCP congestion control, and TFRC used an additive increase/additive decrease (AIAD) scheme that is based on the TCP square root formula.
The HAS protocol, which dominates video transport today, uses a scheme that differs from these early algorithms in the following ways:  HAS is built on top of TCP transport, unlike the earlier schemes, which were based on UDP.
Some of the reasons for using TCP were mentioned in the Introduction.
Instead of the transmitter, the receiver in HAS drives the algorithm.
It keeps track of the TCP rate of the video stream as well as the receive buffer occupancy level, and then using the HTTP protocol, it informs the transmitter about the appropriate video bit rate to use next.
 Instead of sending the video packets in a continuous stream, HAS breaks up the video into chunks of a few seconds each, each of which is requested by the receiver by means of an HTTP request.
HAS adapts the sending rate, and consequently the video quality, by taking longer term averages of the TCP transmit rate and variations in the receive buffer size.
This results in a slower variation in the sending rate, as opposed to TCP congestion control, which varies the sending rate rapidly in reaction to network congestion or packet drops.
6.2.1 CONSTANT BIT RATE (CBR) VIDEO TRANSMISSION OVER TCP To illustrate the transmission of video over packet networks, we will consider the simplest case of live CBR video transmission over a network with variable congestion (  ).
Let S(t) 5Kt be the number of bits the source encoder has transmitted into the network by time t.
If D(t) is the num- ber of bits the receiving decoder has pulled from the receive buffer by time t, then DðtÞ500 #t#τ Kðt2τÞt$τ/C26 (1) where τis the delay before the decoder starts pulling data from the receive buffer.
During this ini- tial start-up time, Kτbits are transmitted by the encoder, which are in transit in the network or waiting to be decoded in the receive buffer.
Note that A(t) is dependent on the congestion state of the network, hence 
If the network is congestion free, then A(t) is close to S(t), so that the amount of traffic in the network NðtÞ/C250b y  , and the amount of bits in the receiver is approximately given byBðtÞ/C25Kτby , so that the receive buffer is full.
In this situation, the decoder may run out of bits to decode, resulting in a frame freeze.
If TCP is used to transport the CBR video stream, then to reduce the frequency of buffer under- flows at the receiver, the designer can resort to policies such as: 1.Make sure that the network has sufficiently high capacity to support the video stream:
[3]have shown that to support a CBR video stream of rate K bps, the throughput of the TCP stream transporting the video should be at least 2K bps.
This ensures that the video does not get interrupted too frequently by frame freezes.
2.Use a sufficiently large receive buffer: Kim and Ammar
Receive Buffer UnderflowPacket Number  Time evolution of transmitted bits, received bits, and buffer size.179 6.2VIDEO DELIVERY OVER PACKET NETWORKS and retransmission timeout T 0, the receive buffer size B that results in a desired buffer underrun probability P u, is lower bounded by B$0:16 pPu119:4T0 T/C18/C192 min 1 ;3ﬃﬃﬃﬃﬃ 3p 8r !
pð1132p2Þ$% (5) This result assumes that the CBR video rate K coincides with the average data rate that the TCP connection can support, so that K51 Tﬃﬃﬃﬃﬃ 3 2ps This last assumption is difficult to satisfy in practice, which may be reason why  is not commonly used to dimension receive buffers.
6.3HTTP ADAPTIVE STREAMING (HAS) The discussion in  showed that when TCP is used to transport CBR video, one can reduce the incidence of receive buffer exhaustion, either by overprovisioning the network so that the TCP rate is double the video rate or by using a sufficiently large receive buffer, which averages out the rate fluctuations attributable to TCP congestion control, but this begs the question of whether there are better ways of solving this problem that use less resources.
Most of the video streaming industry has now s ettled on HAS as the extra ingredient needed to send video over the Internet.
HAS was able to improve the system by adding another layer of rate control on top of TCP.
However, unlike TCP, the HAS algorithm is video aware and is able to interact with the video application at the se nder to adaptively change its sending rate.
The result of this interaction is shown in  , where one can see the video rate decreasing if the network congestion increases, and converse ly increasing when the congestion reduces.
As a result of this, the system is able to stream vi deo without running into frame-freeze conditions and without having to overprovision the network or keep a very large receive buffer (compare  with  in which the receiving buffer underfl ows without bit rate adaptation).
Note that  assumes that the video is encoded at multiple rates, which can be adaptively changed depending on the network conditions.
As mentioned in the Introduction, HAS uses HTTP/TCP for transporting video rather than the traditional RTP/UDP stack.
With HAS, a video stream is divided into short segments of a few sec- onds each, referred to as chunks or fragments.
Each chunk is encoded and stored in the server at a number of versions, each with a different bit rate as shown in  .
At the start of the video session, a client downloads a manifest file that lists all the relevant information regarding the video streams.
The client then proceeds to download the chunks sequentially using HTTP GETs.
By observing the rate at which data is arriving to the client and the occupancy of the video decoding buffer, the client chooses the video bit rate of the next chunk.
The precise algorithm for doing so is known as the ABR algorithm and is discussed in the next section.180  FLOW CONTROL FOR VIDEO APPLICATIONS
Examples of commercial HAS systems that are in wide use today include the following:  Microsoft HTTP Smooth Streaming (HSS)
Apple HTTP Live Streaming (HLS) 
Adobe HTTP Dynamic Streaming (HDS)
[7]: Uses a chunk size of 10 sec and a 300-sec receive buffer These HAS algorithms are proprietary to each vendor, which typically do not reveal details about their schemes.
One can obtain some understanding of how they behave by carrying out mea- surements, and this program was carried out by Akhshabi et al.
[8], from whom the numbers quoted here have been taken.
6.4THE ADAPTIVE BIT RATE (ABR) ALGORITHM
The ABR is the algorithm that HAS uses to adapt the video bit rate.
With reference to  , while the TCP inner control loop reacts to network congestion and tries to match the TCP send rate with the rate that the network can support, the ABR outer loop reacts to the rates that TCP decides to use and tries to match the rate of the video stream to the average TCP rate.
Video SourceDecoder Send BufferReceiver BufferNetwork Congestion Control TCP Control Loop with AQM or Drop-Tail Feedback ABR Control Loop  Illustrating the two control loops: TCP and Adaptive Bit Rate (ABR).182  FLOW CONTROL FOR VIDEO APPLICATIONS several seconds.
Even then, it is a nontrivial pr oblem to estimate the “true” average TCP rate from measurements made at the receiver [9], and we shall see later that ABR’s transmission policy can lead to an overestimation of TCP thr oughput, which results in oscillations in the video rate.
The ABR algorithm controls the following quantities: 
The rate at which the receiver sends HTTP requests back to the video source 
The appropriate video stream to transmit, whose rate most closely matches the rate that can be supported by the network The objectives of an ideal ABR algorithm include the following: 1.Avoid interruptions of playback caused by buffer underruns.
2.Maximize the quality of the video being transmitted: Fulfilling this objective constitutes a trade-off with objective 1 because it is always possible to minimize the number of interruptions by always transmitting at the lowest rate.
3.Minimize the number of video quality shifts to improve user experience: This leads to a trade-off with objective 2 because the algorithm can maximize the video quality by reacting to the smallest changes in the network bandwidth, which, however, increases the number of quality shifts.
4.Minimize the time between the user making a request for a new video and the video actually starting to play: This objective can also be achieved at the cost of objective 2 by using the lowest bit rate at the start.
This is also an output of the ABR algorithm.
~Tn: Download duration for the nth chunk Tn: Actual inter-request time between HTTP requests from the receiver, between the nthand (n11)rstrequest, so that Tn5max ð^Tn;~TnÞ (6)
This is also referred to as the nthdownload period.
Bn: Duration of the video stored in the video buffer at the end of the n thperiod
Because the size of the receive buffer is in terms of the duration of video data stored in it, the download of a chunk causes the buffer size to increase by τseconds irrespective of the rates U nor Rn.
Similarly, it always takes τseconds to play the video data in a chunk, irrespective of the rate at which the video has been coded.183 6.4THE ADAPTIVE BIT RATE (ABR)
ALGORITHM Most ABR algorithms use the following three steps: 1.Estimation: The throughput during the nthvideo chunk is estimated using the following equation: Rn5Unτ ~Tn(7)
3.Scheduling: The ABR algorithm determines the time when the next chunk is scheduled to be downloaded at by computing an HTTP inter-request time of ^Tn, such that ^Tn5GðBn;fRm:m#n11gÞ (10)
Note that  assumes that the video rate is fixed at U nfor the duration of the nthchunk.
In reality, it may vary, and we can take this into account by replacing UnτbyUτ n, which is the number of bits received as part of the nthchunk.
A shows the downloading of multiple video chunks at the receiver.
As noted earlier, the time spent in playing the video contained in a chunk is always τseconds; however, the time required to transmit a chunk is variable and is given by  .
Because the two rates are perfectly matched, it takes exactly τseconds to transmit the chunk, and there is no change in the buffer size at end of the n thperiod (by  ).
This is ideal state for the ABR algorithm because it makes full use of the network capacity, and the buffer size is steady.
Note that in practice, this perfect match does not occur because of the quantization in the video bit rates.
In this case, τ. ~Tn, so the network fills up the buffer faster (two chunks every τseconds) than the constant rate at which the decoder is draining it (one chunk every τseconds).
This condition needs to be detected by the ABR algorithm so that it can take control action, which consists of increasing the video bit rate so that it can take advantage of the higher network capacity.
If the video bit rate is already at a maximum, the ABR algorithm avoids buffer overflow by spacing out the time interval between chunks (see  ).
In this case, τ, ~Tnso the decoder drains the buffer at a higher rate than the rate at which the network is filling it, which can result in an underflow.
This condition also needs to be detected promptly by the ABR algorithm so that it can take control action, which consists of reducing the video bit rate to a value that is lower than the network rate.184  FLOW CONTROL FOR VIDEO APPLICATIONS To understand the dynamics of ABR algorithms, consider the scenario pictured in  .
Assume that the TCP throughput is fixed and is given by R, and furthermore, it lies between the video rates V(m) and V(m 11). 
If the ABR algorithm chooses rates U nthat are V(m 11) or larger, then this will result in the scenario shown in B , in which the decoder drains the buffer faster than the rate at which the network is filling it, resulting in a decreasing buffer size.
If left unchecked, then this will eventually result in an underflow of the receive buffer.
If the ABR algorithm chooses rates U nthat are equal to V(m) or smaller, then this will result in the scenario shown in C , in which the network fills the buffer faster than the rate at which the decoder can drain it.
This will result in an increasing buffer size.
Because of the quantization in video rates, it is impossible to get to U n5R, and this can result in the video rate constantly fluctuating between V(m) and V(m 11) to keep the buffer from over- flowing or underflowing.
To avoid this, most ABR algorithms stop increasing the video rate U n after it gets to V(m) and compensate for the difference between V(m) and R by increasing the gap between successive downloads, as shown next.
The case when the TCP throughput R exceeds the maximum video rate V(L)
( A ) will result in the system dynamics shown in C in which the receive buffer size keeps increasing.
If the ABR algorithm observes that the buffer size is increasing even at the maximum video bit rate, then it stabilizes the buffer by increasing the gap between successive downloads, as shown in B .
There are two ways in which it can choose the gap size: 
The gap size is fixed at τ: In this case, during each period, the amount of video data consumed is equal to the data being added, resulting in a stable buffer.
This is difficult problem because the rate at which the buffer is being filled is constantly fluctuating because it is determined by TCP’s congestion control algorithm as a function of the congestive state of the network.
This problem is made harder because buffer fluctuations may also be caused by the VBR nature of the video stream.
It does not makes sense to constantly keep changing the video bit rate in response to network rate changes because studies have shown that switching back and forth between different video versions significantly degrades the user experience.
Hence, one of the objectives of the ABR algorithms is to avoid video rate fluctuations caused by short-term bandwidth variations and TCP throughput estimation errors.
In general, it has been observed that whereas algorithms that try to keep the buffer size stable cause larger variations in the video bit rate, algorithms that try to keep the video bit rate stable lead to larger variations in the buffer size.
Several techniques are used to smoothen out the video rate fluctuations, such as:  Use the receive buffer to dampen out temporary fluctuations in TCP bit rate: This is illustrated by the Threshold Based Buffer (TBB) algorithm
 Use smoothing filters to reduce fluctuations in the TCP bit rate estimates.
This is illustrated by the STB algorithm (see  ).
 Use fudge factors when making threshold based decisions.
Even though it is not a congestion control algorithm, most ABR designs conform to the AIMD rules.
Hence, the video bit rate is incremented smoothly in an additive manner when the available network bandwidth is consistently higher than the current bit rate.
Conversely, when the network bandwidth decreases because of congestion, ABR should quickly decrease the video bit rate to avoid playback freezes.
Another reason to use multiplicative decrease for the video rate is the fol- lowing:
In case of congestion, the TCP reacts by multiplicatively reducing its own bit rate, and because the objective of the ABR algorithm is to match the video bit rate to the TCP throughput, it follows that it should also try to reduce the video bit rate multiplicatively.
6.5DESCRIPTION OF SOME ADAPTIVE BIT RATE (ABR)
ALGORITHMS Adaptive Bit Rate or ABR algorithms are an active area of research, and new algorithms are being constantly invented in both academia and industry.
These can be broadly classified in the following categories: 1.Algorithms that rely mostly on their estimates of TCP throughput.
Algorithms in this category include: Network
Smoothed Throughput Based (STB)
[11] The FESTIVE algorithm
[12]187 6.5DESCRIPTION OF SOME ADAPTIVE BIT RATE (ABR) ALGORITHMS 2.Algorithms that rely mostly on the receive buffer size.
Algorithms in this category include: Threshold
These algorithms estimate the best video bit rate to use by solving a control theory problem.
In the research literature, there are algorithms that rely on the Send buffer size to control the video bit rate [17].
These algorithms have some inherent disadvantages compared with the receiver-based algorithms, including that they increase the complexity and cost of the video server.
Also, the benefits of caching the content in a distributed fashion are lost.
Hence, these types of algorithms are not covered in this chapter.
6.5.1 THROUGHPUT-BASED ADAPTIVE BIT RATE (ABR) ALGORITHMS This class of algorithms uses estimates of the TCP throughput as the main input to the algorithm.
However, all such algorithms also incorporate a safety feature whereby if the receive buffer size falls below a threshold, then the system switches the video bit rate to the minimum value to avoid a buffer underflow.
6.5.1.1 Network Throughput-Based (NTB) Algorithm
This is the simplest type of ABR algorithm.
Essentially, the video bit rate for the next download is set equal to the latest value of the TCP throughput estimate, so that Un115QðRnÞwhere R n5Unτ ~Tn(11) The quantization function Q chooses the video bit rate that is smaller than R nand closest to it.
Note that  cancels out the instantaneous fluctuations in TCP throughput by averaging over an interval, which is typically 2 to 10 seconds long.
The NTB algorithm is very effective in avoiding buffer underruns because it reacts instantaneously to fluctuations in TCP throughput.
Studies have shown that receive buffer size shows the least amount of variation under NTB com- pared with the other algorithms
However, on the flip side, it has the largest rate of changes in the video bit rate because it does nothing to dampen any of these fluctuations.
As a result, it is not commonly used in practice except as a way to benchmark other ABR algorithms.
shows an example a typical sample path of the NTB algorithm.
The video rate is typically set to the minimum value V(1) at start- up, but it quickly increases to the maximum bit rate V(L) because of application of  .
Downloads are done in a back-to-back manner until the receive buffer is full.
When this happens, the download interval is increased to τsec- onds to keep the average TCP throughput R equal to V(L).
If the TCP throughput falls below V(L), then the receive buffer occupancy starts to r educe, and the algorithm switches to the back- to-back downloads again.
This takes the system to the scenario illustrated in  ,w h e r ei n the absence of any smoothing, the algorithm will constantly switch between the bit rates that are above and below R.188  FLOW CONTROL FOR VIDEO APPLICATIONS 6.5.1.2 AIMD-Based Algorithm (ATB)
[11] makes a number of modifications to the NTB algorithm to smoothen out the bit rate fluctuations.
Using the same notation as before, the ATB algorithm keeps track of the ratio μgiven by μ5τ ~Tn(12)
Note that because μ5Rn Un,i fμ,1, then the video bit rate is larger than the TCP throughput, and the converse is true when μ.1.
Hence, μserves as a measure of network congestion.
The bit rate increment /C0decrement rules are as follows: I f μ.11ε(and the buffer size exceeds a threshold) where ε5maxVði11Þ2VðiÞ VðiÞ;’iA1;...;L
½/C138/C16/C17 , then the chunk download happens faster than the rate at which the decoder can play that chunk (see B).
The video bit rate is switched up from V(m) to V(m 11), which corresponds to an additive increase policy.
Hence, in contrast to NTB, the ATB algorithm increases the bit rate using an additive increase policy, which helps to reduce jumps in video quality.
Also, the factor εhelps to reduce bit rate oscillations for the case when the TCP throughput lies between two video bit rates (illustrated in A ).
I f μ,γd, where γd,1 is the switch-down threshold, then the download of a chunk takes longer than the time required for the decoder to play that chunk, which leads to a reduction in the receive buffer size.
Hence, an aggressive switch down is performed, with the reduced video bit rate chosen to be the first rate V(i) such that VðiÞ,μVðcÞ, where V(c) is the current bit rate.
This corresponds to a multiplicative decrease policy.
The presence of the factor γdmay lead to the situation in which the buffer drains down slowly.
To prevent an underflow, the algorithm switches to the minimum bit rate V(1) if the buffer falls below a threshold.
From this description, the ATB algorithm uses fudge factors in its threshold rules and a less aggressive increase rate to reduce bit rate fluctuations.
If the TCP throughput exceeds the maxi- mum video bit rate, then it spaces out the chun k download times to prevent the buffer from overflowing.
Download StartInitial Buffer FillSteady State for U = VL < R  Drop in Network Throughput so that R < VLRebuffering  Dynamics of the TCP throughput Based (NTB) algorithm.189 6.5DESCRIPTION OF SOME ADAPTIVE BIT RATE (ABR) ALGORITHMS 6.5.1.3 Smoothed Throughput
Based (STB) Algorithm To reduce the short-term fluctuations in the NTB algorithm, the STB algorithm
However, if there is a sudden decrease in TCP throughput, STB is not able to respond quickly enough, which can result in buffer underflows.
However, if the buffer size is big enough, then STB provides a simple and viable ABR technique.
The interval between chunks is controlled in the same way as in the NTB algorithm.
The FESTIVE Algorithm Jiang et al.
[12] pointed out the following problems that exist in ABR algorithms that are caused due to their measurements of TCP throughput to control the video bit rates: 
The chunk download start-time instances of the connections may get stuck in suboptimal locations.
For example, if there are three active connections, then the chunk download times of two of the connections may overlap, while the third connection gets the entire link capacity.
This leads to unfairness in the bit rate allocation.
Their suggested solution is to randomize the start of the chunk download times; that is, instead of downloading a chunk strictly at intervals of τseconds (or equivalently when the buffer size reaches a target value), move it randomly either backward or forward by a time equal to the length of download.
 Connections with higher bit rate tend to see a higher estimate of their chunk’s TCP throughput.
As explained in  , this is because the chunks of connections with higher bit rates occupy the bottleneck link longer, and as a result, they have a greater chance of experiencing time intervals during which they have access to the entire link capacity.
To solve this problem, Jiang et al.
[12] recommended that the rate of increase of the bit rate for a connection should not be linear but should decrease as the bit rate increases (another instance of the averaging principle!).
This policy can be implemented as follows: If the bit rate is at level k, then increase it to level k 11 only after k chunks have been received.
This will lead to a faster convergence between bit rates of two connections that are initially separated from each other.
Simple averaging of the TCP throughput estimates is biased by outliers if one chunk sees a very high or very low throughput.
To avoid this, Jiang et al.
[12] implemented a harmonic mean estimate of the last 20 throughput samples, which is less sensitive to outliers.
As a result of these design changes, the HAS design using the FESTIVE algorithm was shown to outperform the commercial HAS players by a wide margin.190  FLOW CONTROL FOR VIDEO APPLICATIONS 6.5.2 BUFFER SIZE-BASED ADAPTIVE BIT RATE ALGORITHMS This class of algorithms uses receive buffer size as their primary input, although most of them also use the TCP throughput estimate to do fine tuning.
It has been recently shown (see  ) that buffer size /C0based
ABR algorithms outperform those based on rate estimates.
The following rules are used for adapting the video bit rate in the TBB algorithm
t. Define the following threshold levels for the receive buffer, measured in seconds: 0 #Bmin,Blow,Bhigh.
The interval btar5½Blow;Bhigh/C138is called the target interval, and Bopt5ðBlow1BhighÞ=2 is the center of the target interval.
The TBB algorithm tries to keep the buffer level close to B opt.
 Let V(c) be the current video bit rate.
If the buffer level B(t) falls below B lowandVðcÞ.Rn, then switch to the next lower bit rate V(c 21).
The algorithm continues to switch to lower bit rates as long as the above conditions are true.
Note that if B(t) ,BlowandVðcÞ#Rn, it implies that the buffer level has started to increase, and hence there is no need to switch to a lower rate.
 With the current video bit rate equal to V(c), if the buffer level increases above B highand if Vðc11Þ,Ravg n, then switch to the next higher video bit rate V(c 11).
Note that Ravg nis the average TCP throughput, where the average is taken over the last few chunks (this number is configurable), which helps to dampen temporary network fluctuations.
Assuming that initially BðtÞ2τ.Bopt, this policy leads to a steady linear decrease in buffer size by τevery τseconds until it reaches the target operating point Bopt.
so that increasing the bit rate to V(c 11) will cause the buffer to start decreasing.
I f BðtÞAbtar, the algorithm does not change the video bit rate to avoid reacting to short-term variations in the TCP throughput.
If the bit rate is at maximum value V(L) or if  is satisfied, then the algorithm does not start the do wnload of the next chunk until the buffer level B(t) falls below
This keeps the buffer level at B optin equilibrium.
If the buffer level falls below B min, then switch to the lowest video bit rate to V(1).
Simulation results presented by Miller et al.
[10] show that the algorithm works quite well and achieves its objectives.
shows a sample path of the algorithm for the case when Bhigh550 sec, B low520 sec (so that B opt530 sec), and B min510 sec, with a single video stream passing over a network bottleneck link whose capacity is varied.
 When the link capacity goes up at t1 = 200 sec, the buffer size starts to increase, and when it crosses 50 sec, the algorithm starts to increase the bit rate in multiple steps.
When the bit rate reaches the maximum value, the algorithm reduces the buffer size linearly until it reaches B opt.191 6.5DESCRIPTION OF SOME ADAPTIVE BIT RATE (ABR) ALGORITHMS 
When the link capacity is reduced at t2 = 400 sec, it causes the buffer occupancy to decrease, and when it drops below 20 sec, the video bit rate is progressively decreased until it falls below the TCP throughput value.
At this point, the buffer starts to fill up again, and when it reaches Bopt, the algorithm spaces out the chunks to maintain the buffer at a constant value.
A nice feature of the TBB algorithm is that it enables the designer to explicitly control the trade-off between variation in buffer occupancy and fluctuations in video bit rate in response to varying TCP throughput.
This is done by controlling the thresholds B highand B low, such that a large value of the difference B high/C0Blowwill reduce video bit rate changes.
This aspect of TBB is orthogonal to the features in the FESTIVE algorithm, and a combination of the two will result in a superior system.
6.5.2.2 Buffer Based Rate Selection Algorithm The BBRS algorithm
[13] bases its bit rate selection function entirely on the receive buffer level.
This function is illustrated in  .
The TBB algorithm avoids this by keeping track of the TCP throughput in addition to the buffer occupancy, as explained in the previous section.
ADAPTIVE BIT RATE ALGORITHMS Inspired by the success of control theory in analyzing congestion control (see ), several researchers have attempted to use this theory to derive ABR algorithms
The initial work by Tian and Liu
[16] was based the objective of controlling the receive buffer occupancy to a reference level B refand then using the difference (B n/C0Bref) between the current occupancy and the reference value to drive the main control loop as shown in  .
(Note the analogy with the TCP case in which the objective of the controller was to regulate the bottle- neck buffer occupancy to some reference level.)
It is then possible to derive equations for the predicted video bit rate U nas a function of the buffer size difference and the estimated TCP throughput R n, which in turn determines the buffer occupancy level in combination with the actual TCP throughput, thus closing the control loop.
In general, it turns out that trying to keep the receive buffer occupancy at the reference level is not the most suitable way to drive the ABR control loop.
The reason for this is the fact that from a user point of view, it is more important to get the highest quality video that the network can sup- port while reducing the bit rate fluctuations.
Reservoir Cushion Bmax r r+c Buffer OccupancyVideo Bit Rate  Video bit rate control function for the Buffer Based Rate Selection (BBRS) algorithm.193 6.5DESCRIPTION OF SOME ADAPTIVE BIT RATE (ABR) ALGORITHMS long as it does not underflow.
The model in  does not take into account the objectives of reducing the bit rate fluctuations or maximizing the video quality.
In their algorithms, Tian and Liu[15] and Zhou et al.
[16] did include features to reduce the rate fluctuations, but this was done as an add-on to the control loop, not as a consequence of the control model itself.
Recently, Yin et al.
[14] have attacked the problem using tools from model predictive control (MPC) theory with much better results, and their work is described in this section.
In addition to taking multiple constraints into account in the derivation of the optimal bit rate, the theory also pro- vides a framework within which different algorithms can be compared with the performance of the best possible ABR algorithm, thus enabling us to objectively compare different algorithms.
The MPC optimization problem is defined next.
We will use the same notation that was intro- duced in  , with the following additions:
K: Total number of chunks in the video stream so that the duration of the video is Kτseconds q(.):
R-R1This is a function that maps the selected bit rate U kto the video quality perceived by the user q(U k).
It is assumed to be monotonically increasing.
Note that if Bk,τUk Rk, then the buffer becomes empty before the next download is complete, thus resulting in an underflow.
ControllerVideo Rate QuantizerReceive BufferTCP Tpt Estimate Actual TCP Tpt BnBref+ + +Rn Un Q(Un)Rn Wn  Application of control theory to the Adaptive Bit Rate (ABR) algorithm.194  FLOW CONTROL FOR VIDEO APPLICATIONS
Define the following Quality of Experience (QoE) elements for the system: 1.Average video quality:1 KXK k51qðUkÞ 2.Average quality variations:1 KXK21 k51jqðUk11Þ2qðUkÞj 3.Total rebuffer time:PK k51τUk Rk2Bk/C16/C171 Or number of rebufferings:PK k511τUk Rk.Bk/C16/C17
The QoE of video segments 1 through K are defined by a weighted sum of these components QoEK 15XK k51qðUkÞ2λXK21 k51qðUk11Þ2qðUkÞ/C12/C12/C12/C122μXK k51τUk Rk2Bk/C18/C191 (19) By changing the values of λandμ, the user can control the relative importance assigned to video quality variability versus the frequency or time spent on rebuffering.
Note that this definition of QoE takes user p into account and can be extended to incorporate other factors.
Note that QoE MAXK 1is a finite-horizon stochastic optimal control problem, with the source of the randomness being the TCP throughput R(t).
At time t kwhen the ABR algorithm is invoked to choose U k, only the past throughputs fRi;i,kgare known; the future throughputs fRi;i$kgare not known.
However, algorithms known bandwidth predictors can be used to obtain predictions defined as f^Ri;i$kg.
Based on this, the ABR algorithm selects bit rate of the next chunk kas: Uk5fðBk;f^Ri;i$kgÞ: (21)
Model predictive control [20] is a subset of this class of algorithms that choose bit rate U kby looking h steps ahead; that is, they solve the QoE maximization problem QoE MAXk1h kwith band- width predictions f^Ri;k#i#k1hgto obtain the bit rate U k.
The bit rate U kis then applied to the system, and that along with the actual observed TCP throughput R kis used to iterate the optimiza- tion process to the next step k 11.
For a given TCP throughput trace Rt;tA½t1;tK11/C138, we can obtain the offline optimal QoE, denoted as QoE(OPT) (which is obtained by solving QoE MAXK 1) because it has perfect knowl- edge of the future TCP throughputs for the entire duration of the video streaming.
This provides a theoretical upper bound for all algorithms for a given throughput trace.
Define QoE(A) to be the QoE for algorithm A and the normalized QoE of A as n2QoE ðAÞ5QoE ðAÞ QoE ðOPT
Their results are very interesting and are summarized in  , which graphs the distribution function for n-QoEs from 100 simulation runs.
It shows that BBRS performs be tter than NTB, and both are outperformed by the MPC algorithm, which gets quite close to the optimal.
1 0.8 0.6CDF0.4 0.2 0 0.75 0.8 0.85 Normalized QoE0.9 0.95 1MPC Buffer-Based  Rate-Based   Comparison of model predictive control (MPC), rate-based, and buffer-based Adaptive Bit Rate (ABR) algorithms.196  FLOW CONTROL FOR VIDEO APPLICATIONS 6.6THE PROBLEM WITH TCP THROUGHPUT MEASUREMENTS TCP throughput measurements form an essential component of the ABR algorithm.
Algorithms such as ATB and STB use these measurements as their main input, and even algorithms such as TBB or BBRS that are based on buffer size measurements are affected by TCP throughput because the transmit time of a chunk (which determines whether the buffer grows or shrinks) is inversely proportional to the TCP throughput.
Recall that the ABR algorithms estimates the per-chunk TCP throughput using the formula Rn5Unτ ~Tn Consider the case when ABR-controlled video streams pass through a common network bottle- neck node with capacity C and assume that the maximum video bit rate VðLÞ,Cand 2 VðLÞ/C25C. When only one of the video streams is active, it will result in the scenario shown in  where there is a gap between successive chunks in steady state.
When both the video streams become active, then their chunks can get distributed according to one of the scenarios shown in  .
In case (A) in the figure, there is no overlap between the chunks at the bottleneck, which implies that each stream gets the full bottleneck bandwidth (i.e., R1 n5R2 n5C).
In case (C), there is full overlap so that R1 n5R2 n5C 2. Hence, only in case c does the TCP throughput measure ment reflect the ideal allocation of bottleneck bandwidth between the two sources.
This effect, which was first pointed out by Akhshabi et al.
Rn < CNo Overlap Rn = C  Overlap of chunks from two video sources at a common bottleneck.197 6.6THE PROBLEM WITH TCP THROUGHPUT MEASUREMENTS is attributable to the fact that ABR bunches together video packets into chunks for transmission.
If the video sources are allowed to transmit con tinuously, then this effect goes away.
[21] carried out a simulation study with 100 sources sharing a bottleneck node of capacity 100 mbps.
For the case of an ABR interdownload time of τ52 seconds, the transmissions were randomly distributed between 0 and 2 seconds.
The TCP throughput was measured using  using various values of link subscription and is shown in  .
It shows the same behavior that we observed for two sources: If the link utilization is less than 100%, then the mea- sured throughput is much higher than the fair bandwidth allocation (about three times the fair-share bandwidth in this example).
This causes ABR to increase the video bit rate allocations.
As a result of this, the link subscription soon rises above 100%, which causes the TCP throughput to fall pre- cipitously, and the cycle repeats.
This behavior has been christened the “bandwidth cliff” effect by Li et al.
[21]  illustrates the periodic variation in the TCP throughput estimates, the video bit rates and the utilization of the bottleneck link cap acity, caused by the bandwidth cliff.
[21] have recently designed an ABR algorithm called PANDA to avoid the bandwidth cliff effect.
The FESTIVE algorithm uses randomization of the chunk download start times to avoid this problem.
By considerations similar to that used for  , it follows that if two HAS sessions are sharing a link and one of them has higher bit rate than the other, then the TCP throughput estimate for the higher bit rate connection will also be higher than that of the lower bit rate connection, which results in unfairness.
This is because the higher bit rate connection occupies the link for a longer time, and as a result there are periods when it has access to the entire link bandwidth, thus pushing up its TCP throughput.
This problem was first pointed out by Jiang et al.
One of theimportant issues that researchers have started to investigate only recently is How does ABR ratecontrol interact with TCP congestion control, and how do multiple ABR-controlled streams interactwith one another?
These topics are discussed in the following two subsections, but in this section,we will describe the consequences of using TCP for transporting individual HAS chunks.
The video stream is sent in a series of individual chunks over TCP, which results in the follow- ing issues: 1.The TCP window size that is reached at the end of chunk n serves as the initial window size for chunk (n 11).
As a result, when the chunk (n 11) transmission starts, up to W TCP segments are transmitted back to back into the network (where W is the TCP window size).
For largevalues of n, this can overwhelm buffers at congested nodes along the path and result in bufferoverflows.
2.Assuming that a chunk consists of m packets, all transmission stops after the m thpacket is sent.
Consequently, this scenario results in TCP Timer (retransmission timeout [RTO]) expiry, which is followed by a reset of the congestion window to its minimum value.
Hence, the interaction between HAS and TCP results in a burst transmission at the start of a chunk and can result in a RTO at the end of a chunk.
Mitigating these problems is still an area of active study.
To reduce the packet burst problem at the beginning of the chunk, there have been a couple of suggestions.
[22] proposed using pacing of TCP segments for the entire chunk, so that a window of segments is uniformly transmitted over the duration of a RTT.
This did reduce the incidence of packet losses at the start of the chunk but shifted them to the end of the chunk instead.
This can lead to a greater incidence of the second issues described earlier, thus making the performance even worse.
[23] also suggest using TCP pacing but only restricted to the first RTT after the chunk transmission starts, which resulted in a better performance.
[23] also made a suggestion to mitigate the tail loss issue mentioned earlier.
Their tech- nique consists of having the sender send multiple copies of the last packet in the chunk in response to the ACKs that come in after the first copy of the last packet has been transmitted.
Hence, if one of the last three packets in the window is lost, then this mechanism creates a duplicate ACK flow from the receiver, which enables the Fast Retransmit mechanism to work as intended.
6.7.1 BANDWIDTH SHARING BETWEEN TCP AND HTTP ADAPTIVE STREAMING The scenario in which HAS and TCP streams share a bottleneck link is very common in current networks, and hence it is very important to investigate how they share the available bandwidth.
This issue was investigated in detail by Huang et al.
[24], who carried out a measurement-based study.
Their main finding was that HAS streams suffer from what they called a “downward spiral” effect in the presence of a competing TCP flow.
As soon as this happens, the HAS client picks a video rate that is far below the available bandwidth even though both types of flows share the same round trip latency.
If the computed throughput is an underestimation of the actual available band- width, then this can lead to the “downward spiral” shown in  .
This indeed turns out to be the case, and the detailed dynamics are as follows:
The following scenario assumes a bottleneck rate of 2.5 mbps.
In the absence of the competing TCP flow, the HAS stream settles into the usual ON-OFF sequence.
During the OFF period, the TCP congestion window times out because of inactivity of greater than 200 ms and resets cwnd to the initial value of 10 packets.
Hence, the sender needs to ramp up from slow-start for each new chunk.
 When the competing TCP flow is started, then it fills up the buffer at the bottleneck node during the time the video flow is in its OFF period.
As a result, when the video flow turns ON again, it experiences a high packet loss, which results in a decrease in its TCP window size.
Also, the chunk transmission finishes before the video streams TCP window has a chance to recover.
As a result because the ABR estimate of the video rate in  , is actually an estimate of the TCP flow on which the video is riding, and because the TCP throughput suffers in the presence of a competing flow because of the reasons explained earlier, it follows that the ABR algorithm badly underestimates the available link bandwidth.
6.8FURTHER READING The YouTube video service from Google does not make use of HAS [25,26] .
Instead, it uses a modi- fied form of TCP-based Progressive Download (PD) to transmit the video stream, which operates as follows: At startup, the server transmits the video as fast as it can until the playout buffer (which is 40 sec in size) at the receiver is full.
Then it uses rate control to space out the rest of the video trans- mission, which is also sent in chunks.
If the TCP throughput falls, then the send buffer at the server starts to fill up (see  ), and this information is used to backpressure the video server.
[1] Handley M, Floyd S, Padhye J, Widmer J. TCP Friendly Rate Control (TFRC): protocol specification.
An end-to-end rate based congestion control mechanism for real- time streams in the Internet.
Multimedia streaming via TCP: an analytic performance study.
[5] Zambelli A. IIS smooth streaming technical overview.
HTTP dynamic streaming on the Adobe Flash platform.
[7] Watson M. HTTP adaptive streaming in practice.
An experimental evaluation of rate-adaptation algorithms in adap- tive streaming over HTTP.
What happens when HTTP adaptive streaming players compete for bandwidth.
[10] Miller K, Quacchio E, Gennari G, Wolisz A. Adaptation algorithm for adaptive streaming over HTTP.
[11] Liu C, Bouazizi I, Gabbouj M. Rate adaptation for adaptive HTTP streaming.
[12] Jiang J, Sekar V, Zhang H. Improving fairness, efficiency and stability in HTTP based adaptive video streaming with FESTIVE.
Using the buffer to avoid rebuffers: evidence from a large video streaming service.
Toward a principled framework to design dynamic adaptive streaming algorithms over HTTP.
Towards agile and smooth video adaptation in dynamic HTTP streaming.
[16] Zhou C, Lin CW, Zhang X, Guo Z. Buffer based smooth rate adaptation for dynamic HTTP streaming.
An experimental investigation of the Akamai adaptive video streaming.
An evaluation of bitrate adaptation methods for HTTP live streaming.
[19] Bui N, Michelinakis F, Widmer J. A model for throughput prediction for mobile users.
[21] Li Z, Zhu X, Gahm J, et al. Probe and adapt: rate adaptation for HTTP video streaming at scale.
[22] Esteban J, Benno SA, Beck A, et al. Interaction between HTTP adaptive streaming and TCP.
[23] Liu X, Men A, Zhang P. Enhancing TCP to improve throughput of HTTP adaptive streaming.
Confused, timid and unstable: picking a video streaming rate is hard.
[25] Alcock S, Nelson R. Application flow control in YouTube video systems.
[26] Ameigeiras P, Munoz JJ, Navarro-Ortiz J, Lopez-Soler JM.
Analysis and modeling of YouTube traffic.
Server based traffic shaping for stabilizing oscillating adaptive streaming players.
Begen AC, Akgul T, Baugher M. Watching video over the web.
Part 1: streaming protocols.
Begen AC, Akgul T, Baugher M. Watching video over the web.
Part 2: applications, standardization and open issues.
Cicco L, Mascolo S, Palmisano V. Feedback control for adaptive live video streaming.
Erman J, Gerber A, Ramadrishnan KK, et al.
Over the top video: the gorilla in cellular networks.
Gill P, Arlitt M, Li Z, Mahanti A. YouTube traffic characterization: a view from the edge.
Ghobadi M, Cheng Y, Jain A, Mathis M. Trickle: rate limiting YouTube video streaming.
Kim T, Avadhanam N, Subramanian S.
Dimensioning receiver buffer requirements for unidirectional VBR video streaming over TCP.
Moving Picture Experts Group.
Network characteristics of video streaming traffic.
CoNEXT 2011:25.203 SUGGESTED READING CHAPTER 7CONGESTION CONTROL IN DATA CENTER NETWORKS 7.1INTRODUCTION
This chapter discusses the topic of congestion control in data center networks (DCN), which have assumed a lot of importance in the networking arena lately.
DCNs are used to create “cloud”-based massively parallel information processing systems, which are integral to the way computing is done today.
They underlie the massive computing infrastructure that run web sites such as Google and Facebook and constitute a key competitive advantage for these types of companies.
Modern data centers are created by interconnecting together a large number of commodity servers and their associated storage systems, with commodity Ethernet switches.
They create a “warehouse-scale” computing infrastructure
[1]that scales “horizontally”; that is, we can increase the processing power of the data center by adding more servers as opposed to increasing the proces- sing power of individual servers (which is a more expensive proposition).
The job of a DCN in the data center architecture is to interconnect the servers in a way that maximizes the bandwidth between any two servers while minimizing the latency between them.
The DCN architecture should also allow the flexibility to easily group together servers that are working on the same application, irrespective of where they are located in the DCN topology.
From the congestion control point of view, DCNs have some unique characteristics compared with the other types of Internet Protocol (IP) networks that we have seen so far in this book: 
The round trip latencies in DCNs are extremely small, usually of the order of a few hundred microseconds, as opposed to tens of milliseconds and larger for other types of networks.
Applications need very high bandwidths and very low latencies at the same time.
 There is very little statistical multiplexing, with a single flow often dominating a path.
To keep their costs low, DCN switches have smaller buffers compared with regular switches or routers because vendors implement them using fast (and expensive) static random-access memory (SRAM) to keep up with the high link speeds.
Moreover, buffers are shared between ports, so that a single connection can end up consuming the buffers for an entire switch [2].
As a result of these characteristics, normal TCP congestion control (i.e., TCP Reno) does not perform very well because of the following reasons:  TCP requires large buffers; indeed, the buffer size should be greater than or equal to the delay- bandwidth product of the connection to fully use the full bandwidth of the link as shown in . 205Internet Congestion Control.
End-to-end delays of connections under TCP are much larger than what can be tolerated in DCNs.
The delays are caused by TCP’s tendency to fill up link buffers to capacity to fully use the link.
Another special characteristic of DCNs is that they are homogeneous and under a single administra- tive control.
Hence, backward compatibility, incremental deployment, or fairness to legacy protocols are not of major concern.
As a result, many of the new DCN congestion control algorithms require fairly major modifications to both the end systems as well as the DCN switches.
Their main objective is to minimize end-to-end latency so that it is of the order of a few milliseconds while ensuring high link utilization.
DCN congestion control algorithms fall in two broad categories: 
Algorithms that retain the end-to-end congestion control philosophy of TCP: Data Center TCP (DCTCP)
[3], and High bandwidth Ultra Low Latency (HULL)
[4], fall into this category of algorithms.
They use a more aggressive form of a Random Early Detection (RED) /C0like Explicit Congestion Notification (ECN) feedback from congested switches that are then used to modify the congestion window at the transmitter.
Algorithms that depend on in-network cong estion control mechanisms: Some recently proposed DCN congestion control protocols such as D3[5], Preemptive Distributed Quick Flow Scheduling (PDQ)
hey all use additional mechanisms at the switch, such as bandwidth reservations in D3, priority scheduling in pFabric or packet by packet load balancing in DeTail.
The general trend is towards a simplified form of rate control in the end systems coupled with greate r support for congestion control in the network because this leads to much faster response to conge stion situations.
This i sam a j o rd e p a r t u r ef r o m the legacy congestion control philosophy of pu tting all the intelligence in the end system.
To communicate at full speed between any two servers, the interconnection network provides multiple paths between them, which is one of the distinguishing features of DCNs.
This leads to new open problems in the area of how to load balance the traffic among these paths.
[9,10] is one of the approaches that has been suggested to solve this problem, using multiple simultaneous TCP connections between servers, whose windows are weakly interacting with one another.
The Incast problem is a special type of traffic overload situation that occurs in data centers.
It is caused by the way in which jobs are scheduled in parallel across multiple servers, which causes their responses to be synchronized with one another, thus overwhelming switch buffers.
The rest of this chapter is organized as follows:  provides a brief overview to the topic of DCN Interconnect networks.
We also discuss the traffic generation patterns in DCNs and the origin of the low latency requirement.
discusses the DCTCP algorithm, and  explores Deadline Aware algorithms, including D2TCP and D3. is devoted to MPTCP, and  describes the Incast problem in DCNs.
An initial reading of this chapter can be done in the sequence 7.1 -7.2-7.3-7.5, which contains an introduction to data center architectures and a description and analysis of the DCTCP206  CONGESTION CONTROL IN DATA CENTER NETWORKS and Multipath TCP algorithms.
More advanced readers can venture into Sections 7.4 and 7.6 , which discuss deadline aware congestion control algorithms and the Incast problem.
7.2DATA CENTER ARCHITECTURE AND TRAFFIC PATTERNS
A typical data center consists of thousands of comm odity servers that are co nnected together using a special type of Ethernet network ca lled an Inter-connection Network .
An example of a data center using a traditional tree-type interconnection arc hitecture is shown in  , which illustrates the main features of this type of network:
Servers are arrang e di nr a c k sc o n s
Ea ch ToR is connected to tw o aggregation switches (ASs) for redundancy, perhaps through an interm ediate layer of L2 switches.
Each AS is further connected to two aggregation routers (ARs), such th at all switches below each pair of ARs form a single Layer 2 domain, connecting several t housand servers.
Se rvers are also partitioned into Virtual Local Area Networks (VLANs) to limit packet flooding and Addr ess Resolution Protocol (ARP) broadcasts and to create a logical server group that can be assigned t o a single application.
ARs in turn are connected to core routers (CRs) that are responsible for the interc onnection of the data center with the outside world.
Some of the problems with this architecture include the following:  Lack of bisection bandwidth: Bisection bandwidth is defined as the maximum capacity between any two servers.
Even though each server may have a 1-Gbps link to its ToR switch and hence to ToR ToR ToR ToRAS ASARCR CR ToR ToR ToR ToRAS ASAR AR Layer 2 DomainsInternet AR  Traditional tree-type data center network architecture.207 7.2DATA CENTER ARCHITECTURE AND TRAFFIC PATTERNS other servers in its rack, the links further up in the hierarchy are heavily oversubscribed.
For example, only 4 Gbps may be used on the link between the ToR and AS switches, resulting in 1:5 oversubscription when there are 20 servers per rack, and paths through top-most layers of the tree may be as much as 1:240 oversubscribed.
As a result of this, designers tend to only user servers that are closer to each other, thus fragmenting the server pool (i.e., there may be idle servers in part of the data center that cannot be used to relieve the congestion in another portion of the system).
Note that bisection bandwidth is a very important consideration in DCNs because the majority of the traffic is between servers, as opposed to between servers and the external network.
The reason for this is that large DCNs are used to execute applications such as Web search, analytics, and so on that involve coordination and communication among subtasks running on hundreds of servers using technologies such as map-reduce.
 Configuration complexity: Another factor that constrains designers to use servers in the same Layer 2 domain for a single service is that adding additional servers to scale the service outside the domain requires reconfiguration of IP addresses and VLAN trunks because IP addresses are topologically significant and are used to route traffic to a particular Layer 2 domain.
As a result, most designs use a more static policy whereby they keep additional servers idle within the same domain to scale up, thus resulting in server under-utilization.
Poor reliability and redundancy: Within a Layer 2 domain, the use of Spanning Tree protocol for data forwarding results in only a single path between two servers.
Between Layer 2 domains, up to two paths can be used if Equal Cost Multi-Path (ECMP)
To address these problems, several new data center architectures have been proposed in recent years.
Some of the more significant designs include the following: The VL2 Inter-Connection Network: Instead of using the Tree Architecture, Greenberg et al.
[12] proposed using a Clos network
As shown, each 1GToR ToR ToR ToRAS ASIS IS ToR ToRASISInternet DI Aggregation Switches With DA Ports eachDA/2 Intermediate Switches With DI Ports each DADI/4 ToR Switches10G 10GDA/2 Links DA/2 Links 5DADI Servers  The VL2 interconnection network.208  CONGESTION CONTROL IN DATA CENTER NETWORKS ToR switch is still connected to two AS switches using 1-Gbps links, but unlike the Tree Architecture, each AS switch is connected to every other AS switch in 2-hops, through a layer of intermediate switches (ISs) using 10 Gbps or higher speed links.
As a result, if there are n IS switches, then there are n paths between any two ASs, and if any of the IS fails, then it reduces the bandwidth between 2 AS switches by only 1/n.
For the VL2 network shown in  , the total capacity between each layer is given by D IDA/2 times the link capacity, assuming that there are D I AS switches with D Aports each.
Also note that the number of ToR switches is given by D ADI/4, so that if there are M servers attached with a 1-Gbps link to the ToR and the links between the AS and IS are at 10 Gbps, then equating the total bandwidth from the AS to the ToR switches to the total bandwidth in the opposite direction, we obtain M3DADI 45103DADI 2i:e:M520 servers per ToR, so that the network can support a total of 5D ADIservers, with a full bisection bandwidth of 1 Gbps between any two servers.
The VL2 architecture uses Valiant Load Balancing (VLB)
[14] among flows to spread the traffic through multiple paths.
VLB is implemented using ECMP forwarding in the routers.
VL2 also enables the system to create multiple Virtual Layer 2 switches (hence the name), such that it is possible to configure a virtual switch with servers that may be located anywhere in the DCN.
All the servers in Virtual Switch are able to communicate at the full bisection bandwidth and further- more are isolated from servers in the other Virtual Switches.
To route packets to a target server, the system uses two sets of IP addresses.
An application’s AA does not change if it migrates from server to server, and each AA is associated with an LA that serves as the identifier of the ToR switch to which it is connected.
VL2 has a Directory System (DS) that stores the mapping between the AAs and LAs.
When a server sends a packet to its ToR switch, the switch consults the DS to find out the destination ToR and then encapsulates the packet at the IP level, known as tunneling, and forwards it into the DCN.
To take advantage of the multiple paths, the system does load balancing at the flow level by randomizing the selection of the Intermediate Switch used for the tunnel.
Data center networking /C0oriented switching protocols such as VxLAN and TRILL use a similar design.
A VL2-based DCN can be implemented using standard switches and routers, the only change required is the addition of a software layer 2.5 shim in the server protocol stack to implement address resolution and tunnel selection functions.
The Fat Tree Architecture: Vahdat et al.
[15] proposed another Clos-based DCN architecture called Fat Tree (  ).
The network is built entirely with k-port 1-Gbps switches and is con- structed around k pods, each of which has two layers of switches, with (k/2) switches in each layer.
The bottom layer of switches in a pod is connected to the servers, with (k/2) servers connected to each switch.
Similarly, it can be shown that there are k2/4 core switches per network, so that the number of equal-cost multipaths between any pair of hosts is also given by k2/4.
Because the total bandwidth between the core switch and aggregation209 7.2DATA CENTER ARCHITECTURE AND TRAFFIC PATTERNS switch layers is given by k3/4, it follows that the network has enough capacity to support a full 1-Gbps bisectional bandwidth between any two servers in the ideal case.
The main distinction compared with VL2 is the fact that the Fat Tree network is built entirely out of lower speed 1 Gbps switches.
However, this come at the cost of larger number of intercon- nections between switches.
To take full advantage of the Fat Tree topology and realize the full bisection bandwidth, the traffic between two servers needs to be spread evenly between the (k/2)2 paths that exist between them.
It is not possible to do this using the traditional ECMP-based IP routing scheme, so Vahdat et al. designed a special two-level routing table scheme to accom- plish this.
Modern data center designs use a simpler form the Clos architecture called Leaf Spine.
VL2 is an example of a 3-Layer Leaf Spine design with ToR, AS and IS switches forming the three layers.
It is possible to do a 2-Layer Leaf Spine design with just the ToR and IS layers, with each ToR switch directly connected to every one of the IS switches.
As both the VL2 and the Fat Tree interconnections illustrate, the biggest difference between DCNs and more traditional networks is the existence of multiple paths between any two servers in a DCN.
This opens the field to new types of congestion control algorithms that can take advantage of this feature.
7.2.1 TRAFFIC GENERATION MODEL AND IMPLICATIONS FOR CONGESTION CONTROL
We provide a short description of a typical online transaction that generates most of the traffic in large DCNs today (  ).
Applications such as web search, social network content AS AS AS  ESES ESAS AS  ES ESAS AS  ES ESAS AS  ES ES(k/2)2 Core k port switches k2/2 Aggregation k port switches k2/2 Edge k port
The Fat Tree interconnection network.210  CONGESTION CONTROL IN DATA CENTER NETWORKS composition, and advertisement selection are all based around this design pattern.
Each of the nodes in  represents the processing at a server node; the connector between nodes stands for the transmission of a flow across the interconnection network.
A typical online transaction is generated when a user enters a query into a search engine.
Studies have shown that the response needs to be generated within a short deadline of 230 to 300 ms.
A very large data set is required to answer the query, which is spread among thousands of servers in the DCN, each with its own stor- age.
The way a query progresses through a DCN is shown in  .
The query arrives at the root node, which broadcasts it to down to the next level, which in turn generate their own queries one level down until the leaf nodes holding the actual data are reached.
Each leaf node sends its response back to its parent, which aggregates the replies from all its child nodes and sends it up to the root.
Furthermore, answering a query may involve iteratively invoking this pattern; one to four iterations are typical, but as many as 20 may occur.
This overall architecture is called scatter-gather, partition-aggregate, or map-reduce.
The propa- gation of the request down to the leaves and the responses back to the root must complete within the overall deadline that is allocated to the query; otherwise, the response is discarded.
To satisfy this, the system allocates a deadline to each processing node, which gets divided up into two parts: the computation time in the leaf node and the communication latency between the leaf and the par- ent.
The deadlines for individual computational nodes typically vary from about 10 to 100 ms.
Some example delay allocations to the nodes and the communications links are shown in  , from which we can see that the deadlines for communications between hosts cannot exceed tens of milliseconds if the system is to be able to meet the overall job deadline.
This model of data center job computation leads to a few different models for congestion con- trol algorithms that are optimized for DCNs, namely: 
Minimization of flow latency becomes a very important requirement in DCNs, which was not the case for other congestion control scenarios.
At the same time, other flows in the data center are throughput intensive.
For example, large transfers that update the internal data structures at a server node fall into the latter category; hence, throughput maximization and high link utilization cannot be ignored as the congestion control objective.
Based on this insight, a number of new congestion control algorithms, such as DCTCP, try to minimize latency with an aggressive form of AQM while not compromising on throughput.
A second class of algorithms tackles the issue of deadlines for flows directly by using congestion control algorithms that take these deadlines into account.
A third class of algorithms have adopted a more radical design by parting ways with the “all intelligence should be in the end system” philosophy of the Internet.
They contend that end system /C0based controls have an unsurmountable delay of one round trip before they can react to network congestion, which is too slow for DCNs.
Instead they propose to make the network responsible for congestion control while limiting the end system to very simple nonadaptive rate regulation and error recovery functions.
Some of these algorithms take advantage of the fact that there are multiple paths between any two servers; hence, the network nodes can quickly route around a congested node on a packet-by-packet basis (e.g., see [7]).
7.3DATA CENTER TCP (DCTCP) DCTCP was one of the earliest congestion control algorithms designed for DCNs and is also one of the best known.
It was invented by Alizadeh et al.
[2], who observed that query and delay sensitive short messages in DCNs experienced long latencies and even packet losses caused by large flows consuming some or all of the buffer in the switches.
This effect is exacerbated by the fact that buffers are shared across multiple ports, so that a single large flow on any one of the ports can increase latencies across all ports.
As a result, even though the round trip latencies are of the order of a few hundred microseconds, queuing delays caused by congestion can increase these latencies by two orders of magnitude.
Hence, they concluded that to meet the requirements for such a diverse mix of short and long flows, switch buffer occupancies need to be persistently low while maintaining high throughput for long flows.
Traditionally, two classes of congestion control algorithms are used to control queuing latencies:  Delay-based protocols such as TCP Vegas: These use increases in measured R ound Trip Latency (RTT) as a sign of growing queue lengths and hen ce congestion.
They rely heavily on accurate RTT measurements, which is a problems in a DCN environment because the RTTs are extremely small, of the order of a few hundred microsecond s. Hence, small noise fl uctuations in latency become indistinguishable from congestion, which can cause TCP Vegas to react in error. 
AQM algorithms: These use explicit feedback from the congested switches to regulate the transmission rate, RED being the best known member of this class of algorithms.
DCTCP falls within this class of algorithms.212  CONGESTION CONTROL IN DATA CENTER NETWORKS Based on a simulation study, Alizadeh et al.
[2]came to the conclusion that legacy AQM schemes such as RED and PI do not work well in environments where there is low statistical multi- plexing and the traffic is bursty, both of which are present in DCNs.
DCTCP is able to do a better job by being more aggressive in reacting to congestion and trading off convergence time to achieve this objective.
DCTCP operates as follows:
1.At the switch: An arriving packet at a switch is marked with
Congestion Encountered (CE) codepoint if the queue occupancy is greater than a threshold K at its arrival.
A switch that supports RED can be reconfigured to do this by setting both the low and high threshold to K and marking based on instantaneous rather than average queue length.
2.At the receiver: A DCTCP receiver tries to accurately convey the exact sequence of marked packets back to the sender by ACKing every packet and setting the ECN-Echo flag if and only if the data packet has a marked CE codepoint.
This algorithm can also be modified to take delayed ACKs into account while not losing the continuous monitoring property at the sender [2].
3.At the sender: The sender maintains an estimate of the fraction of packets that are marked, called α, which is updated once for every window of data as follows: α’ð12gÞα1gF (1) where F is the fraction of packets that were marked in the last window of data and 0 ,g,1i s the smoothing factor.
Note that because every packet gets marked, αestimates the probability that the queue size is greater than K. Features of TCP rate control such as Slow Start, additive increase during congestion avoidance, or recovery from lost packets are left unchanged.
However, instead of cutting its window size by 2 in response to a marked ACK, DCTCP applies the following rule once every RTT:
Hence, if very few packets are marked then the window size hardly reduces, and conversely in the worst case if every packet is marked, then the window size reduces by half every RTT (as in Reno).
Following Alizadeh et al.
It can be shown that in the fluid limit, the DCTCP window size W, the marking estimate α, and the queue size at the bottleneck node b(t) satisfy the following set of delay-differential equations:
Equations 4 to 6 are equivalent to the corresponding equations 54 and 55 for TCP Reno pre- sented in .
can be derived as the fluid limit of  , and  can be derived from the fact that the data rate for each of the N sessions is given by W(t)/T(t), while C is the rate at which the buffer gets drained.
Using the approximation T 5D1K/C (where D is the round trip propagation delay), it was shown by Alizadeh et al.
[16] that this fluid model agrees quite well with simulations results.
Because of the 0-1 type marking function p(t), this system of equations does not have a fixed point and instead converges to a periodic limit-cycle behavior in steady state.
It is possible to do a stabil- ity analysis of the system using sophisticated tools from Poincare Map theory and thus derive con- ditions on the parameters C, N, D, K, and g for the system to be stable, which are gAð0;1/C138and (CD1K)/N.2.
They also showed that to attain 100% throughput, the minimum buffer size K at the bottleneck node is given by K.0:17CD (8) This is a very interesting result if we contrast it with the minimum buffer size required to sus- tain full throughput for TCP Reno, which from  is given by CD.
Hence, DCTCP is able to get to full link utilization using just 17% of the buffers compared with TCP Reno, which accounts for the lower latencies that the flows experience.
Using a simpler model and under the assumption that all N connections are synchronized with each other, it is possible to obtain expressions for the fluctuation of the bottleneck queue size [2], and we do this next (  ).
This model, called the Sawtooth model by Alizadeh et al.
[16],i s of the more traditional type that we have analyzed in previous chapters, and the synchronization assumption makes it possible to compute the steady-state fraction of the packets that are marked at a switch.
This quantity is assumed to be fixed, which means that the model is only accurate for small values of the smoothing parameter g. An analysis of the Sawtooth model is done next: Let S(W 1,W2) be the number of packets sent by the sender while its window size increases from W 1to W 2.W1.
Note that this takes (W 2/C0W1) round trip times because the window increases by at most 1 for every round trip.
However, it takes one more round trip d elay for this information to get to the source, during which the window size increases to W/C311.
To compute the magnitude of oscillations in the queue size, we first compute the magnitude of oscillations in window size of a single connection, which is given by Δ5ðW/C311Þ2ðW/C311Þ12α 2/C16/C17 5ðW/C311Þα 2(12) Because there are N flows, it follows that the oscillation in the queue size bδ, is given by bδ5NΔ5NðW/C311Þα 2/C25Nﬃﬃﬃﬃﬃﬃﬃ W/C3 2r 5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ NðCD1KÞ 2r (13)
From  , it follows that the amplitude of queue size oscillations in DCTCP is Oﬃﬃﬃﬃﬃﬃﬃ CDp/C0/C1 , which is much smaller than the oscillations in TCP Reno, which are OðCDÞ.
This allows for a smaller threshold value K, without the loss of throughput.
Indeed, the minimum value of the queue size b min, can also be computed and is given by bmin5bmax2A 5K1N2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ NðCD1KÞ 2r(14) To find a lower bound on K, we can minimize  over N and then choose K so that this minimum is larger than zero, which results in K.CD 750:14CD (15) which is close to the value 0.17D derived from the more exact analysis by Alizadeh et al.
[16]. Simulation results in Alizadeh et al.
[2]show that DCTCP achieves its main objective of reduc- ing the size of the bottleneck queue size; indeed, the queue size with DCTCP is 1/20ththe size of the corresponding queue with TCP Reno and RED.
Using this model, it is also possible to derive an expression for the average throughput of DCTCP as a function of the packet drop probability, and we do so next.
Using the deterministic approximation technique described in  of , we compute the number of packets transmitted during a window increase /C0decrease cycle M and the length of the cycle τto get the average throughput Ravg5M τ, so that Ravg5M TW/C32ð12α 2ÞW/C3/C2/C3 5M Tðα=2ÞW/C35M Tﬃﬃﬃﬃﬃﬃﬃ 2 W/C3r (16) In this equation, ðα=2ÞW/C3is
the number of round trip latencies in a cycle, and αis approxi- mated by  .
and 19 imply that DCTCP is comparable to high-speed protocols such as High Speed TCP (HSTCP) in being able to use large link capacities in an efficient manner.
Unlike HSTCP or CUBIC, DCTCP does not make allowances to be Reno friendly, so there is no cut-off link capacity below which DCTCP behaves like Reno.
The higher throughput of DCTCP compared with Reno can be attributed to DCTCP’s less drastic reductions in window size on encountering packet drops.
The exponent d of the packet drop probability is given by d 50.75, which points to high degree of intraprotocol RTT unfairness in DCTCP.
However Alizadeh et al.
[2]have shown by simulations that the RTT unfairness in DCTCP is actually close to that of Reno with RED.
Hence, the unfairness predicted by  is an artifact of the assumption that all the N ses- sions are synchronized in the Sawtooth model.
THE HULL MODIFICATION TO IMPROVE DCTCP Alizadeh et al.
[4], in a follow-on work, showed that it is possible to reduce the flow latencies across a DCN even further than DCTCP by signaling switch congestion based on link utilizations rather than queue lengths.
Note that the PQ is not really a queue because it does not store packets; however, it is simply a counter that is updated as packets exit the link to determine the queuing that would have occurred on a slower virtual link (typically about 10% slower).
It then marks the ECN for packets that pass through it when the simulated queue is above a fixed threshold.
The PQ attempts to keep the aggregate transmission rate for the congestion-controlled flows to be strictly less than the link capacity, which keeps the switch buffers mostly empty.
The system also uses DCTCP as its congestion control algorithm to take advantage of its low latency properties.
To further reduce the queue build-up in the switch es, HULL also implements Packet Pacing at the sender.
The pacer should be implemented in hardware in the server Network Interface Card (NIC) to smooth out the burstiness caused by the data transfer mechanism between the main memory and the NIC.
As a result of these enhancements, HULL has been shown achieve 46% to 58% lower average latency compared with DCTCP, and 69% to 78% slower 99th percentile latency.
7.4DEADLINE-AWARE CONGESTION CONTROL ALGORITHMS  showed that flows in DCNs come with real-time constraints on their completion times, typically of the order of tens of milliseconds.
One of the issues with DCTCP is that it does not take these deadlines into account; instead, because of its TCP heritage, it tries to assign link bandwidth fairly to all flows irrespective of their deadlines.
As a result, it has been shown [5]that as much as 7% of flows may miss their deadlines with DCTCP.
A number of recent DCN conges- tion control proposals seek to address this shortcoming.
All of them try to incorporate the flow deadline information into the congestion control algorithm in some way and in the process have a varying impact on the implementation at the sender, the receiver, and the switches.
In this subsection, we will describe two of these algorithms, namely, Deadline Aware Datacenter TCP (D2TCP)
[3]and D3[5], in order of increasing difference from the DCTCP design.
7.4.1 DEADLINE AWARE DATACENTER TCP Historically, the D3algorithm came first, but we will start with D2TCP because it is closer in design to DCTCP and unlike D3does not impact the switches or make fundamental changes toData SourceDTCP SenderDCTCP ReceiverC  Phantom Queue
γCEmpty Queue  Implementation of the phantom queue.218  CONGESTION CONTROL IN DATA CENTER NETWORKS TCP’s additive increase/multiplicative decrease (AIMD) scheme.
In fact, just like DCTCP, it is able to make use of existing ECN-enabled Ethernet switches.
The basic idea behind D2TCP is to modulate the congestion window size based on both deadline information and the extent of congestion.
The algorithm works as follows: 
As in DCTCP, each switch marks the CE bit in a packet if its queue size exceeds a threshold K. This information is fed back to the source by the receiver though ACK packets.
 Also as in DCTCP, each sender maintains a weighted average of the extent of congestion α, given by α’ð12gÞα1gF (20) where F is the fraction of marked packets in the most recent window and g is the weight given to new samples.
 DCTCP introduces a new variable that is computed at the sender, called the deadline imminence factor d, which is a function of a flows deadline value, and such that the resulting congestion behavior allows the flow to safely complete within its deadline.
Define T cas the time needed for a flow to complete transmitting all its data under a deadline agnostic behavior, and δas the time remaining until its deadline expires.
If T c.δ, then the flow should be given higher priority in the network because it has a tight deadline and vice versa.
Accordingly, the factor d is defined as d5Tc δ(21) Note that δis known at the source.
To compute T c, consider the following: Let X be the amount of data (in packets) that remain to be transmitted and let W mbe the current maximum window size.
CONTROL ALGORITHMS  Based on αand d, the sender computes a penalty function p applied to the window size, given by p5αd(23)
At the other extreme, when α51, then p 51, and the window size is halved just as in regular TCP or DCTCP.
For 0,α,1, the algorithm behaves differently compared with DCTCP, and depending on the value of d, the window size gets modulated as a function of the deadlines.
Note than when d 51, then p5α, so that the system matches DCTCP.
If d .1, then from  , it follows that the time required to transmit the remaining data is larger than allowed by the deadline; hence, the flow should be given higher priority by the network.
brings this about by reducing the value of p for such a flow, resulting in a larger window size by  .
Conversely, if d ,1, leads to a smaller window size and hence lower priority for the flow.
Hence, the net effect of these window change rules is that far-deadline flows relinquish bandwidth so that near-deadline flows can have greater short-term share to meet their deadlines.
If the network congestion keeps increasing despite the far-deadline flows backing off, then  shows that the curves for d ,1 converges toward that for d .1, which implies that in this situation even the near-deadline flows reduce their aggressively.
[3]simulated D2TCP and showed that the algorithm reduces the fraction of missed deadlines by 75% when compared to DCTCP and 50% compared with D3.
It is also able to coexist with TCP flows without degrading their performance.
1.0 p α1.0d < 1 d = 1 d > 1  Deadline-Aware Data Center TCP (D2TCP) correction function.220  CONGESTION CONTROL IN DATA CENTER NETWORKS 7.4.2 THE D3ALGORITHM Both the DCTCP and D2TCP algorithms are designed to make use of regular Ethernet switches, albeit those that support RED.
In this section, we describe the D3algorithm that requires signifi- cantly enhanced switch functionalities
[5].D3was the first algorithm that attempted to directly adapt the classical EDF scheduling algorithm to DCNs, and later another algorithm called PDQ [6] built upon it to significantly and improved its performance by adding preemptive scheduling of flows.
Indeed, both of these algorithms take us out of the province of AIMD-type window increment /C0decrement
algorithms that have dominated congestion control designs since the 1980s.
Note that both of these algorithms are based on the use of First In First Out (FIFO) scheduling at the network nodes.
These algorithms are able to simulate the effect of node level EDF scheduling by controlling the rate (in D3) or the admission of packets into the network (in PDQ).
A traditional EDF scheduler operates as follows: All packets arriving into the queue carry an extra piece of information in their headers, which is the de adline by which they have to be finish transmission.
The EDF scheduler prioritizes transmissions, such th at packets with the smallest or earliest deadline are transmitted first.
EDF scheduling can be done eith er on a nonpreemptive or preemptive basis.
In non- preemptive scheduling, after a transmission is starte
d, it finishes to completion; in preemptive scheduling, a packet with a smaller deadline can preempt the transmission of a packet with a longer deadline.
A preemptive EDF scheduler can be proven to be optimal [17] in the sense that if the link utilization is under 100%, then all packets under EDF scheduling will meet their deadlines.
A straightforward application of EDF to a DCN runs into the following problems: (1) It is difficult to do EDF on a packet-by-packet basis at switches because packets do not carry any infor- mation about their deadlines, and (2) DCN switches do not incorporate priority-based transmission such that packets with closer deadlines could be transmitted earlier.
The D3algorithm solves the first problem by assigning deadlines on a flow basis rather than on a packet basis.
Furthermore, it solves the second problem by translating the flow deadline requirement into an equivalent bandwidth requirement, and then reserving this bandwidth, on a per round trip basis at all switches along the path.
PDQ, on the other hand, attaches extra fields to the packet header with information about flow deadlines and size among other quantities and lets the switches make priority decisions regarding the flows, which are communicated back to the sources.
We now provide a brief description of the D3algorithm: 
Applications communicate their size and deadline information when initiating a flow.
The sending server uses this information to request a desired rate r given by r 5s/d, where s is the flow size and d is the deadline.
Note that both s and d change as a flow progresses, and these are update by the sender. 
The rate request r is carried in the packet header as it traverses the switches on its path to the receiver.
Each switch assigns an allocated rate that is fed back in the form of a vector of bandwidth grants to the sender though an ACK packet on the reverse path.
Note that a switch allocates rates on an First Come First Served (FCFS) basis, so that if it has spare capacity after satisfying rate requests for all the deadline flows, then it distributes it fairly to all current flows.
Hence, each deadline flow gets allocated a rate a, where a 5r1fsand f sis the fair share of the spare capacity (Note that f scan be computed using the techniques described for the RCP algorithm in .)221 7.4DEADLINE-AWARE CONGESTION CONTROL ALGORITHMS 
On receiving the ACK, the source sets the sending rate to the minimum of the allocated rates.
It uses this rate for a RTT (rather than using an AIMD-type algorithm to determine the rate) while piggybacking a rate request for the next RTT on one of the data packets.
Note that D3does not require a reservation for a specific sending rate for the duration of the entire flow, so the switches do not have to keep state of their rate allocation decisions.
Also, D3does not require priority scheduling in the switches and can work with FIFO scheduled queues.
The D3protocol suffers from the following issues: 
Because switches allocate bandwidth on a FCFS basis, it may result in bandwidth going to flows whose deadlines are further away but whose packets arrive at the switch earlier.
It has been shown [3]that this inverts the priorities of 24% to 33% of requests, thus contributing to missed deadlines.
D3requires custom switching sets to handle requests at line rates, which precludes the use of commodity Ethernet switches.
It is not clear whether the D3rate allocation strategy can coexist with legacy TCP because TCP flows do not recognize D3’s honor based bandwidth allocation.
This means that a TCP flow can grab a lot of bandwidth so that the switch will not be able to meet the rate allocation promise it made to a D3source.
The PDQ [6]scheduler was designed to improve upon D3by providing a distributed flow sched- uling layer, which allows for flow preemption, while using only FIFO tail-drop queues.
This, in addition to other enhancements, improves the performance considerably, and it was shown that PDQ can reduce the average flow completion time compared with TCP and D3by 30% and can support three times as many concurrent senders as D3while meeting flow deadlines.
PDQ is able to implement sophisticated strategies such as EDF or SJF (Shortest Job First) in a distributed manner, using only FIFO drop-tail queues, by explicitly controlling the flow sending rate and retaining packets from low-priority flows at senders.
7.5LOAD BALANCING OVER MULTIPLE PATHS WITH MULTIPATH TCP (MPTCP) One of the features that differentiates a DCN from other types of networks is the presence of multiple paths between end points.
As explained in  , these multiple paths are integral to realizing the full bisection bandwidth between servers in DCN architectures such as a Clos-based Fat Tree or VL2 networks.
To accomplish this, the system should be able to spread its traffic among the multiple paths while at the same time providing congestion control along each of the paths.
The traditional way of doing load balancing is to do it on a flow basis, with each flow mapped randomly to one of the available paths.
This is usu ally done with the help of r outing protocols, such as ECMP, that use a hash of the address, port number and other fields to do the randomization.
However, randomized load balancing cannot achieve the full bis ectional bandwidth in most topolo- gies because often a random selection causes a few links to be overloaded and other links to have little load.
To address this issue, researchers hav e proposed the use of centralized schedulers such as Hedera [18].
This algorithm detects large flows, which are then assigned to lightly loaded paths,222  CONGESTION CONTROL IN DATA CENTER NETWORKS while existing flows may be reassigned to maximize overall throughput.
i t ht h e flow arrivals.
It has been shown by Raiciu et al.
[9]that even if a scheduler runs every 500 ms, its performance is no better than that of a randomized load-balancing system.
To address these shortcomings in existing load balancers, Raiciu et al.
[10] designed a combined load-balancing plus congestion control algorithm called Multipath TCP (MPTCP), which is able to establish multiple subflows on different paths between the end systems for a single TCP connection.
It uses a simple yet effective mechanism to link the congestion control dynamics on the multiple subflows, which results in the movement of traffic away from more con- gested paths and on to less congested ones.
It works in conjunction with ECMP, such that if ECMP’s random selection causes congestion along certain links, then the MPTCP algorithm takes effect and balances out the traffic.
The MPTCP algorithm operates as follows:  MPTCP support is negotiated during the initial SYN exchange when clients learn about additional IP addresses that the server may have.
Additional subflows can then be opened with IP addresses or in their absence by using different ports on a single pair of IP addresses.
MPTCP then relies on ECMP routing to hash the subflows to different paths.
 After the multiple subflows have been established, the sender’s TCP stack stripes data across all the subflows.
The MPTCP running at the receiver reconstructs the receive data in the original order.
Note that there is no requirement for an application to be aware that MPTCP is being used in place of TCP. 
Each MPTCP subflow have its own sequence space and maintains its own congestion window so that it can adapt independently to conditions along the path.
Define the following: Wr: Window size for the rthsubflow WT: Sum of the windows on all the subflows Tr: Round trip delay for the rthsubflow The following window increment /C0decrement
rules are used by MPTCP:
On detecting a dropped packet on subflow r, decrease its window according to Wr’Wr 2(27)
Hence, MPTCP links the behavior of the subflows by adapting the additive increase constant.
These rules enable MPTCP to automatically move traffic from more congested paths and place it223 7.5LOAD BALANCING OVER MULTIPLE PATHS WITH MULTIPATH TCP on less congested ones.
[10] used heuristic arguments to justify these rules, which we explain next:
 Note that if the TCP window is increased by a/W per ack, then the flow gets a window size that is proportional toﬃﬃﬃﬃαp(see  in  of ).
Hence, the most straightforward way to split up the flows is by choosing α51=n2and assuming equal round trip delays, resulting in an equilibrium window size of W/n for each subflow.
However, this algorithm can result in suboptimal allocations of the subflows through the network because it uses static splitting of the source traffic instead of adaptively trying to shift traffic to routes that are less congested.
Adaptive shifting of traffic to less congested routes can be done by the using the following rules: Wr’Wr11 WTon every ACK ;and (28)
Wr’WT 2on detecting packet loss (29) Consider the case when the packet drop rates are not equal.
As a result of equations 28 and 29 , the window increment and decrement amounts are the same for all paths; hence, it follows that the paths with higher drop rate will see more window decreases, and in equilibrium, the window size on these paths will go to zero.
Assuming that each of the paths have the same packet loss rate p and using the argument that in equilibrium, the increases and decreases of the window size must balance out, it follows that Wr Trð12pÞ/C22/C231 WT5Wr Trp/C22/C23WT 2(30) so that WT5ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2ð12pÞ ps /C25ﬃﬃﬃ 2 ps (31) It follows that the total window size W T, is the same as for the case when all the traffic was car- ried on a single path.
Hence, unlike the case of static splitting, the adaptive splitting rule does not use more bandwidth by virtue of the fact that it is using multiple paths.
The window increment /C0decrement rules ( equations 28 and 29 ) lead to the situation where there is no traffic directed to links with higher packet drop rates.
If there is no traffic going to a path for a subflow, then this can be a problem because if the drop rate for that link decreases, then there is no way for the subflow to get restarted on that path.
To avoid this, it is advisable to have traffic flow even on links with higher loss rates, and this can be accomplished by using the following rules: Wr’Wr1a WTon every ACK and (32)
Wr’Wr 2on detecting packet loss (33)224  CONGESTION CONTROL IN DATA CENTER NETWORKS Note that  causes a decrease in the rate at which the window decreases for higher loss rate links, and the factor a in  increases the rate of increase.
We now derive an expression for W rfor the case when the packet drop rates are different but the round trip latencies are same across all flows.
Wm r: Maximum window size for the rthsubflow τr: Length of a window increase /C0decrease cycle for the rthsubflow Mr: Multiple of RTTs contained in an increase /C0decrease cycle Nr: Number of packets transmitted an increase /C0decrease cycle for the rthsubflow pr: Packet drop rate for the rthsubflow T: The common round trip latency for all subflows We will use the fluid flow model of the system (see  ).
Because the window size increases by aW r/WTfor every RTT and the total increase in window size during a cycle is W r/2, it follows that Mr5Wr=2 aWr=WT5WT 2a so that τr5MrT5WTT 2a Also Nr51 Tðτr 0WðtÞdt51 TτrWm r 21τrWm r 4/C20/C21 53WTWm r 8a(34) Using the deterministic approximation technique, it follows that 3WTWm r 8a51 pr(35)
Hence, unlike the previous iteration of the algorithm, this algorithm allocates a nonzero window size to flows with larger packet drop rates.
The window increment /C0decrement rules ( equations 32 and 33 ) are effective in guiding traffic toward links with lower loss rates; however, they do not work very well if the paths have differing round trip delays.
This is because the path with the higher round trip delay will experience a lower rate of increase of window size, resulting in a lower throughput even if the225 7.5LOAD BALANCING OVER MULTIPLE PATHS WITH MULTIPATH TCP packet loss rates are the same.
They also changed the window increment /C0decrement rules to the following: Wr’Wr1mina WT;1 Wr/C18/C19 on each ACK (39)
The difference between this algorithm and the previous one is that window increase is capped at 1/W r, which means that the multipath flows can take no more capacity on any path than a single-path TCP flow would.
The expression for a in  can be derived by combining equations 39 and40with the condition (  ), and we proceed to do this next.
Using the same notation as before, note that the window size for the rthsubflow increases by minðaWr WT;1Þin each round trip.
[9]have carried out extensive simulations of MPTCP and have shown that it leads to appreciable improvement in DCN performance.
For example, for an eight-path Fat Tree CLOS network, regular TCP with an ECMP-type load balancing scheme only realized 50% of the total bandwidth between two servers, but MPTCP with eight subflows realized about 90% of the total bandwidth.
Moreover, the use of MPTCP can lead to new interconnection topologies that are not feasible under regular TCP.
[9]suggested a modified form of Fat Tree, which they call Dual-Homed Fat Tree (DHFT), which requires a server to have two interfaces to the ToR switch.
They showed that DHFT has significant benefits over the regular Fat Tree topology.
7.6THE INCAST PROBLEM IN DATA CENTER NETWORKS
The Incast problem in DCNs is caused by the temporal dependency created on message traffic that is generated as a result of the Map-Reduce type parallel execution illustrated in  .
The par- ent node sends queries in parallel to multiple leaf nodes, and the subsequent responses from the leaf nodes also tend to be bunched together, as shown in  .
The response size not very large on the average, usually a few kilobytes, but even then it has been observed that they suffer from a large packet loss rate.
The reason for this has to do with the hardware design of the DCN switches, which tend to have shallow buffers, which are shared among all the ports in switch.
To solve the Incast problem, researchers do one of the following: (1) try to avoid packet loss or reduce the rate of packet loss or (2) quickly recover from lost packets.
In the first category are techniques such as increasing the switch buffer size and adding a ran- dom delay to query responses from the leaf nodes.
The latter technique reduces the occurrence of Incast-generated timeouts but at the cost of an increase in the median Job response time.
The use of a DCN-specific congestion control protocol such as DCTCP also falls in this category because227 7.6THE INCAST PROBLEM IN DATA CENTER NETWORKS they are mode effective in keeping buffer occupancy low compared with regular TCP.
In the sec- ond category are techniques such as reducing the TCP packet lost timer RTO from the default value of a few 100 ms to less than 1 ms.
This has been very effective in improving Incast performance.
7.7FURTHER READING Congestion control in DCNs is an extremely active area, with researchers exploring new ideas with- out being constrained by legacy compatibility issues.
One such idea is that of in-network conges- tion control in which the switches bear the burden of managing traffic.
The justification for this is that traditional TCP congestion control requires at least one RTT to react to congestion that is too slow for DCNs because congestion builds very quickly, in less than RTT seconds.
The in-network schemes go hand in hand with the multipath nature of modern datacenter architecture, and a good example of this category is the Detail protocol [7].
Routing within the network is done on a packet- by-packet basis and is combined with load balancing, such that a packet from an input queue in a switch is forwarded to the output interface with the smallest buffer occupancy (thus taking advan- tage of the fact that there are multiple paths between the source and destination).
This means that packets arrive at the destination out of order and have to be resequenced before being delivered to the application.
This mechanism is called Adaptive Load Balancing (ALB).
Furthermore, if an output queue in a switch is congested, then this may result in input queues becoming full as well, at which point the switch backpressures the upstream switch by using the IEEE 802.1Qbb Priority based Flow Control (PFC) mechanism.
From ParentTo Leaf 1 To Leaf 2 To Leaf 3 From Leaf 1 From Leaf 2 From Leaf 3 To ParentX Re-Tx from From Leaf 2Timeout  Illustration of the Incast problem.228  CONGESTION CONTROL IN DATA CENTER NETWORKS The pFabric
[8]is another protocol with highly simplified end system algorithm and with the switches using priority-based scheduling to reduce latency for selected flows.
Unlike in D2TCP, D3 or PDQ, pFabric decouples flow priority scheduling from rate control.
Each packet carries its prior- ity in its header, which is set by the source based on information such the deadline for the flow.
Switches implement priority-based scheduling by choosing the packet with the high priority number for transmission.
If a packet arrives to a full buffer, then to accommodate it, the switch drops a packet whose priority is lower.
pFabric uses a simplified form of TCP at the source without conges- tion avoidance, fast retransmits, dupACKs, and so on.
All transmissions are done in the Slow Start mode, and congestion is detected by the occurrence of excessive packet drops, in which case the system enters into probe mode in which it transmits minimum size packets.
Hence, pFabric does away with the need to do adaptive rate (or window) control at the source.
Computing services such as Amazon EC2 are a fast-growing category of DCN applications.
These systems use Virtual Machines (VMs), which share the servers by using a Processor Sharing (PS) type scheduling discipline to make more efficient use of the available computing resources.
Wang and Ng [21] studied the impact of VMs on TCP performance by using network measurements and obtained some interesting results: They found out that PS causes a very unstable TCP throughput, which can fluctuate between 1 Gbps and 0 even at about tens of milliseconds in granularity.
Furthermore, even in a lightly congested network, they observed abnormally large packet delay var- iations, which were much larger than the delay variations caused by network congestion.
They attrib- uted these problems to the implementation of the PS scheduler that is used for scheduling VMs.
They concluded that TCP models for applications running on VMs should incorporate an additional delay in their round trip latency formula (i.e., the delay caused by end host virtualization).
The datacenter as a computer: an introduction to the design of warehouse scale machines.
Data Center TCP (DCTCP).
Deadline aware Datacenter TCP (D2TCP).
Less is more: trading a little bandwidth for ultra-low latency in the data center.
Better never than late: meeting deadlines in datacenter networks.
[6] Hong C, Caesar M, Godfrey P. Finishing flows quickly with preemptive scheduling.
[7] Zats D, Das T, Mohan P, Katz R. DeTail: reducing the flow completion time tail in datacenter networks.
pFabric: minimal near optimal datacenter transport.
Design, implementation and evaluation of congestion control for multipath TCP. NSDI 2011;11:8.
[12] Greenberg A, Hamilton JR, Jain N, et al. VL2: a scalable and flexible data center network.
[13] Clos C. A study of non-blocking switching networks.
[14] Kodialam M, Lakshman TV, Sengupta S. Efficient and robust routing of highly variable traffic.
[15] Al-Fares, Loukissas A, Vahdat A. A scalable, commodity data center network architecture.
Analysis of DCTCP: stability, convergence and fairness.
Scheduling algorithms for multiprogramming in a hard real time environment.
Hedera: dynamic flow scheduling for data center networks.
Understanding TCP Incast through collapse in datacenter networks.
A survey on TCP Incast in data center networks.
The impact of virtualization on network performance of Amazon EC2 data center.
distributed congestion aware load balancing for datacenters.
Benson T, Akella A, Maltz D. Network traffic characteristics of data centers in the wild.
Towards minimal delay deadline driven datacenter TCP.
Multipath congestion control for shared bottlenecks.
Kandula S, Sengupta S, Greenberg A, et al.
The nature of datacenter traffic: measurements and analysis.
Liu S, Xu H, Cai Z. Low latency datacenter networking: a short survey.
Perry J, Ousterhout A, Balakrishnan H, et al.
Fastpass: a centralized “zero-queue” datacenter network.
ACM SIGCOMM 2014.230  CONGESTION CONTROL IN DATA CENTER NETWORKS CHAPTER 8CONGESTION CONTROL IN ETHERNET NETWORKS 8.1INTRODUCTION Ethernet is the most prevalent Layer 2 technology used in Internet Protocol (IP) networks; it is almost as old as IP itself.
Traditionally, Ethernet left the job of congestion control to IP while providing basic medium access control (its original purpose in shared media links) and later Layer 2 bridging and switching functionalities.
This state of affairs started to change in the past 10 to 15 years as the use of Ethernet expanded to wider domains.
One of these domains is data center networking, and  discusses server-to-server networking in DCNs using Ethernet as the switching fabric with TCP running on top.
An aspect of data center networks (DCNs) not discussed in  is that of communications between the servers and storage devices.
The networks that are used to interconnect them together are known as Storage Area Networks (SANs).
Fiber Channel (FC) is a Layer 1 /C02 high-speed point-to-point interconnect technology that is used to create SANs.
A very strong requirement in SAN networks is that the packet drop rate caused by buffer overflows should be zero.
FC networks use a hop-by-hop congestion control method to accomplish this goal, whereby a node that is run- ning out of buffers sends a signal to the node upstream from it to stop it from transmitting.
Link speeds in FC increased over time, but they were not able to keep pace with the more rapid increases in Ethernet link speeds that have gone from 1 to 10 Gbps and then to 100 Gbps in the past decade and a half.
In addition, Ethernet adaptors and switches come at a lower cost because of their wider adoption in the industry.
Motivated by these considerations, the industry created a FC over Ethernet (FCoE) standard, in which Ethernet is used as the Layer 1 /C02 substrate in SANs.
However, using TCP as the congestion control mechanism will not work in an FCoE SAN because TCP does drop packets during the course of its normal operation.
One of the outcomes of this effort is a congestion control protocol called IEEE 802.1Qau
[2], also known as Quantum Congestion Notification (QCN), which operates at the Ethernet layer and aims to satisfy the special requirements in DCB networks
The QCN increase /C0decrease protocol has some novel features described in  .
[4] 231Internet Congestion Control.
carried out a comprehensive control theoretic analysis of the fluid model of the QCN algorithm that forms the subject matter of  . 8.2DIFFERENCES BETWEEN SWITCHED ETHERNET AND IP NETWORKS Alizadeh et al. listed the following contrasts between switched Ethernet networks and IP networks
We already mentioned one of them in the Introduction (i.e., packets should not be dropped); the others are: 
 There are no Packet Sequence Numbers in Ethernet.
Sources start at Line Rate because there is no equivalent of the TCP Slow-Start mechanism.
Because Ethernet implements its congestion control in hardware, a Slow-Start /C0like algorithm would have required a rate limiter, which are few in number and used up by the main congestion control algorithm.
The next three differences are more specific to DCNs in general and served as constraints in the design of a transport level congestion control algorithm as well:  Very shallow buffers: Ethernet switch buffers are of the order of 100s of kilobytes in size.
 Small number of simultaneously active connections.
Multipathing: Traditional Layer 2 networks used Spanning Trees for routing, which restricted the number of paths to one.
The use of Equal Cost Multi-Path (ECMP) opens up the system to using more than 1 path. 8.3OBJECTIVES OF THE QUANTUM
CONGESTION NOTIFICATION ALGORITHM The QCN algorithm was designed based on all the learning from the design and modeling of TCP congestion control algorithms over the previous 2 decades.
As a result, it incorporates features that keep it from falling into performance and stability traps that TCP often runs into.
Some of the objectives that it seeks to satisfy include the following: 
Stability: We will use this term in the same sense as in the rest of this book, that is, the bottleneck queue length in a stable system should not fluctuate widely, thus causing overflows and underflows.
Whereas the former leads to excessive packet drops, the latter causes link underutilization.
Responsiveness to link bandwidth fluctuations  Fairness 
Implementation simplicity: Because the algorithm is implemented entirely in hardware, simplicity is a must.232  CONGESTION CONTROL IN ETHERNET NETWORKS 8.4QUANTUM CONGESTION NOTIFICATION ALGORITHM DESCRIPTION
The algorithm is composed of two main parts: Switch or Control Point (CP) Dynamics and Rate Limiter or Reaction Point (RP) Dynamics.
THE CONTROL POINT OR CP ALGORITHM The CP algorithm runs at the network nodes, and its objective is to maintain the node’s buffer occupancy at the operating point b eq( ).
It computes a congestion measure F band randomly samples an incoming packet with a probability that that depends on the severity of the congestion (  ).
It sends the value of F bback to the source of the sampled packet after quantizing it to 6 bits.
Define the following: b: Value of the current queue length bold:
Value of the buffer occupancy when the last feedback message was generated boff:5b2beq bd:5b2bold Then F bis given by the formula: Fb5boff1wbd (1) where w is a non-negative constant, set equal to 2 for the baseline implementation.
Note that equa- tion 1 is basically the PI Active Queue Management AQM controller from .
The first term on the RHS is the offset from the target operating point (i.e., the buffer oversubscription), and the second term is proportional to the rate at which the queue size is changing.
As per  in , this is proportional to the difference between the link capacity and the total traffic rate flowing through the link (i.e., the link oversubscription).
When Fb,0, there is no congestion, and no feedback messages are sent.
When Fb$0, then either the buffers or the link or both are oversubscribed, and control action needs to be taken.
An incoming packet is sampled with probability ps5φðFbÞ(see  ), and if p s51 and Fb$0, then a congestion feedback message is sent back to the source.beq b Sample Incoming Packets  Congestion detection at the Control Point (CP).233 8.4QUANTUM CONGESTION NOTIFICATION ALGORITHM DESCRIPTION 8.4.2
THE REACTION POINT OR RP ALGORITHM
The RP algorithm runs at the end systems and controls the rate at which Ethernet packets are transmitted in to the network.
Unlike TCP, the RP algorithm does not get positive ACKs from the network and hence needs alternative mechanisms for increasing its sending rate.
Define the following: Current rate (R C): The transmission rate of the source Target rate (R T): The transmission rate of the source just before the arrival of the last feedback message Byte counter: A counter at the RP for counting transmitted bytes; it is used to time rate increases Timer: A clock at the RP that is used for timing rate increases.
It allows the source to rapidly increase its sending rate from a low value when a lot of bandwidth becomes available.
Initially assuming only the Byte Counter is available, RP uses the following rules for increasing and decreasing its rate.
8.4.2.1 Rate Decreases A rate decrease is only done when a feedback message is received, and in this case, CR and TR are updates as follows: RT’RC (2) RC’RCð12GdjFbjÞ (3)
Because only 6 bits are available for feedback, it follows that Fbmax564, so that Gd51=128 accomplishes this objective.
FR consists of 5 cycles, in each of which 150 Kbytes of data are transmitted (100 packets of 1500 bytes each), as counted by the Byte Counter.
At the end of each cycle, R Tremains unchanged, and R Cis updated as follows:
The rationale behind this rule is if the source is able to transmit 100 packets without receiving another Rate Decrease message (which are sent by the CP once every 100 packets on the average since p s50.01), then it can conclude that the CP is uncongested, and therefore it increases its rate.
Note that FR is similar to the way in which a Binary Increase Congestion Control (BIC) TCP source increases its window size after a packet drop (see ).
This mechanism was discov- ered independently by Alizadeh et al.
Active Increase (AI): After 5 cycles of FR, the source enters the AI state, where it probes for extra bandwidth.
AI consists of multiple cycles of 50 packets each.
When R Cis extremely small after a rate decrease, then the time required to send out 150 Kbyes can be excessive.
To speed this up, the source also uses a Timer, which is used as follows: The Timer is reset when the rate decrease message arrives.
The source then enters FR and counts out 5 cycles of T ms dura- tion (T510 ms in the baseline implementation), and in the AI state, each cycle is T/2 ms long.
In the AI state, the R Cis updated when either the Bye Counter or the Timer completes a cycle.
The source is in the AI state if and only if either the Byte Counter or the Timer is in the AI state.
In this case, when either completes a cycle, R Tand R Care updated according to equations 5 and 6 .Fast
Recovery Active Increase Target RateRT RC Rd Rd/2Rd/4Rd/8RAI tRate Congestion Message Received  Quantum Congestion Notification (QCN)
Control Point (CP) operation.235 8.4QUANTUM CONGESTION NOTIFICATION ALGORITHM DESCRIPTION 
The source is in the Hyper-Active Increase (HAI) state if both the Bye Counter and the Timer are in AI.
In this case, at the completion of the ithByte Counter or Timer cycle, R Tand R Care updated as follows: RT’RT1iRHAI (7) RC’RC1RT 2(8) where R HAIis set to 50 mbps in the baseline.
8.5QUANTUM CONGESTION NOTIFICATION STABILITY ANALYSIS The stability analysis is done on a simplified model of the type shown in , , with N connections with the same round trip latency, passing through a single bottleneck node with capacity C [4].
Following the usual recipe, we will first write down the differential equations in the fluid limit and then linearize them around an operating point, which allows us to analyze their sta- bility using tools such as the Nyquist stability criterion.
We will assume that p sis fixed at p s51% in this section to simplify the analysis.
Also, all connections are assumed to have the same round trip latency equal to τseconds.
FbðtÞ5bðtÞ2beq1w CpsðNRCðtÞ2CÞ (12) prðtÞ5ps1½FbðtÞ.0/C138 (13) To justify the negative first terms in equations 9 and 10 , note that RCðt2τÞprðt2τÞis the rate at which negative ACKs are arriving at the source.
Each of these causes R Cto decrease by RCðtÞGdFbðt2τÞand RT to decrease by RTðtÞ2RCðtÞ. To derive the positive second term in  , consider the following: The rate R Cis increased on the transmission of 150 Kbytes, or 100 packets of 1500 bytes each, if no negative ACK is received in the interim.
The change in R Cwhen this happens is given by ΔRCðtÞ5RCðtÞ1RTðtÞ 22RCðtÞ 5RTðtÞ2RCðtÞ 2236  CONGESTION CONTROL IN ETHERNET NETWORKS To compute the rate at which the R Crate increase events occur, consider the Markov chain in  : It is in state k if k packets have been transmitted back to back without the receipt of a single negative ACK.
Starting from state 0, in general, the system may undergo several cycles where it returns back to state 0 before it finally transmits 100 packets back to back and gets to state 100.
Because the average time between packet transmissions is given by1 RCðt2τÞ, it follows that the average time between increase events is given by ΔT5ð12prðt2τÞÞ210021 RCðt2τÞprðt2τÞ(15)
The second term on the RHS of  follows by dividing  by .
The second term on the RHS of  is derived using similar considerations.
In this case, R T increments by R AIwhen 500 packets are transmitted back to back without returning to state 0, which results in a Markov chain just like that in  , except that it extends up to state 500.
We now compute the equilibrium values of the variables in equations 9 to 13 .
In equilibrium, we replace p rby p sthroughout.
CONGESTION NOTIFICATION STABILITY ANALYSIS The fluid flow model in equations 9 to 13 has the following equilibrium points: R/C3 C5C N(16)
Hence, it follows that jGðjω/C3Þj,1.
Because jGðjωÞjis a monotonically decreasing function of ω, it follows that the critical frequency ωcat which jGðjωcÞj51, is such that ωc,ω/C3.
By the Nyquist criterion, if we can show that arg ðGðjωÞÞ,πfor all 0 #ω,ω/C3, then the system is stable.
The last inequality follows from the fact that ω#ω/C3.
Moreover ΨðωÞis convex for 0 #ω#ω/C3, which implies that ΨðωÞ#πforωA½0;ω/C3/C138, and from  , it follows that arg ðGðjωÞÞ,πin this range, so that the Nyquist stability criterion is satisfied.
This is in contrast to TCP Reno (see , ), whose loop gain (without RED) is of the order KRENOBOC3τ3 N2/C18/C19 The absence of the round trip latency from the loop gain in  is attributable to the fact that QCN is not a window-based congestion control algorithm.
In both cases, an increase in link capacity C or a decrease in number of connections drives the system toward instability.
Because τ,,1, the window based feedback loop plays a role in stabilizing Reno compared with QCN by reducing the system loop gain.
ω/C3 β5Gdw ps1ηðpsÞ 21ηðpsÞςðpsÞNRAI 2Cps/C20/C21ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:251Nps Gdw2/C18/C192svuut(33) The last two terms in the denominator of  are much smaller than the first term.
As a result, it follows thatω/C3 g/C25ω/C3 β, so that the stability threshold for latency is given by τ/C3/C25tan21Gdw2
Npsﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:251Nps Gdw2/C18/C192svuut6666477775
GdwC Nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:251Nps Gdw2/C18/C192svuut(34)
Substituting G d51/128, w 52 and p s50.01, we obtain ω/C35C 64Nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:2510:1024 N2pq (35) and τ/C3/C2564N Cﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:2510:1024 N2p p tan213:125 Nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:2510:1024 N2pq /C22/C23 (36)
Hence, the stability threshold for the round trip latency is inversely proportional to C and directly proportional toﬃﬃﬃﬃ
The algorithm by Jiang et al.
[5]uses an AQM scheme with explicit rate calculation at the network nodes that is fed back to the source.
This scheme has some similarities to the RCP algorithm from .
The algorithm by Bergamasco and Pan
[6]has some similari- ties to QCN because it is also based on an AQM scheme that provides PI feedback back to the source using a quantized congestion number F b.
The source nodes then use this number to adjust the parameters of their additive increase/multiplicative decrease (AIMD) scheme, such that the rate is additively increased if F b.0 and multiplicatively decreased if F b,0.241 8.6FURTHER READING
Data Center Bridging Whitepaper. 2010.
Virtual bridges local area networks—Amendment: Congestion Notification.
Data center transport mechanisms: congestion control theory and IEEE standardization.
[4] Alizadeh M, Kabbani A, Atikoglu B, Prabhakar B. Stability analysis of QCN: the averaging principle.
An explicit rate control framework for lossless Ethernet operation.
[6] Bargamasco D, Pan R. Backward congestion notification, Version 2.0.
IEEE 802.1 Meeting, 2005.242  CONGESTION CONTROL IN ETHERNET NETWORKS CHAPTER 9EMERGING TOPICS IN CONGESTION CONTROL 9.1INTRODUCTION
In this chapter, our objective is to provide a brief survey of some emerging topics in congestion control.
There has been renewed interest in the topic of congestion control in recent years, driven mostly by applications to data center networks and video streaming.
Meanwhile, broader trends in networking, such as the emergence of Software Defined Networks (SDNs), are beginning to have their influence felt on this topic as well.
Other advances in technology, such as the progress that has been made in artificial intelligence (AI) and Machine Learning (ML) algorithms, are also having an impact on our subject.
In , we describe Project Remy from Massachusetts Institute of Technology (MIT), which uses ML to automatically generate congestion control algorithms; these have been shown to perform better than their human-designed counterparts.
These algorithms are based on the theory developed in  in which congestion control rules are derived as a solution to a distributed optimization problem.
Unlike the window increase /C0decrease rules that we have encountered so far, these rules number in the hundreds and are a function of the specific point in the feedback space.
The framework of Project Remy also helps us to explore the delay versus throughput tradeoff in congestion control, as well as provide bounds on how well the best performing algorithm can do.
In , we give a brief description of the field of SDNs.
They have been used to facili- tate routing and switching level functionality, but they may have a significant impact on congestion control as well.
We describe a couple of topics in congestion control that can benefit from the cen- tralized control point that SDNs provide.
The first topic is on the use of SDNs to choose an appro- priate Active Queue Management (AQM) algorithm at a switch as a function of the application running on top of the connection.
For example, drop-tail queues are good enough for applications that prioritize throughput over latency, but more sophisticated AQM is needed for applications for which low latency is important.
The second topic is on the use of SDNs to set appropriate para- meters for the AQM algorithm.
There are some interesting new ideas in GCC, such as the active participation of the receiver in the congestion control algorithm, which are sure to influ- ence the future direction of this subject.
The GCC algorithm does not run on top of TCP but instead uses the Real Time Transport protocol (RTP) for sending data and its accompanying Real Time Control Protocol (RTCP) for obtaining feedback from the receiver.
The receiver analyzes the pattern 243Internet Congestion Control.
All rights reserved. of packet inter-arrival times and then uses a Kalman filter to estimate whether the bottleneck queue is changing in size.
It chooses a rate based on this information and feeds this information back to the source, which bases the sending rate on this and some other pieces of information.
9.2MACHINE LEARNING AND CONGESTION CONTROL:
PROJECT REMY In this section, we describe the work carried out by Winstein et al. at MIT in which they did a fun- damental rethink on how to solve the congestion control problem, based on which they came up with an algorithm called Remy that is radically different from all existing work in this area
This algorithm is based on applying ideas from partially observable Markov decision processes (POMDPs) to the congestion control problem.
The basic idea behind the derivation of this algorithm is something that we discovered in , that is, the congestion control problem can be regarded as the solution to the distributed optimization of a network wide utility function (see  and Appendix 3.E in ).
By using POMDP/ML techniques, the algorithm learns the best possible congestion control action, in the sense of minimizing this utility function, based on its observations of the partial network state at each source.
The system uses a simple ON-OFF model for the traffic generation process at each source.
The sender is OFF for a duration that is drawn from an exponential distribution.
Then it switches to the ON state, where it generates a certain number of bytes that are drawn from an empirical distribu- tion of flow sizes or a closed form distribution.
The important ingredients that go into the algorithm include: 
The form of the utility function that is being minimized 
The observations of the network state at each source 
Note that the parameters αandβexpress the fairness vs efficiency tradeoffs for the throughput and delay, respectively (see Appendix 3.E in ), and δexpresses the relative importance of delay versus throughput.
By varying the parameter δ, enables us to explicitly control the trade-off between throughput and delay for the algorithm.
The reader may recall from the section on Controlled Delay (CoDel) in  that legacy protocols such as Reno or Cubic try to maximize the throughput without244  EMERGING TOPICS IN CONGESTION CONTROL worrying about the end-to-end latency.
To realize the throughput /C0delay tradeoff, legacy protocols have to use an AQM such as Random Early Detection (RED) or CoDel, which sacrifice some the throughput to achieve a lower delay.
Remy, on the other hand, with the help of the utility function in  , is able to attain this trade-off without making use of an in-network AQM scheme.
2.Network state observations: Remy tracks the following features of the network history, which are updated every time it receives an ACK: a.ack_ewma: This is an exponentially weighted moving average of the inter-arrival time between new ACKs received, where each new sample contributes 1/8 of the weight of the moving average.
b.send_ewma: This is an exponentially weighted moving average of the time between TCP sender timestamps reflected in those ACKs, with the same weight 1/8 for new samples.
c.rtt_ratio: This is the ratio between the most recent Round Trip Latency (RTT) and the minimum RTT seen during the current connection.
[3]arrived at this set of observations by doing a comprehensive test of other types of feedback measurements that are possible at a source.
Traditional measurements, such as most recent RTT sample or a smoothed estimate of the RTT sample, were tested and discarded because it was observed that they did not improve the performance of the resulting protocol.
In par- ticular, Remy does not make use of two variables that almost every other TCP algorithm uses: the packet drop rate p and the round trip latency T. The packet drop rate is not used because a well- functioning algorithm should naturally lead to networks with small queues and very few losses.
The round trip latency T was intentionally kept out because the designers did not want the algo- rithm to take T into account in figuring out the optimal action to take.
Note that Remy does not keep memory from one busy period to the next at a source, so that all the estimates are reset every time a busy period starts.
3.Control actions: Every time an ACK arrives, Remy updates its observation variables and then does a look-up from a precomputed table for the action that corresponds to the new state.
A Remy action has the following three components: a.
Multiply the current congestion window by b, i.e., W ’bW. b.
Increment the current congestion window by a, i.e., W ’W1a.
c.Choose a minimum of τ.0 ms as the time between successive packet sends.
Hence, in addition to the traditional multiplicative decrease/additive decrease actions, Remy implements a combination of window 1rate-based transmission policy, where the interval between transmissions does not exceed τseconds.
At a high level, Remy Congestion Control (RemyCC) consists of a set of piecewise constant rules, called the Rule Table, where each rule maps a rectangular region of the observation space to a three dimensional action.
On receiving an ACK, RemyCC updates its values for (ack_ewma, send_ewma, rtt_ratio) and then executes the corresponding action (b, a, τ).
We now describe how these rules are generated: Remy randomly generates millions of different network configurations by picking up random values (within a range) for the link rates, delays, the number of connections, and the on-off traffic distributions for the connections.
At each evaluation step, 16 to 100 sample networks are drawn245 9.2MACHINE LEARNING AND CONGESTION CONTROL:
PROJECT REMY and then simulated for an extended period of time using the RemyCC algorithm.
At the end of the simulation, the objective function given by  is evaluated by summing up the contribution from each connection to produce a network-wide number.
The following two special cases were considered: U5logR2δlogT; (3) corresponding to proportional throughput and delay fairness and U521 R(4) which corresponds to minimizing the potential delay of a fixed-length transfer.
RemyCC is initialized with the following default rule that applies to all states: b 51, a51, τ50:01.
This means that initially there is a single cube that contains all the states.
Each entry in the rule table has an “epoch,” and Remy maintains a global epoch number initialized to zero.
The following offline automated design procedure is used to come up with the rule table:
1.Set all rules to the current epoch.
2.Find the most used rule in this epoch: This is done by simulating the current RemyCC and tracking the rule that receives the most use.
At initialization because there is only a single cube with the default rule, it gets chosen as the most used rule.
If we run out of rules to improve, then go to step 4.
3.Improve the action for the most used rules until it cannot be improved further:
This is done by drawing at least 16 network specimens and then evaluating about 100 candidate increments to the current action, which increases geometrically in granularity as they get further from the current value.
The modified actions are used by all sources while using the same random seed and the same set of specimen networks while simulating each candidate action.
If any of the candidate actions lead to an improvement, then replace the original action by the improved action and repeat the search, with the same set of networks and random seed.
Otherwise, increment the epoch number of the current rule and go back to step 2.
At the first iteration, this will result in the legacy additive increase/multiplicative decrease (AIMD) /C0type algorithm because the value of the parameters a, b are the same for every state.
4.If we run out of rules in this epoch: Increment the global epoch.
5.Subdivide the most-used rule:
Subdivide the most used rule at its center point, thus resulting in 8 new rules, each with the same action as before.
Then return to step 1.
An application of this algorithm results in a process whereby areas of the state space that are more likely to occur receive more attention from the optimizer and get subdivided into smaller cubes, thus leading to a more granular action.
shows the results of a comparison of TCP Remy with legacy congestion control protocols (the figure plots the throughput averaged over all sources on the y-axis and the aver- age queuing delay at the bottleneck on the x-axis).
A 15-mbps dumbbell topology with a round246  EMERGING TOPICS IN CONGESTION CONTROL trip of 150 ms and n58 connections was used, with each source alternating between a busy period that generates an exponentially distributed byte length of 100 Kbytes and an exponen- tially distributed off time of 0.5 sec.
RemyCC is simulated with the utility function in equation 3and with three different values of δ, with higher values corresponding to more importance given to delay.
The RemyCC algorithm that was generated for this system had between 162 and 204 rules.
The results in  show that RemyCC outperformed all the legacy algorithms, even those that have an AQM component, such as CUBIC over CoDel with stochastic fair queuing.
Further experimental results can be found in Winstein [3].
also illustrates that existing congestion control algorithms can be classified by their throughput versus delay performance.
Algorithms that prioritize delay over throughput, such as Vegas, lie in lower right hand corner, and those that prioritize throughput over delay lie in the upper left-hand part of the graph.
The performance numbers for RemyCC trace an outer arc, which corresponds to the best throughput that can be achieved, for a particular value of the delay con- straint.
Thus, this framework provides a convenient way of measuring the performance of a conges- tion control algorithm and how close they are to the ideal performance limit.
In general, these algorithms are notoriously difficult to change because they involve the creation and maintenance of a network-wide distributed system model that maintains a current map of the network state and is capa- ble of functioning under uncertain conditions in a wide variety of scenarios.
Rather than running an instance of the NCP in every node, as is done in legacy networks, an SDN centralizes this function in a controller node.
This leads to a number of benefits as explained below.
shows the basic architecture for an SDN network, which is made up of two principal components, namely OpenFlow-controlled switches and the SDN controller.
Note that the NCP located in the controller is centralized and exists separately from the data plane located in the net- work switches.
The controller communicates with the switches using an out-of-band communica- tion network (shown with the dotted line) using the OpenFlow protocol.
The SDN controller can be implemented in software on a standard server platform.
It is implemen- ted using a layered architecture, with the bottom layer called the Network Operating System (NOS) and the upper layer consisting of one or more applications, such as routing, that are used to control the OpenFlow switches.
The NOS layer is responsible for managing the communications between the con- troller and the switches.
In addition it also responsible for creating and maintaining a current topology map of the network.
As a result of this design, every control application shares a common Distributed System, unlike in current networks, where every application needs to implement its own Distributed System (e.g., the network map maintained by Layer 3 protocol such as OSPF is different from that maintained by a Layer 2 protocol such as Spanning Tree).
Moreover, the problem of designing a dis- tributed control algorithm is reduced to a pure software problem because the control algorithm can operate on the network abstraction created by the NOS rather than the real network.
Even though most of the initial work in SDNs has gone into the investigation of its impact on the NCP, researchers have begun to investigate ways in which it might be used to improve the operation of the data plane as well.
9.3.1 USING SOFTWARE DEFINED NETWORKS TO CHOOSE ACTIVE QUEUE MANAGEMENT ALGORITHMS ON A PER-APPLICATION BASIS In the discussion of the CoDel AQM algorithm in , we first introduced the notion that the most appropriate AQM algorithm is a function of the application, and this line of reasoning is developed further here.
In general, based on their performance requirements, applications fall into the following three categories: 1.Throughput maximization: Applications in this category, such as File Transfer Protocol (FTP), are only interested in maximizing their data throughput, regardless of the latency.
2.Power maximization: Recall that the power of a connection was defined as a function of the average throughput R av, and average latency T sas Power5Rav Ts(5) Applications such as video conferencing are interested in achieving a high throughput, but at the same time, they like to keep their latencies low.
Hence, they belong to the power maximization category.
3.Minimization of Flow Completion Time (FCT): Web surfing (or HTTP transfer) is a typical application in this category, in which the transfer size is smaller compared with FTPs and the user is interested in minimizing the total time needed to complete a page download.
Note that throughput maximization for a connection also leads to FCT minimization; hence, the first and third categories are related.
In the context of a network where both types of connections pass through a common link, the presence of the FTP connection can severely downgrade the FCT of a web page download if they both share a buffer at the bottleneck node.
Ways in which this problem can be avoided are: a.
By using an AQM scheme such as CoDel to control the queue size at the bottleneck node: CoDel will keep the buffer occupancy and hence the latency low but at the expense of a reduction in the throughput in both the FTP and HTTP connection.
This is not an ideal situation because the FTP connection has to give up some of its throughput to keep the latency for the HTTP connection down.249 9.3SOFTWARE DEFINED NETWORKS AND CONGESTION CONTROL b.
By isolating the FTP and HTTP connections at the bottleneck node into their own subqueues, by using a queue scheduling algorithm such as Fair Queuing:
In this case, neither subqueue requires an AQM controller.
The HTTP connection will only see the contribution to its latency caused by its own queue, which will keep it low, and the FTP connection can maintain a large subqueue at the bottleneck to keep its throughput up without having to worry about the latency.
This is clearly a better solution than a).
If an FTP and video conferencing flow (as representatives of categories 1 and 2) share a bottle- neck node buffer, then again we run into the same problem, that is, the queuing delay attributable to the FTP will degrade the video conferencing session.
Again, we can use an AQM scheme at the node to keep the queuing delay low, but this will be at a cost of decrease in FTP throughput of 50% or more (see Sivaraman et al.
A better solution would be to isolate the two flows using Fair Queuing and then use an AQM scheme such as CoDel only on the video confer- encing connection.
From this discussion, it follows that the most appropriate AQM scheme at a node is a function of the application that is running on top of the TCP connection.
By using fair queuing, a node can isolate each connection into its own subqueue and then apply a (potentially) different AQM scheme to each subqueue.
[5]propose that this allocation of an AQM algorithm to a con- nection can be done using the SDN infrastructure.
Hence, the SDN controller can match the type of application using the TCP connection to the most appropriate AQM algorithm at each node that the connection passes through.
Using the OpenFlow protocol, it can then configure the queue manage- ment logic to the selected algorithm.
The capability of dynamically reconfiguring the AQM algorithm at a switch does not cur- rently exist in the current generation of switches and routers.
These are somewhat inflexible because the queue management logic is implemented in hardware to be able to function at high speeds.
[5]propose that the queue management logic be implemented in an Field Programmable Gate Array (FPGA), so that i t can be reconfigured by the SDN controller.
They actually implemented the CoDel and RED AQM algorithms in a Xilinx FPGA, which was able to support a link speed of 10 Gbps, to show the feasibility of this approach.
The other hurdle in implementing this system is that currently ther e is no way for applications to signal their per- formance requirements to the SDN controller.
9.3.2 USING SOFTWARE DEFINED NETWORKS TO ADAPT RANDOM EARLY DETECTION PARAMETERS Recall from the analysis in  that a RED algorithm is stable (in the sense of not causing large queue fluctuations) for number of connections greater than N2and round trip latency less than T1, if the following conditions are satisfied
Hence, a RED algorithm can become unstable if 
The number of connections passing through the link becomes too small, or 
The round trip latency for the connection becomes too big, or 
The link capacity becomes too large.
One way in which instability can be avoided was described in , which is by using the Adaptive RED (ARED) algorithm that changes the parameter max pas a function of the observed queue size at the node.
We propose that another way of adapting RED is by making direct use of  and using the SDN controller to change RED parameters as the link conditions change.
Note that the number of connections N and the link capacity C are the two link parameters that influence the stability condition (  ), and the end-to-end latency T is a property of the con- nection path.
As explained in , C is subject to large fluctuations in cellular wireless links; however, it may not be possible to do real-time monitoring of this quantity from the SDN control- ler.
On the other hand, for wireline connections, C is fixed, and the only variables are N and T, which change relatively slowly.
Moreover, the SDN controller tracks all active connections in the network as part of its base function, and hence it can easily figure out the number of active connec- tions passing through each node.
The round trip latency can be reported by applications to the SDN controller using a yet to be defined interface between the two.
Using these two pieces of information, the SDN controller can then use  to compute the values of L redand K to achieve stability and thereby set the RED parameters max p, max th, min th, and w q.
Following ARED, it can choose to vary only max pwhile keeping the other para- meters fixed, but it has more flexibility in this regard.
Another example of this is the use of the equation τ/C3/C25tan21Gdw2 Npsﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:251Nps Gdw2/C18/C192svuut2 643 75 GdwC Nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:51ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 0:251Nps Gdw2/C18/C192svuut which gives the stability threshold for the QCN algorithm in Ethernet networks as a function of the link speed C, number of connections N and algorithm parameters (p s,w ,G d) (see , ).
As the number of connections N varies, the SDN controller can vary the algorithm parameters to maintain the stability threshold in the desired range.
9.4THE GOOGLE CONGESTION CONTROL (GCC) ALGORITHM The GCC algorithm
[6,7] belongs to the category of congestion control algorithms that are used for real-time streams such as telephony and video conferencing.
Delivery of real-time media is actually a fairly large subcategory in itself that is characterized the by following:  Rather than TCP, real-time streams make use of the Real Time Transport Protocol (RTP) for transport, which is implemented on top of UDP.
This is because reliable delivery is not as important as additional services provided by RTP, such as jitter compensation and detection of out-of-sequence frames.251 9.4THE GOOGLE CONGESTION CONTROL (GCC) ALGORITHM 
Unlike TCP, every RTP packet is not ACK’d.
Instead, the receiver provides feedback by using the Real Time Control Protocol (RTCP).
It generates periodic RTCP Receiver Report (RR) packets that are sent back to the source.
These contain information such as number of received packets, lost packets, jitter, round trip delay time, and so on.
The information in the RTCP RR report is then used by the source to control its transmission rate.
Traditionally, the TCP Friendly Rate Control Algorithm (TFRC) has been used for this purpose (see Floyd et al.
This algorithm controls the transmit rate as a function of the loss event rate statistic that is reported by the receiver.
There are a number of problems in using TFRC in modern cellular networks, including: 
The packet loss rate in these networks is close to zero, as explained in , because of the powerful ARQ and channel coding techniques that they use.
The only losses may be due to buffer overflow, but even these are very infrequent because of the large buffers in the base stations.
Loss- based protocols such as TFRC tend to fill up the buffer, leading to unacceptably large delays.
 Cellular links are subject to large capacity variations, which lead to congestion in the form of the bufferbloat problem described in , which are not taken into account by TFRC.
The GCC algorithm is designed to address these shortcomings.
It is part of the RTCWeb project within the IETF, which also includes other specifications to enable real-time audio and video com- munication within web browsers without the need for installing any plug-in or additional third- party software.
GCC is available in the latest versions of the web browser Google Chrome.
The GCC algorithm tries to fully use the bottleneck link while keeping the queuing delay small.
We came across the problem of minimizing queuing delays on a variable capacity link in the context of TCP in , where the solutions included using AQM techniques such as CoDel or end-to-end algorithms such as Low Extra Delay Background Transport (LEDBAT).
The GCC algorithm belongs to the latter category since it does not require any AQM at the intermediate nodes.
(Note that LEDBAT is not directly applicable here because it relies on the regular flow of TCP ACKs for feedback.)
GCC uses a sophisticated technique using Kalman filters to figure out if the bottleneck queue is increasing or decreasing and uses this information to compute a number called the Receive Rate.
It sends the Receive Rate back to the source in a special control packet called REMB, which then combines it with other information such as the packet loss rate to determine the final transmit rate.
The GCC algorithm is composed of two modules, one of which runs at the sender and the other at the receiver.
These are described in the next two sections.
9.4.1 SENDER SIDE RATE COMPUTATION The sender combines several pieces of information to come up with the transmit rate, including the packet loss rate (as conveyed by the RTCP RR packets), the rate of an equivalent TFRC source, and the rates coming from the receiver in the REMB packets.
Sending rate as computed using the TFRC algorithm at time t 5tk fl(tk):
Fraction of lost packets at t 5tk On receipt of the kthRTCP RR packet, the sender uses the following formula to compute its rate: RsðtkÞ5max fXðtkÞ;Rsðtk21Þð120:5flðtkÞgflðtkÞ.0:1 1:05ðRsðtk21Þ11kbps Þ flðtkÞ,0:02 Rsðtk21Þ otherwise8 >>< >>:(7) The reasoning behind these rules is the following:  When the fraction of lost packets exceeds 10%, then the rate is multiplicatively decreased but not below the equivalent TFRC rate.
 When the fraction of lost packets is less than 2%, then the rate is multiplicatively increased. When the packet loss rate is between 2% and 10%, then the rate is kept constant.
Hence, in contrast to TCP, the GCC sender does not decrease its rate even for loss rates as high as 10%, and actually increases the rate for loss rates under 2%.
This is done because video streamsare capable of hiding the artifacts created because of smaller packet loss rates.
When the sender receives the r thREMB packet, it modifies its sending rate as per the following equation: RsðtrÞ’minfRsðtrÞ;RrðtrÞg (8) 9.4.2 RECEIVER SIDE RATE COMPUTATION
The novelty of the GCC algorithm lies in its receiver side rate computation algorithm.
By filtering the stream of received packets, it tries to detect variations in the bottleneck queue size and uses thisinformation to set the receive rate.
It can be decomposed into three parts: an arrival time filter, an overuse detector, and a remote rate control.
9.4.2.1 Arrival Time Model and Filter In the RTCWeb protocol, packets are grouped into frames, such that all packets in a frame share the same timestamp.
We define the sequence d ito be the difference between these two sequences, so that di5ti2ti212ðτi2τi21Þ (9)253 9.4THE
All of these factors are captured in the following model for d
The calculations are as follows:
Define ξi5½1=CðtiÞbðtiÞ/C138Tas the “hidden” state of the system, which obeys the following state evolution equation: ξi115ξi1ui (11) where u iis a white Gaussian noise process with zero mean and covariance Q i. From  , it follows that the observation process d iobeys the following equation di5hiξi1vi (12) where hi5½LðtiÞ2Lðti21Þ1/C138and v iis a white Gaussian noise process with zero mean and vari- ance S i. Let ^ξi5½1=^CðtiÞ ^bðtiÞ/C138Tbe the sequence representing the estimate for ξi.
Assuming that the last packet in the ithframe is received at time t i, the receiver computes the rate Rr(ti) according to the following formula: RrðtiÞ5ηRrðti21Þif state 5Increase αAðtiÞ if state 5Decrease Rrðti21Þif state 5Hold8 ><
Also, the R r(ti) cannot exceed 1.5/C3A(ti) in order to prevent the receiver from requesting a bitrate that the sender cannot satisfy.
Note that the rates are controlled by the state of the Finite State Machine (FSM) in  , which is called the Remote Rate Controller.
The state of this FSM is changed according to the signal produced by the Overuse Detector, which is based on the output of the Arrival Time Filter, as follows: I f ^bðtiÞincreases above a threshold and keeps increasing for a certain amount of time or for a certain amount of consecutive frames, it is assumed that the network is congested because the bottleneck queue is filling up, and the “overuse” signal is triggered.
I f ^bðtiÞdecreases below a threshold, this means that the bottleneck queue is being emptied.
Hence, the network is considered underused, and the “underuse” signal is generated.
Upon this signal, the FSM enters the Hold state, where the available bandwidth estimate is held constant while waiting for the queues to stabilize at a lower level.
When ^bðtiÞis close to zero, the network is considered stable, and the “normal” signal is generated.
The REMB messages that carry the value of R rfrom the receiver to the sender are sent either every 1 sec if R ris decreasing or immediately if R rdecreases more than 3%.
The system state x tis not observable directly; its value has to be inferred through the k 31 dimensional state observation process z tgiven by zt5Ctxt1δt (F2) where C tis k3n dimensional matrix and δtis the observation (or measurement) noise process.
The distribution of δtis assumed to be multivariate Gaussian with zero mean and covariance Q t. The Kalman filter is used to obtain estimates of the state variable x tat time t, given the control sequence u tand output sequence z t. Because x tis a Gaussian random variable, the estimates are actually estimates for its mean μtand covariance Σtat time t. The Kalman filter uses an iterative scheme, which works as follows: 1.At time t 50, start with initial estimates for the mean and covariance of x tgiven by ðμ0;Σ0Þ. 2.At time t /C01, assume that the estimates for the mean and covariance of x t21have been computed to be ðμt21;Σt21Þ.
Given the control u tand output z tat time t, the estimates ðμt;ΣtÞ are computed recursively as follows, in two steps.
3.In step a, the control u tis incorporated (but not the measurement z t), to obtain the intermediate estimates ðμt;ΣtÞgiven by μt5Atμt211Btut (F3)
The variable K tcomputed in equation F6 is called the Kalman gain, and it specifies the degree to which the new measurement is incorporated into the new state estimate.256  EMERGING TOPICS IN CONGESTION CONTROL
